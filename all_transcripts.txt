Hello, everyone. So today's class will be about the all indexing techniques. Before you call what we discussed in the storage in the hour class that we have this role based storage. We have also the columnar based storage and the hybrid storage layout. So after that, we mentioned that this is that kind of the physical representation or the data itself. And in the devices that we use, whether it's main memory or hard disk, for example. But on top of this data, we need to have accelerated structures or what we call indexing to the structure to basically help us like navigate this data and retrieve that records or couples that we have to answer our queries. And today we are going to focus on the lab workload. So today's class will be based on metrics from that CMU Advanced Database Systems course, and we are going to focus mainly on three. Main is State of the art pieces of research in this area. The first one is the pit waiting. The second one is the columns catchers and the third one is the column inference. So in our presentation, I'm going to give you a blank overview over the topics and the main ideas of of each one of them. But the main details will be in the paper. So I strongly encourage you to go there if you are interested and want to know more about the techniques, about the different operations they support, about their main features and so on. Okay. But before delving into the details of these techniques, I just want to might remind you with what we mentioned in the storage NIE out class. Basically, we said also that in the lab data, we have two main assumptions. The first assumption is that we mainly consider this as a read only data. So basically most of the data that we use in the lab workloads or the lab case, it's coming from application logs. So data that are already generated based on all transactions or some information or monitoring, monitoring tools, whatever. And as a company or as a data scientist or a data analysis analysis person or whatever. I want to do some statistics, but for example, I can do some aggregate quiz by give me the average of the values in this data or give me the minimum or maximum and so on. So basically it's a read only data and I do some kind of operations like aggregate operations. We don't usually optimize for any inserts, updates or DVDs. This is a happening in the OTB workloads only and we have their own techniques there. And the second assumption is that usually when we retrieve the data or read the data that we use in the case, we read them in large chunks. So we usually do largest case. Typically, we don't want to access only one topic at a time. And as I told you, this is typical in the other queries because when I do an aggregate operation, this will be on many, many, many, many couples. At the same time, because I'm getting the average, but only for one topic for many topics. Okay. Also another another use case of using this largest cams or like large portions of data access happens as an intermediate step and other database operations like, for example, in the joint operation when we have two tables that we want to join. So basically what we want to do here is that we read that columns from these two tables at the same time, and we somehow linked between them to get the joint output. Okay. So in order to read this columns from the tables, we usually read them as a whole, so we don't read them one by one. Okay. So this is the second assumption. So given these two assumptions, we want to build a data structure that can help us to, like, improve the data access for this product and want a straightforward solution that can come to your mind that if we have partitions, for example, in a row storage layout or even like in column, the storage me out, what we can do here is that we can smartly try to avoid accessing all the partitions or scanning them when we have a. What I want to do as an optimal solution is that if I have a query, I want to just go directly to the partitions. That will be for sure having the answer for my way and avoid as much as they can. All the partitions that will be having like redundant information to this query or will be maybe like, I mean having some data that's out of the range. Okay. That's basically what we do want to select from a solution to do that. And that for each partition, for example, if I have a column like Grades and I'm usually asking about the grades in this column, we can for each partition, we can store the minimum and maximum value, for example, for this grade. So now let's say that in partition AIM, we have grades between ten and 20 and they have the first query to ask about grade 15. So when I have this query before to start scanning all the tuples in this partition, I just go to the minimum and maximum values and check if my query value like the grade I'm using for answering my query is already in the range between this minimum and maximum values. Then I can start scanning this partition because I know for sure that it will have some tuples matching my my for example, my key. But then me or like my grade that they have here. But if I have another query, like, for example, asking for it, students with grade 40. So I know that this partition, the first partition that has many as ten and large as 20, will not be having any business. Was this quick, so I can totally skip this partition. Okay. So we usually call this as zoning information, but it's mostly about pre filtering the data. It's not like building an index structure that helps me to access the physical properties that we use in this in this part. Okay. So to build this physical index that we use for retrieving the data, we use techniques called data skipping. Okay. So in this data skipping, we have the same objective as in zoning, which we want to avoid getting published individually, but still it helps us to improve the data access while considering three main things. The first thing here is that we we face different types of quiz. Like some quiz can retrieve a large number of topics as an output or other types of queries can be like just retrieving a small number of bubbles. Okay. We called the first type of query that retrieves the large number of studies that has low selectivity because now the predicted that I'm using for retrieving the data is not filtering as much as have. It's just retrieves most of the data that we have there. So we call this low selectivity and vice versa like mean the other type of queries that we usually in this quiz like retrieve very few number of tables, we call this high selectivity queries. Okay. So we need when we build this, the skimming technique to consider the two types. Of course, the second design consideration that this little skimming technique will be helping us is building balance and distribution of the data that we know. Okay. So we want to make sure that that data that we have here in this partition is somehow in this partition, somehow balancing so that we avoid any skewness issues in that competition while we are doing this data process. Okay. The third consideration here is that sometimes the data that we physically stored based on a certain attribute or whatever maybe is not helping us efficiently for answer, obviously. So, for example, that data could be stored based on a primary key or an ad like maybe I.D., but I usually issue queries with the name of the student or the grade of the suit. So in this case, this physical layout will not help us, will not be helping us to answer this question. Very efficient. Okay. So data skipping technique should be providing us a solution that can accommodate for this limitation. Okay. So the main technique that we use in this giving is a bitmap. It's this this considered the kind of the straight forward solution that you will find in any lab device and basically sync with as a compression technique. So as we mentioned in the lab, we have large amount of tables and most of the data that we have in the columns here will be repeated. So think of it as most of the data that we have will be based on categorical variables. Let's say that it's a we know that the grades will be is are good. Like excellent, very good, good. So we have like some categorical values that we have here. And each value of this column in each table will be either one of this. For example, five categorical values. Okay, for the great value. So we will have many, many records with that. Good. For example, grade, we have other many records with that excellent grade and so on. So we can avoid just repeating this values in the column that we have here by just storing some equivalent representation for them. And so that just storing the actual value each couple and using this equivalent representation for checking whether the data that I'm looking for is already there or not, and if it's already there. Now I can just to go and retrieve the origin of that. Okay. So it's basically think of it as a compression technique. And once I compress the data, I use this compression form to do my chicken or query answering. And when I get the output that I need or like I know that this is the actual table that I'm retrieving, it's a value. Now I go to the original value and retrieve it. Okay. So I'm going to give you an example on how we use this big map and like representation to do the compression and do indexing there. But one main observation here is that usually since we have like very large number of couples in the lab workload, we typically want to like segment this like bitmap representation to avoid any continuous memory allocation, because most of the data that we have, you will be having like moderate amount of categorical values. So instead of just representing each value here with an equivalent representation, we we just segment this data that we use for the bitmap in two chunks, and then we build this bitmap index for each chunks separately. This would be easier in terms of managing the resources and avoiding cache validation a lot. Okay. So here's one example on like of this bitmap index. So assume here on the left side we have this original data. And in this original data we have a set of tables and each topic here we have an ID, as you see here, if one, two, one, two, three, four, and so on. And also we have a column that has a value either yes or no. Okay. So think of it as we store one character with Y and one character with No. If the data is yes or the data is okay. What we want use as an equivalent representation for a bitmap index is the one that we have here on the right hand side. But this is the compressed representation. So we are going to keep the idea column. Maybe this is the a primary index that we have, but the column like the column of of this value, yes or no will be represented with bits and instead of the actual character value of y and everything. So if it's yes, we have a bit or yes. And if it's no, we have a bit for both. And if the actual value in this table, for example, is. Yes. Which is, which, is that correct. The Y now we go to the corresponding bit and we put that but there one and if it's the value is, is no or like n we go to that and that represents the end value and we put this one there. So for example, if, if I want to get the value of this column for ID four, then I go to ID for property there and then to take the bits I see here that the bit at the end value, the big corresponds to the end value is already one. Then I retrieve the end that. So I return as an output. The end. Okay. So this kind of representation here by having one bit equivalent to each value or each unique value is efficient only if we have very low number of categorical values in each category. So for example, here the best case for us is to represent use this bitmap representation for representing boolean value because we know that the domain value would be only either yes or no, like two values money. So now we have only two bits to represent the domain values. Okay. But on the other hand, there might be a case that for each ID we generate a unique value. For example, for like, for example, if, if we want to build a bitmap index for this ID, this would be extremely inefficient because now we have one bit for one, one bit for two for value. Two one bit for value three. One bit for value four. And now in each table we only sit one bit was one and the remaining bits will be zero. So this will be very, very sparse and will be you just sort of that. So bitmap index, it's not that efficient if you have like like large domain values in this case. Okay. Typically we have different types of encoding to represent this bitmap index. Okay. So I already covered the basic one, which is having one bit for each unique value like what we mentioned here in this example, this or that. But there's another smarter way that actually we can represent each range of values with one bit. So assume here that we want to in the same example of the bitmap and the building and bitmap index form ID like we know that for the IDS one, two, three, four, five, six we have like a unique value for each ID value. So now we should have one bit for each ID value as we mentioned in the first type of encoding. But now let's state that I'm gonna like, like represent the whole range between one and ten was only one bit. So if we have like values like that, unique values from 1 to 100. Then if I represent each ten numbers with one bit, then we have only ten bits to represent the whole range between one and 100. Okay. So of course this is much better in terms of the storage overhead, but this will be bringing us to another tradeoff that we are going to see also later, soon, which is now we are not having exact representation for target values, but we have approximate representation and this is not accurate because if I'm looking for the value five, then the bit represents the range between one and ten will be one. Okay. And then I should check all the values with this bit set as one, because now maybe like I'm in the record that makes this bit was one is having four fights or having six instead of five or seven. All of this values will be valid to this range. Okay, because they are still between one and ten. Okay. So we need to do a further check step to make sure that we are retrieving the correct value. Okay. So this is a tradeoff between the storage overhead and the efficiency of processing. Okay. We're going to see also I mean, I made a little example about this. Like later. So. Okay, another way here is that we use this better presentation to do a hierarchical traversal like what we do in the B three, what I try and structure or whatever there. But this kind of encoding is somehow rare in all indexing. So we're going to skip it now, but just good to know that this kind of technique also using a hierarchical structure like B three also for what we want to focus on here is the first one is that bit sliced encoding. So in this encoding we use that binary representation of the data itself. So if we have an integer or if we have a character or whatever data type we have here, we use it's a binary representation to do this indexing for the IT. And I'm going to show you how we do this here by an example. So assume here that we have this data on the left. We have original data here, a set of six tables. Each table has its ID zip code. So the zip code here is usually like a number of five numerical digits, like what we see here, like 21042. So each digit represent is represented by an integer. Like for example, it's four bytes. So now if we want to represent the whole zip code, it will be four times five means like we need to represent each zip code with 20 bytes, for example. Okay, if we just use this 21042, what we do here and instead of just using the zip code as is, in the end that column, the values here, we transform this, this zip code into its binary representation as we see here at the bottom. So we find the equivalent binary presentation. Usually we use the 32 better presentation and we use the bits that we have here for this value like to be used for the indexing like operations self. So assume here that this 32 bit representation we want to sunk it and then store it in only 17 bits because the the remaining bits would be zeros. So we have here the bits from 0 to 16 they are representing here this binary representation of the value to 1042 and we have only one additional bit that will be using for sitting it, whether it's zero or one to indicate that this table, for example, is satisfying a certain predicate tick. But you're checking for a zip. If if I'm looking for the zip code 21042 and I find the exact match representation for the exact match for the binary representation here, then I go for this light bit and I check this was one. So now at the end of the processing or the indexing, I go and look for the bits in the red bit here that has a value one and then retrieve this as an output for the query. Okay. So we do this binary representation for the different like the different values of the zip code here and we have the equivalent binary representation here presented in this example and 17 bits. Okay, so assume that we have this equity. So we want to answer this query based on the data that we have here and the representation. Okay. So you're saying select all from the customized where the zip code is less than 15217. Okay. So basically we want to retrieve all the couples with zip codes. This then this number came. One observation here that you can see is that if we have numbers less than 15217 for example, and 15217 is having the first three bits as you see here. For example, as zeros. So all the zip codes that will be less than this number will be also having zeros in the first three digits, the digits here corresponding to the 14, 15, 16. Okay. So if I'm using an efficient bit bitwise operation, that can take this three bits in each record if they have one or not. If they have one, then we should skip this record because we know for sure that this will be larger than the 15217. Okay. Because this is like that. This is like an integer to like two like to explain here. So because you know that the numbers here will be less than this 15217. So for sure we don't have any one of the numbers less than anyone having one. And this like last three bits. Okay. So what we do here is that, for example, to answer this queries that we check for all the the tables that we have here, the first three bits and skip all the entries that have one in the first slides, the slices. And if we have still some records that have all of the values zeros, then we go and take them one by one. So we've narrowed the space that we use for searching for the answers that we're looking for. Very, very good. Okay, So this is like an idea that I described to you. This is the idea of the bits slicing in general. So basically, you go there, you find an equivalent better presentation, and then you use this better representation to quickly filter out the things that will not match. Okay. An interesting technique or like an optimization of this technique is that bit weighting and basically it's a still a binary presentation. We use the bits as we did in the bits slicing, but here we are using is essentially like the simple operations. Was that so simple? Operations are like the correspond to the disambiguation is corresponding to the single instruction multiple data. So basically it's a set of instructions that that support the know by most of the processors that basically you can have like I mean a whole world of, of bits or maybe like a cache line or whatever, like structure of data that we can apply, all that we can apply one operation and all of this data at the same time in parallel. So in let's say that we have a cache line or a word processor of eight bits, like let's assume it's eight bits, then we can apply that eight checking operations on these bits at the same time in part, and this is like, I mean a built in feature in most of the processors we call this simple operation. And if you're optimizing your source code to use the same operations a lot, then you will find a lot of performance improvements into, Okay. So in between here they are combining the idea of the pit slides and the simple operations so that we can improve the checking for whether we are treating that that like the right tables or not. And of course this this is a good way for utilizing the personalization and usually we use the dictionary encoding that we will present the whole integer, all the whole string with small number of bits. And usually in this simple operations we just rely on very, very common, logical and comparison instructions. So we don't use and like very complicated instructions like, I mean gathering or scattering the data from the memory or so. So we just like use a very simple one like and shift X or stuff like this. Okay, So the paper is very interesting. I really recommend like everyone to go there and read it. It's one of them. It's one of my favorites. So typically in between, we, we, we support the two types of it's like the horizontal and the vertical, like the row oriented storage and the column oriented storage. So basically the differently outs that we talked about before, bit weaving usually can support all of them together. So we're going to here show examples on this row oriented and column one. But typically this can be also the same case for the hybrid storage that okay, we're going to start here with the horizontal structure. So assume here that we have this data that we have on the left side. We have records from 0 to 10 nine. Okay. So let's assume that this is the values for one column and now we this column is numerical somehow. So so we have here 15616. So this, this are the absolute values in this column. What we do here is that we use the binary representation, as we mentioned in the last one. So for the one here we have this binary representation 001 and four five, we have 101 and four six, we have 110 and so on. Okay? And now we are doing this horizontal partitioning. So each a set of tuples, we partition them together. Okay, so remember that. So here we partition the data between 202t7 in one segment and the remaining data will be in another thing that we have you. It's okay. So let's now zoom and each one of these segments. So for segment one here, what we do here is that we align this data in something we call a word. So this word will be that basic unit that any symbol operation would be using. So basically, if we have a word of eight bits as we see here, so this is a word of eight bits, then the send operation would be applied on all of this, eight bits at the same time. Okay. So since we have here, for example, two zero represented as three digits or like three bits and also a T, one and T two and so on. So it's column values that we have here is represented with exactly three bits. Then we can fit in one or two topics like that. The values of the columns corresponding to two to like t zero 21 k. What we do here is that we take the data, as you can see here. t0d13223 and then we, we put them in parallel to the other set of w t 45 to 67 and we basically you combine t zero and t for the value of the column, t for n in the same word and we have a delimiter here for each one of these columns that will be used for sitting the value as we did in the pit slicing. So think the value if this column value matches the predicted that we used for certain elements. Okay. So assuming that they are all zero at the beginning and we can fit each one like like to the column values of two tuples at the same time and each one of now if we have the square, this assumes that we have this quick select all from table where the value is less than five. What we do here is that we have here as x one words. This is like m in one word representation like what we mentioned here. So X here is the word representation in the first two tables here that is zero and four. And now we see the predicted value, which is five in this case, and we build another word for this value. So basically we are going to have this value five check against the column value of each one of these tables. So we repeat it here we find binary representation of this five and we repeat it here for the two tables, like two zero 94. Okay. And then we have a mask. Okay. So this mask will be used with along with some shift operations to represent the logic that we need here in the break. So basically that predicates here is representing less than a value, then using this mask value and some operations like end and not X or whatever, then we use this operations to implement the the chicken with each value with its list conformance. So given this expression that we use here to represent this value, we check whether the first the call, the value of the first, the table, which is corresponding to zero here is less than five. This one is one. This the value here is one and it's less than five. Then it will put the delimiter here as one. So this means that the value here, which is in zero, is actually less than the value that we have here for one, for one. But on the other side, for four, we don't have this case. So basically here we check the value in T for the binary representation again is the value of Y, the binary representation of five and use this operation to check whether it's this value which is six is less than five or not. So now here we found that five. That six is not less than five. So we put here as So this means that the first table satisfies the predicate that we are looking for the second table, not satisfying it. Okay, so now we do this for all of the tuples that we have. 202123d 45 de 60 and so on. And we, we we, we put the value of the bits here based on the checks. So we find some of them already satisfying the condition less than five and other. Okay. So we have here this let's find this satisfying the satisfying the satisfying and we have other for that. I'm not satisfied, but this is not the answer we want to get actually decouples. We want to retrieve the table saying that the first table, the second table, the third table, whether what are the couples that answer this find it. So we need to wait. That retrieves that the first double, the third or like that first table the the fifth double the sixth table. And the third table here should be the answer. Okay. What we do here is that since we are like using shift, like since we are using binary operations here and similar operations, we can make advantage of using the shift operation to put that bit here that we have for each table in that correct index. So basically D zero and T four will be the same indices because the bits here are in the first offset and force opposite but de one and t five. We need to put this value and this value at offset one and offset five. So basically we shift them with one and then we or it was the previous value. So this means that when we shift this was was one the value here that we have at offset zero will be at offset one. And the value here that we have at offset four will be at offset five. Okay. So now this will be in the correct place. And we do this also for the other, like two words here by shifting this with two bits and shifting this with three bits. So now we have the bits already in the correct offsets of the corresponding outputs. Okay. What we do at the end is that we all, all of them. So now this bit will be in the first offset, this bit will be in the second offset and so offset for offset and so on. So the bits that we had here from the checks will be actually in the offsets of their tables. And in this case, we we know that a couple of zero table three table for table, table zero, table three a couple five and six will be satisfying. The condition which is valuable is complexity. Okay. But we know this look logical. How can we retrieve it efficiently in the Pacific? Okay, so one way, which is like we typically do in programing, is just we go through this vector that we already said the values was 001 and we just checked it bit one by one. If it's one, then we add the corresponding offset to that. Okay, but this is not efficient. What actually people do now is that since we know that number of topics and we can actually store for each like, you know, tuple or representation the corresponding key and now in for each key we can physically put the values or like know that like put the tuples corresponding to the values that we have here in the payload. So basically here I know that this like representation will be corresponding to 150. Like if we use this binary representation. So we just have one 150, we, we record there 0356 so we don't need to check the value of it. Take the values of this bits one by one, and then like retrieve the corresponding tables. Now we can just go there and store these values corresponding to the keys that we have. So this will be like sacrificing a bit of storage. But on the other hand, it will be very efficient to retrieve the labor corresponding tables. Immediate ten. Now let's move to the vertical storage so that the same idea and concept will be applied here. But now, instead of just trying to align the tuples with what like we put two tables in or like the columns of value of the values of columns corresponding to two couples in one world. We are trying to pack this world with values from the same column. This is similar to the column representation. That's why it's called the vertical structure. Okay, So as you can see here, we have this same example. But now instead of putting D0 and D4, for example in one word, we are putting here eight bits from 0 to 7 in one word, and we do the same thing for the second column. We do the same thing for the third column. And so now you see in segment two we have only two couples. So this means that we already set the values for the first two columns here only, and the remaining columns will be zeros, because we don't have many. But we we we have to put this as there is so that we can fill the whole segment and make it a line. Okay. So once we do this, this alignment for that, the words that we have here, we basically one answer query like this, for example, what we do here is that we want to check our or retrieve the topic that already matches this predicate, saying that the value is equal to. So since we have everything here in a binary representation, what we try to do here is that we get the equivalent binary representation for the predicate value here, like two, which is 010. And now we know that these values which which have three bits will be checked against this, A three columns that we have here. So we have one column here would be against all the values in the first column here from all tables, and the second bit will be checked against the values of the second column in all couples and so on. So what we do here is that, for example, in the first one here we have zero in the first column, we check this zero with the whole values that we have here in the first column of this whole tables. Again, in order to do this efficiently as a simple operation, we just create a vector with the same size. So if it's like a base here, so the word there will be a bits and then we just replicate the zero in all of them. Now what happens here is that we do a compare operation. So basically we use this zero and compare this deal here. Zero here will be compared with one here, zero here will be compared with one here, 000 here and so on. So each bit will be compared with the corresponding one. And if there is a match we set, the value here of the bid was one. So here the first bit and zero is equivalent like is zero and the first bit here is zero. So we retrieve one. The third bit, the first empty three also is zero. So we set here one the first bit and T six is derived. So we set here one and so now in order to find the exact matching, we have to repeat the same process for all columns. So we do the same thing also for the second column and then we check it and so on. So we have here one is one cell compare result for each column and then we combine them together to find the final answer. Okay. Now if we don't have any matches at any point, for example, if I don't find what I already started with zeros and I found some records matching. Now if I move to the second bit and I don't find any type that has, for example, once, so the is the same, the compare result will be all zeros for all the tables that we have here. So in this case, this means that there is no number that like matches the values to here in this and we can do a specific optimization is that we can stop here because we know that it doesn't matter whether we are going to check that coming at columns or not because it's already lost this fine in the current column. So in the future columns as well, we will be not satisfying it anyway. So we can actually use this as an optimization and stop skipping queue. Okay. So what we did now is that we described one way of using this. It's idea using combined with the simple operations and a technique called betway, so that we take the values that we have in the predicates using the bit representation. And then after we get that it's set corresponding to the tables, we retrieve the actual values for the top. Okay. Another idea, as I mentioned before, is that we don't need actually to represent one bitmap for each unique value or one like having one bit representation for each unit. This would be too much overhead so we can actually sacrifice some of the accuracy here to improve, to improve actually the the storage and also do some faster evaluation and in some cases, Okay. But of course, since this is an approximate solution, if we want to guarantee the correctness of the answers, we are going to do some redundant checks at the end. Six checks, we call them self checks to make sure that we are retrieving the correct the correct answer. So I quickly here give you the idea of the two main techniques that we use here. The column imprints and the column sketches. So in column imprints, the idea is very simple. So recall the same bitmap index of presentation that we have. So assuming here that we have this original values 184 so we have the eight representations for them over eight bit representations, sorry. So here we have the this is the representation for the value one. So we have here only the bid corresponds to the, the first, the value one here is set by one and the second for value eight and the set for value four. And so what we do here is that we can represent the three rows was only one column print. You can imagine. You can like observe here that this three values they have three different bits that correspond to one. Okay, So what we do here is that we can have one like bitmap representation that actually puts one at this index. This index and this index at one and four and eight. Okay? If we have a like a value of a bracket to check and we found that it matches any one of these bits like one year or one here, one here, this means that we have a possibility that the this value could be one of this one or eight or four, but we don't know for sure whether it's one or eight or four at this point. We have to go and take the corresponding value, whether it's one or eight or four. Okay. So this approach, the column imprint, is very useful for filtering the columns or the values that don't match. But for the column and values that match the predicate we have here, we have to go there and double check it again to make sure that we are returning the correct. Okay. So the idea of the other idea, which is the column sketches, is somehow also similar to this. Now we are trying to make sure that we are partitioning the data that we have, like the column values that we have here, any column values that we have here into a set of ranges, and then we give a code for its representation or like each range. So as an example here, assume that we have this original data that we have here on the left side. So 13 191, 56, 92, 81 and so on. So we have them as a base representation. So what we do here as a first step is that we build a histogram for all the values that we have here and we divide the value domain into equivalent proportions. So we know for sure from these values that we can partition them and partition domain between zero and 256 and we want to represent them with two better representation. So we partition this into four so that we have for each render one binary representation. Like for example, in the first part we have zero zero in the second part zero one and then one zero and then one one. So this means that all the values that we have between zero and 60 will take only one binary representation, which is zero zero. The second set of values between 60 and 132 and so on will be between which will take the values of one and a third of the force would be similar. So this is called the compression map here. So as you can see, it is compression. What we do here is that we replace the values that we have, the original values, the representation, ways to beat representation based on the codes that we have here and now. If we have a query like this, select all from the table that we have here where the value is less than 90, for example. So if it's less than 90, what we do here is that we go and take this value 90 what our the ranges that should be serving this predicate value less than 90. And by checking this map, we will see that this first code corresponding to 60, this means that the values between zero and 60 will be zero zero. So I need to take all the column values that are represented with zero zero because I know there could be some values here. Less than 90. Okay. And also the same thing, between 100, between 60 and 132. There are some values overlapping with values. This that 90. Okay. Of course does some other values between 90 and 132 that are not overlapping, but there are some here still overlap. Okay. So basically what we do is that based on this predicate, we have to check these two ranges or the values was was this two good maps that we have here? And then we go there. Whenever we see a value corresponding to zero zero, we go check the original value, whether it's already there less than 90 or not. So we know that this one will be 3013 will be less than 90. But for others, like, for example, zero one, which still corresponds to 132, But we know that there are some numbers between 90 and 132. But for example, here in the zero one, we can still get values more than 90. And in this case this will not satisfy that predicate. So we just retrieve the values of the columns that are already less than 90 only. So this is the importance of having this redundant check after we use the map code that we have here in the beginning. Okay, so in summary here, we like structured upon that some techniques that we use for compressing the data and indexing them for the purpose, which is very essential feature and it's not supported in most of the devices. And we focused mainly on the bitmap representation because this is the most straightforward and state of the art techniques that we use there. As you see here, we have always a tradeoff between efficiency and storage, and we typically use either like exact or approximate representation. Like, for example, we have one bit representation for each value or we have one better representation for, our range of values. Okay? And we also use this zoning, like having this data summaries stored in each partition so that we can be the partitions that we cannot use for answering like we also we can use this information with the bitmap indices and all combine them together to optimize everything. So this is kind of an overview of these techniques and I strongly recommend again to read the papers and if you have any questions at these papers, please like stop by my office and I'm happy to discuss them in my office hours with you can Okay, Thank you. This.The Dallas Morning News. So based on your requests, I'm going to record the class. So basically, after each class, I'm going to like upload the recording that we have here. So what I'm going to do is that we are going to record our live discussion so that you can see the video will be just having that presentation and that will be facing me like this. So it will not be like by looking to the camera. So it will be a little bit distracting. But if you are attending here, this will not be a problem. I just like I mind that if you are trying to find a specific detail or like want to get that recap with us, I just want to make sure that you are not using this as an excuse for not showing up. Please be responsible and show up. Okay. I'm doing this for your benefit, not for my bad. I know this, but okay. Before starting that class today, just like that kind of part. So administrative stuff. So it you start like working on that first review report that everything is going well. Okay. I know that some questions about the format, about things, but trust me, after that first report, you will get used to this and it will not be a problem. Maybe like some kind of steps for you when you read the paper. I usually do this, but that's my preference. But maybe someone else prefer to do something else. I go for the introduction section. I read it completely and carefully. I give it as much as I can time because this will be a very important piece for us. This will give you an overview of the problem, a high level of a high level like direction about the proposed solution. What we are trying to do, and also most of the good papers will try to position it as this work compared to others. So they say that is a paragraph. Say that. Okay, so other previous words and we put some references doing business while we are doing this. You have this limitation. So we are trying to address this limitation unsolved. So it's kind of a brief summary for what already existing and what we are trying to have. This would be important for you to scope the neutrality of the paper. So because I ask you, what's the mobile aspect and the report? So this will be one of the points. Okay. And indeed, describe also the experimental setup of the evaluation results. And after you do that, you want to hit and want to get to the action. So you go to the related work section. Sometimes it's just the right after the introduction. Sometimes it's at the end of the paper, this kind of section called related work or better. Sometimes they combine is within the background section, but usually what we have section by itself called related work. This will be many focusing on the existing techniques and how they are different from what they have. Basically, they are detailing what we mentioned and the introduction was more specific because this will deepen your understanding of where this problem is looking. Okay, after you do that now, we want to go through the technical details. You start now going through that system overview section or the proposed solution. This would be a high overview of that solution. And then after that, you'll find maybe two pages or three pages about the technical details themselves for reviewing a paper. Usually for experienced guys, it's enough for us to go through the system overview and then we build our own ideas about this. I assume that this is not the case, so you have to go through the technical details sometimes to understand what's happened, and then after that we'll find there is an experimental evaluation, but usually experimental evaluation is divided into two parts experimental setup and then experimental results. And the experimental setup, they just describe what they did and what are the baselines that they are comparing against. Okay. And then in the experimental results, they are showing the results of two things. The first thing is that overview system or the overview solution in general, when we compare this with other solutions in general and usually the report one metric, maybe the execution time, maybe the scalability may be something, and then the want to come up with a conclusion. Like, for example, the system we are proposing is like ten times better than others. This is kind of the main experiments. And then after that we are trying to do some kind of micro benchmarking. Like we have some details in this solution. We are trying to change the parameters, we are trying to change the thing so that we can give you more details about what happens if I increase the number of processors, What happens if I shrink the number of processors? Well, the solution will be scaled up or not. And so for you, you don't need to go through the micro benchmarking results. This could be like a 2 to 3 pages or maybe two one half pages. You don't need to go through this if you don't wanna, like, deeply understand everything, it's fine for you. As long as you know the main benefit of the paper, which is the mechanics for getting it. So you get the conclusion how they are compared to others. So to summarize this, you go through the introduction of related work, spend as much as time you can have in the introduction. This is really important and usually people, when they write the paper, they spend so much time carefully drafting this, but they know that the readers will be reading this the first thing. And if I'm not convinced with the idea in the introduction, I will not continue the reading. You say like, I mean, this is the best thing and I'm not convinced the introduction, this is the first thing. So I'm not going to go through the digits. I already made my opinion. Okay, So it's been there, but I'm usually in the introduction. We use an easy language. They are not using very technical language. You will find that this is like a language like and maybe like that very plain language of English. No, it's not like I mean, having so much technical details while in the rest of the paper you'll find maybe some terms that you don't know, for example, maybe theoretical terms, analytical terms. But in the introduction, you will not face this in most of the case. Okay, So introduction related work, system overview and like maybe a little bit of details and the technical stuff and then the first part of the experiment. Okay, usually weaving the paper for who didn't do this to Now, I was reading the paper for the first time to grasp the idea from 2 to 4 hours to read this carefully and put some notes not just as giving it, but reading it as a story. Reading it was taking. Sometimes if you're really interested in this, you go and look up for some terms using Google or whatever. This also adds more time. Okay, So plan. If you have a review report, at least you need 2 to 4 hours to read the paper itself and then maybe grading on your writing is completed in one hour or 2 hours more for writing. So it's like more 6 hours on average, they write to stick to the guidelines that we had and that if you report like I mean, the number of sentences, the details and everything for evaluating this report, it's not like a01 evaluation. I know this is like kind of subjective thing. The only main message that I want to deliver to you and you should get out of this report is that you critically think about the stuff that to happen. You don't take everything for granted. Okay, You're not an undergrad anymore. You've got students. You are a certain phase doing this stuff. You're building the textbook or building the materials for the undergraduate. So you should build this. That's so we have here MIT. This is our idea. So you should reach them, reach them out like if you have any questions. Also, I'm assuming you posted quite. Yeah. And of course, like if you have any questions directly to me, you can send to me on the forum. It's fine. Okay. For that review presentations, I noticed that some of you didn't do that. Not reservation. Okay, so now we were supposed to start this today, but we discovered that based on the number, we still have some slots to start from the next week or like the next lecture, which is like in plain English. Okay, so if you still didn't do like the selection for your review presentation, you should do this and select from the possible source of the draft. Most probably the next class would be you want. Okay, so try to do this review presentations. We have also portion of your grades. So don't like, you know, underestimate. One final thing. I got a confirmation from our guest speaker that he will be presenting his work somehow. Maybe on February 30. Okay. So I know that we already scheduled some of the presentations with you on this date. So what we are going to do here is that we are going to shift this to another date later. I mentioned this earlier. I'm going to ship this later. But who planned to do this at this time? It will be more civilized for them to move it live. Okay. But I still have to figure out some logistics whether we are going to do this online or who's going to be here physically. So confirmation would be sent to you as the estimate. It gets to any questions so far before starting to materials. Okay. Okay. So let's start today's lecture. So it will be about equity, pipeline and materials. So today's class will be covering as usual, some material on the new advantages of assisted Ghost. And I hope that some of you also try to see the pages there. This would be helpful. I know Alibaba is looking so fast and somehow we embedding some jargon on technical stuff there. I'm trying to clean it up as much as I can and try to do like, I mean in a simpler way. But if you get a chance to go and see those videos, this will be a very good this will be giving you another you know, it's a discussion phase about what we have to get most of that most of that, like technical details that I'm having here in the first part of the course will be based on this course. So you can go there and see also this. And we'll be covering here this to make papers monitored to watch versus poll based waiting execution models. And as usual, we pick the references that should that should be that state of the art in this pass. There are many, many papers after that and many of the papers before that. But these papers are influential and inspire so many people after they publish this work. So what we are going to do here today, we're going to now focus on the execution. So in that last portion of the class that we cover, the storage layouts, all that indexing oil to be indexing, we dealt with the data. So we had the data. We thought how to store them physically on the storage device, whether it's in memory or Autodesk. And then we built some auxiliary data structure like indices without it helping us for updated data or read only data. Okay. So now the defense really and we tried to do as much as we can to improve their access when we need them for equity. Now, most of the parts of the class will be about how we do the execution, what are the problems that happened when we do the execution. I was hoping okay, and that personally I prefer this kind of but more than the data storage for two reasons. The first reason is that it has more action like a menu or it's more dynamic and it's still active area because that that progress that we have you in the hardware and the devices that we have here, it always opens a challenge for improving this. So whatever techniques you have now after one or two years from now can be outdated. So you still have to we think, okay, so it's kind of very rich for research. And the second reason is that once we do that, like the storage preparation for the data and fixing it, we don't have so much to do there. So we don't have like so many, like requirements to do over. So you just like somehow storage and whatever cookies you have after that, you can process this using you have. Okay, so what? Execution has a lot of things to do starting from optimizing that query, how to execute the query, how to resolve some issues regarding about concurrency, regarding that transaction management. So we have a lot of stuff to do there. It's not like just one program that's very simple to implement and you write it and you run it. It's very hard to do. So we're going to touch on some of the first concepts of how to do this execution in an efficient way. In the model, if it is hardware systems that we have. So first of all year we jumped a little bit in terms of the stack. So here we assume that we already got the plan from the query optimization, but we are going to cover the query optimization later in some parts of Texas. Okay. So we have here that we optimizer. You give it a query like what we have textbook, we select all from students and the give me the grades, right? Larger than 100 or so. We give them equity. Liquid Optimizer gives you a plan. And then we did this plan and now we start doing this plan. So we're going to start from this point to you how to optimize the plan in the first place. We're going to discuss the best. So now we have the plan and we're going to do some optimizations just to hit up to you. We are using a term here for query materialization. This is somehow we'll overriding the term because what we usually use the term of materialization and this is for view maintenance, like for view materialization, like in materialized views are that the use that you are saving some results from certain queries and you put them on the hardware on your device or whatever for future queries. What we are doing here is kind of materializing the intermediate outlook for during the processing. So do not confuse this with still materialize here, but in the intermediate step. Okay, this is just life ahead. But before we started here, we did already some query execution optimization in the lesson plan during that for example, that it to be or allowed indexing. We designed the index to skip the data, for example, that we are not going to use or for that to be to basically improve the access to the data that we are going to update or whatever. Like, I mean, give them access to. Okay, so we already this. So we have in our mind that some optimization techniques to improve the quality. Okay, but this is not the only part that we can improve the quality execution, but we still have the part that's very important for the quick execution, which is the CPU, the processing power that we have. We need to take into account the nature of this architecture of the system that we have so that we do the processing efficiency. Yeah, I did a skipping audit and Dixon can avoid the data that we don't need, but still for the data that we need, we can do better than what we had. Okay, so this is the main thing that we are going to focus. We have like a lot of optimization opportunities to do that. This is for the execution part, like that CPU, but not like for the data. So we already agreed that we already did what we can do for the data access and defense. Okay. Okay. So for example, I already have a query and the sentence format and to a set of instructions that the processor will be executing. Okay. So I can naively get this in instructions to be like, you know, not efficient or I can reduce the number of instructions that we can have. That's one way. And typically we do this using the compiler, but if you're using C++, you can just sometimes pass a parameter there or two or three or whatever optimization. The usually go there and see whatever hardware you have and whatever infrastructure that we can run this. And what how many memory, how much memory they have or what are the requirements that you are having as default and so on, and try to optimize the source code for you. At this point, you don't have so much to do in your hand. You just delegate this to the compiler, Although we are going to cover some parts of the compilation itself, how can we help the compiler to compile the query in a in a better way? But let's assume that we don't do the query compilation. We just let the compiler like, you know, do whatever they want to optimize the source. Another way is to reduce the cycles that we can have for each instruction. So the compiler now give us the minimum number of instructions that we have. But when we run them during the execution time, we think piece of problem. The problem comes from the efficiency of writing this instructions. So I run this instruction, but I take so much time to run each one of them. And this really doesn't depend on the compiler. It depends on what happens in the execution part. That's why the compiler doesn't have to do anything or cannot optimize this second part. Okay. And usually we have to maintain in the database management systems we really, really care about when we do processing through the city. As I told you before, fixing and Gantry. So in prevention, what we do here is that whenever we have like large amount of like memory slots that we want to access, we want we know that somehow I'm going to access all of there. So when I start exiting them one by one, I'm while I'm in the first one, I start prepared for accessing the second one. So I perfect it somehow I put it in the cache. Okay, that's usually what we do. And the queries, as I told you before, since we have large portions of the data that we have, we do usually this prediction and the scanning for large number of items. Okay. So that's one thing. The second thing here that your source code is already having some conditions like you have if statements, you have switched cases, you have different classes that you got. If you're running a program for a long time, what happens in the CPU level that we are trying to speculate? Basically, they say that okay, based on your whereas you usually go through like if I have this condition so that if ID is less than 100. Okay. And then if yes, you go to do something, if not, you go to do something else. Okay, so the CPU is having his own memory and observation. They usually see like a back flip. You see that you go through that first practice and most of the time, so as a matter of optimization, they take some stuff ahead and decision. Okay, in the future you still go through this. So what I'm going to do is that I'm going to go and go for like go for the future, like tuples or whatever there. And I start executing the steps that you have. You. But at the real execution of the table that you have here, let's say that this condition doesn't happen and you have to go here. But already Sibiu did some work with that and spend some time to do this work and also cache this output in the cache. So put some data here in the cache so that will be ready for you. Okay. As long as I'm in this branch, it's fine. I met everything so I use the cache and I like somehow optimize the time. But what happens if I missed this condition? so the CPU realized that he did a mistake. And at this point I have to roll back, so I have to go and remove that because I think have you from the cache and then I have to go and I all the instructions that they have here, I'm going to start doing this instructions from scratch again. Okay. This code branch mis prediction. So this is a non problem called the branch prediction. This is not for all even the basis for any program, but when you're using a machine learning program or any like extensively processing program, you're having this situation if you have a lot of it or switch cases and your prediction doesn't happen frequently as expected, then you have this branch and it's pretty good. So this is a problem in the branch. So what's the problem that can happen during the application? So if I have a data that they want that I expect that I'm going to access in the future. So I go and get this data. If the data is not ready for me at the lowest level of caching, I'm going to search for the next level. So for example, we had as you know, we have L1, we have L2, we have L3 levels of caches. What happens in the CPU, it goes first to search for the data and L1, but doesn't find it goes for L2 plus three and then goes for memory. And sometimes there are some techniques that speculate if it's on L1 to go and see the time and both search for the main memory does some kind of mix. But let's say that the default case that you're going hierarchy and whether it's there was this pre fixing steps if you data is always not in the cache and it's in the memory whenever I done some destruction I have to wait for a while to get the data from the memory and this called this time so that instruction is ready to be executed. But the data that will be used for executing this instruction is not stated. So the instruction stops, but I get the data, I catch it, and then I continue. So if this instruction is blocking to everyone else after that, so the whole program will be brought. Okay, So now here we have two main problems as I described, retention prevention missing. And we call this because installing more than the the instructions that we have and we have branch mis protection and it causes rolled back in for the instructions and doing a lot of the steps. Okay. So basically there's two problems. I think this second part, that reduction of the number of cycles for the instruction for only one instruction, because if we are doing pre fixing missing so I is bent so more cycles, more like many many cycles to wait till I get the data. So I spend some cycles you know wasted and here also I already wasted some cycles and the missed prediction when I speculated that this will go here and it doesn't happen. So I really wish that some type that could be useful for other instruction. Okay, So what we want to do here is that we optimize our database execution engine to avoid this kind of problem somehow to help our CPU utilize its like, okay, and most straightforward step that can come to the mind of any one of us is the parallel execution. So now you have a core you can buy to execute the quasi or more course and this is a quarter. Okay, so it is having only one. And if you provide me three, five, ten cores, then I should theoretically improve that performance time by a factor of this number of like if I have to cause I should improve the performance. Two times but three to be three times. Theoretically that that should be the case. But practically this is not the case. That's some overhead and the coordination between the battery force that can reduce this effect. But let's see that this is so this is the most simple one. Well, I'm going to cover this today because this is a straightforward and you'll find a lot of people doing that. We're going to also touch on it in the rest of lectures, like I mean, as a side thing. But that second one is the most important thing that we can touch on. So they ask any questions so far about what you are going to do. Okay. So before like going through the details here, it's like just recap on some terminologies, maybe you know about them. That's what execution plans. So I assume that we have this way. This is a skilled way and it's trying to join two tables and be based on the idea columns. So this is like the joint ticket that we have here. And also it's filters based on the value that we have the and date. And the value is that we have to you can be okay. This is kind of a plan that usually we have in coming from the optimizer. So we have these two tables and we do filtering based on the values. And then once we filter that the request that we need. So we need to do the time over by this and then we just get the columns that we need at the end by doing the projection. Yeah. Anyone doesn't know about this? Simple. I'm assuming that if you, if you took 45 or 45, you got somehow kind of two samples. Okay. One thing that you may not get introduced to is the pipeline notion. This is very important. So what we do here is that this is a logical plan for the query, right? What happens during the execution that the did you mess this plan into parts that can be running as a sequence of orders in parallel somehow or like the are not interfering each other as much as they can. Okay. So for example, if you if you look at a I can do the filtering based on the value here without needing to know anything about the other plan. Right. I just like to have this operator that can, you know, do the filtering for all that, getting them ready for any processing that happens after that. Okay. And also the same thing here. I can do the same thing for P after I do this election year for a produce a drawing, Usually we use a drawing algorithm. Let's say that we are using good measure on anyone knows about. You don't know. Yeah. So if you don't know about history, basically we have to means that we take one table, we build the hash people. This is a hash structure. You know how to do this on hashing designs. So I build it on that piece of one table. And once I'm ready with this table, I use the other table with the same T that we used there and the first one, the same type of T, and I brought this hash table to find the matching puppets. So we have to but I have to build the hash table one table and I have to probe the hash table using the second OC. So if we are going to use a hash joint, you were going to build the hash table you based on the output for example, here on end. And then once it's ready, we're going to use it during this joint operation with the key that we are filtering here from B to get that output of the joint, giving me the whole samples that are matching based on this. Okay. And then after that we do the projection, which is just selecting the columns that we need from the tables that we got that OC So if we build the hash table here on eight, we still don't need anything from B right? So we can do this filtering for the selection here and then we build the hash table and once we build the hash table here and we filter the sequence, all of the profits from B, we can use the hash triple here to rob it and then get the joint and then if the projects. So if you break this work into two parts, you can do it like this. Both Me I'm building the hash table and then filtering and robbing the hash table and then doing the projection, this kind of things. But which one can start first? Anyone can be getting it right. It should be. The first one becomes sort, but it will be locked together, right? It can start wanting this and this important. It's fine. But if I finish you earlier and this one still didn't finish, I'm going to be blocked because I need the hash table. Continue. Okay. So we call this part here a pipeline breaker. Okay, So we have pipelines and there are some points and executions, local pipeline breakers, because at this point, we cannot proceed until other participation. This pipeline doesn't have any breaker. It can finish without anything until building capacity. But this one will have a point by point. But that's why this will be dependent on this to reflect you what's next. So we have here that before operations and we break this into sequence operators we like group them into pipelines. As we showed you this operator, we see that for example here no end up you have your filtering gate. So let's say that the operator here is just scanning. They're reading them from the harvest or from the memory, like fetching them in the catch or whatever. And this will be filtering. So this will be kind of a filter operator and this would be a joint operator, this kind of operator has it has its own pipe, but it can be implemented in different ways. That's why we call it somehow. Operator Okay, because an operator and we can use different implementations on different parts of the operator. So logically we have, for example, as an operator, the and we can say that we have operator instance that for example works on that service to 100 records for purposes from 80 using a certain implementation can. And the second hundred companies may be using this can be, but all of them are instances of the operators. So we call it an operator and it can be working on that very ground like fine grained level, like the segments object, like we can chunk them and to say, well, why do we shrunk them into segments? Can anyone guess they can do it? That's one thing. Another thing, okay, is why do we do this? The coupling is working. We have to roll back, you know, because here this is a same operation and the data is independent from each other. Like, let's select the first 102nd, the second 100. And so on the hand, head of the right and also particularly if I have no more threats now I can assign trying to improve. Each one of them will be using one operator instance whatever the implementation there and then run them after we finish, we get the results again. Okay, So usually we segment the data and do this instance. Operator And this something for Paris. Okay, We call that sequence of more operators in here that we have like in each pipeline as a process. So usually we have a task. So the first year could be one operator, could be a whole pipeline would be bigger pipeline like this and so on. Okay. So this is kind of like this are the notions that we have. Okay. And you will find also some of the most popular is by blind because people used to have this pipeline. But sometimes we call it task. Okay. So this is a part of work that we can. Okay. So one important system that makes a difference and doing that analysis of equity execution for modern hardware is money. So money to be used by German people and usually German people are very, very skillful in building systems. They have like great systems coming from the database groups. They're in different universities. And that people that started wanted to be the even did some amazing after work after that we have like many many systems like hyper like we have this snowflake like incubation that we had before. So it's not like it's built by so many German there. Usually we have like so much people working on database management systems on a low level, so they are stuck with it. So this is one of the early system that we did some analysis inside the system for the modern hardware at this part at this time, I think it was around 2005, we started to have like more powerful memory, main memory structures, the structure and the schema that we have for organizing. The layout of the memory from a modular perspective was like, I mean, giving us so much so we can access that and write it, like use them and like in the system itself so you can optimize the memory access and how you divide it into the platform out and so on. As a software engineer like, you can access so many things. Okay, So they did some analysis there for how we can do what the execution and this modern hardware architecture, mainly in memory and using the most stressful workloads that we have, which is like the lab quiz and we are trying to focus on the very low level CPU instructions and we see what are the big bottlenecks that we have here so that we can, you know, change our way of designing the systems. Okay. And the this system is, is like was acquired after that by startups. Like I'm usually we don't have the system under the name wanted to be now as Victor Weiss and we even also changed it somehow to be Victor somehow. But bottom line here that this system is referred to or for many many people like I use it as a reference and most of the ideas there are inspired. So I really encourage you to read the paper will be a lot of details, but this will give you a lot more details about how the system will be implemented in an efficient way. So what we found there are two main bottlenecks then something dependance. If we have queries that can be broken down into instructions that are really depending on each other, then at this point we cannot do so much work because there are some dependencies. Okay. And the other part is that prediction that we talked about to you so that if you try to predict what branch the program will take, okay, and most of the time it will be like randomly happening. Like sometimes if you have like a workload that can shift backwards in a random way. And usually we take, for example, based on the ID, if it's less than 100 or not and the data is not sorted. So at one time I can find the data which was ID like in less than 100, and the second record would be more than 100. But if the data is sorted somehow I can do more like more robust prediction. Okay. Because now I know that the first 100, they are going to be like, I mean less than 100. So I usually go for the prediction where after that I usually take the other side. Okay. But if the data is random, the quiz that you have a year a random, then this would be a problem because now you were having randomness in your work. So this is confirmed by their analysis and they say that the effect of like throwing away the work that you did due to the Benjamin's prediction is really heavy. Okay. We have also some analysis and some experiment evaluation result that would be interesting. If you do this, you see that the performance gain can be dropped like significantly, like less like less than ten times the one. Okay, so this will be useful. So you have to avoid this kind of branch prediction thing. Okay, they already cover this. So one thing that we can get by the conclusion from this paper is that in the elapsing, usually we do have we felt that what we did in the filter on based on the value of the idea or whatever. So this is the most executed branch prediction happening in all lab workload and even whatever what they did, they're still very hard to predict this correctly because it depends on the nature of the workload, not on that, but because that you have. Okay, whether the data is sorted or not, it doesn't have anything to do with the workload, whether it's random or not. Yes. So it's still a problem. So in order to solve this problem, you shouldn't think of how to improve the branch prediction, but you should think of avoiding branch prediction. How can we avoid branch prediction? What is branch prediction happening if we have if right or switch case? Can we get trade offs, right? Yeah, right. I try hard as much as I can to rewrite my operator implementations to avoid if. Okay. So that's what they come up as a conclusion. Will this be a 100% domination for all the operations? No, but I try as much as I can, at least for the heavy operators like this can, because I do scanning or scanning I use can have 1 billion tickets. So I can look at this data, Can we write This can operate to be much better by avoiding this performance. And let's take this example you like they say that you can do this. So make the old form a table and you take the exact value that you are checking based on like log like that, larger than a certain value or like between low and large values going high values. So it's like a chicken. If the cheese is in a certain range, what can we do? This is the typical thing that we can do in any skill our system. This is like our straightforward implementation because it do implement Operator That stands for a key between arrays, but like between the two values in arrays, I spend the whole company in the table identity and then I use in this branch. If yes, I return, turn it as an output for me. Okay. And then I look for the next record and prepare that place in the buffer for the next day. The tool like the patient about this. The first one you look I do that for each couple so I have 50 50% having dismissed prediction. Okay. And also I do that copying you only if I succeeded in this condition right. This will inherit all the implementations that we talk about in the prediction. If that prediction here is happening is wrong and for example, it's not satisfying the key, then the whole copy that I did, the everything that I do will be lost. Okay, I have note that this is another implementation that could be surprisingly much better than what we have, although after I go through with like you can think of it, this doesn't make sense. So here I go through each topic, I copy it anyway, I do that as an output, and then after that I check if it's and the key that I have, you know, the chicken little chicken, let's say logical checking. Okay. If it's yes, I leave it and then I that's my point to go to the next slot so that for the next record would be checked or next topic to be checked to be in this new slot. Okay. If don't, I'll leave my point as it so that the next topic will be overriding what I'm writing them. Okay, so let's say I start from the first couple. I have output to you. This like this is about the buffer. It's the output I have to you want. Okay, so I took the one I think you want and I put it to you at index zero. And then I check using this very like efficient shift operations or psychological operations. It's nothing if this is just a numerical calculation, it is here. It's just me. It was a to zero one and then taking the end of this operation. So basically, if it's in the range, it will return one and one. Q And then take the and, and the delta will be one. Then this means that I already put this in the right place, advance the pointer by one for the next slot. So this is not in the range this way be zero because one of them at least would be zero. Then the advancement will be zero, then the pointer will be still zero in the next couple will be written if it's already matching this you. So I didn't return any incorrect out. Right. I overwrite it. Okay How is this efficient in any one thing? Okay. First like is this a picture you don't like. I mean. I mean logically, what's one drawback to you for this compared to this? Or like a me straight forward then copying. I'm copying every single one. I'm copying only if it matches. Yeah, I'm copying every single couple. Okay. So that's, that's the counterintuitive, right, Right. I copy everything. So let's say like I have 1 billion and only the maximum. These are 100 and the first one I'm only copying 100. But here I'm copying the 1 billion. Right? This is a problem. What happens at the system level is that the copy overhead is already hidden by the prevention. I'm copying the next one while I'm checking the first. Okay, so I'm not long. I'm still working on the first one while the second one is right. So I'm not wasting titles. I'm utilizing them. Okay, so still the overhead is manageable, but at this point I avoided dismiss protection, right? I utilize that prevention thing for avoiding damage predictions like those. Now that compiler will know that at each table I'm going to do this copying so it will prevent everything. And there is no problem here because I am for sure having the next stop to be copied. So there's no waste cycles. Okay. But at the same point, if I have any missed prediction, I'm avoiding. Okay, so surprisingly this will be much, much better. I'm more efficient for 90% of the cases. What's the only case that this will be inefficient? The anyone? This one? Yeah. Or like very, very few. But I'm really actually having, you know, in the range. It's still like I mean I returned only ten couples out of compliance so in this case definitely the ultimate I have for best prediction for ten couples still much much better than going for billion but let's say it's 1 million and return now there is a significant overlap for protection and the prediction would be identical. Okay. So again, as we always have as a conclusion from all the path that we have, there is a tradeoff visually designs into the pages and any system has attributes. So you as an engineer, you'll have to put your special, for example, if you're an operator, implementer, and you know that this can operate, would be returning certain number of copies beyond the memory budget that you have there. You can switch this to the next case or you can use this branch implement. Okay, Anyone have like as a question So you send a copy of be prepared. Yeah, but it depends on the media. But I is not know what. those all here will be advanced anyway by then. We don't know that. It depends. Yeah. The pre fixing will be happening anyway. And then in the case that we are correcting something wrong we are going to look at but you go to see you are right but here we know that this is not happening right so so we are is actually items on the same visible copy. Okay the only thing happening is if we have a zero or not. Right. So I'm copying this and the same place. I only have a shift for me. If you want to look at the simplest like an existing by only one, the fixing despite only one. And it's just having this as a copy in a certain in a certain place. And this would be most predicted 100 points or maybe 1000. This would be predicted in a level on a level list than this one. So it's it can happen also as a missed prediction, but this will not be noticed because the all that we have to build can be also predicted from the keys that we have. Okay. But this like once certain 100% certainty of if it chips at the level of each stop, this will be a problem. Okay. Okay. So what happens in an impulse case cases if we have like large number of switches and if but usually we have in the database operation different implementations for different attacks. Let's say I'm implementing I can operate on a column level. So in the column level we can have the column as integer, we can have the column a slot. We don't have the column as character or string or whatever. Usually we are not using the same implementation for each one of them because, you know, we can optimize the byte alignments, the implementation, whatever that we have, and then we, we do whatever we need in each type of operator implementation. Okay. So we have a lot of cases in the in the database implementation that you cannot avoid the if or switches. Okay. But in this first one you are increasing the probability of having more FS in the second one. You still using this if and switches only on the level of the operator type and then within the operator type you can do this branch with implementation. Okay. So still, even if you have like so many switches that can switch that different data types, you still have more efficient implementation using differentials. One. Okay. Yeah. That's regarding the reason I have been since the days of the living the so that's why do this or are you saying that I also like to do that for opportunistically but yeah no. So here we are with fixing this in the catch I this in the case of the data in the case yeah we cannot like we copy this when we do that but the data will be ready for us when we presented. The question is the thing that happens here like you are we do I totally did the same thing, but here we are not doing this prediction only if if we are inside this right. And also we have this problem that rolling back because here we are not just predicting when we should be going to implement this. I like this and this and this and this until we gender prediction specifically, we didn't this is like happening by data compiled by by the computer itself. So basically you had its own heuristics. They keep, for example, something like like with counters for each branch. See that? Okay, after ten times you really went through this branch for eight times, then most probably you're going to go through with. So I have to go down to do this, right? So it's not like I'm in a very spot with and we cannot access this because this is implemented on a hardware level, like using this like, you know, using hardware implementation so that it can be okay in order to do that, have to overwrite this implemented like you have to provide a new hardware for the for the database. There are some companies doing this like, I mean they are coupling them like the software and hardware development for database. But this is really, really, really corner case. This is not what people does usually in this case happens in implementing machine learning applications usually have this customized chips using GPUs for certain unit of design, whatever, because it's really expensive in databases. We don't go through this level. We try to decouple the software development from the hardware without the growth of the what is it? These is restrictions. That's pretty much the same across. Yeah, yeah, yeah, yeah. Happening across every any questions. Okay. We can take 3 to 4 minutes break and then go back to the second part of the email within the last week. Yeah. So I'm going to give you a course one of the right. Yeah. So I need to find the gold. Yeah. So what do we have over here in general? I can see. I mean, you can find that rule on your own if you are someone that you want to work with or if you don't find anyone, you can. I mean, if I want to do my job in beta and send them an email and could I have all that? Because I notice that the assignment this and it's not that due date is not today. In 2020, we delayed that person report on June 25. So what would be the temptations? This is not the percentage of this is 14.30. But there is a report I'm going as a presentation. So that report will be having a deadline in place. And because you have to write a summary and evaluation, as I mentioned in the beginning. So there is another type of notes for that you have to present at a certain time. And you can also speak about the time slots and choose a presentation and then you present it would be like a presentation between 15 to 20 minutes. And one of the leaders of, okay, we're going to make like three organized one one. So, so like you said in the guest lecture on 13th of every year. So that is one. But presentation. So are you sure that the lecture will be on 13 and then two, how is it going to be? Yesterday I received a message from him that see that this is the most suitable date for that. But I just that's why I always get it. But if this is the case, this happening, your presentation would be in the next lecture and the next lecture that people would be presenting in this different which I would be warming up to prevent. So it would be shifted by your thank you so nicely and the prefect in resolving whenever you mean prefect we just mean we are looping things into the cache and potentially. Yeah. So is no right of execution of construction execution. No, no, no. We're just switching back into the question. The most important thing, because I need to have the data of it like yesterday. I can write them and be fair enough. So that's the so in that case, if we have a branch implementation and we just let's say we ignore the branch and we prefer every so again in the punch list, what we're doing there, execution for every row, and then we are just like presenting we we're preventing every the correct. So I see do that in the crunching one and then where does why do we need some kind of rollback back. Because if it's just getting data into the columns, then we can ignore it every time. But that missed prediction is the rolling back is not for the intention is for the execution was in the missed prediction we actually execute like this not like so the copy and then we go and like increment this one. And if there are some other next steps after we do it, the now if there is a missed prediction happening, all of the work that you did, we did. And you have to put through in a way because this would be inconsistent with this is not the case. So this extra work that we're doing, this is not prevention. No, not, not this refactoring is this particular application. And then branching is actually running, running the things inside the branch and then again, turning it into something different. That. Exactly. Yes. Thank you, Professor. So in this case, like because in case from just any business, it would be okay, like we could have conditions and there's only one NGO but it is I'll switch that. I'm like, yeah, zero one. Right, right. So in that case, however, something like this, there are some implementations using shift operations and the chicken operation is doing this. Usually we do this with a bitmap, not only one because one one bit can choose between two options. If you have like two bits, you can choose between four. You have three, you can choose between eight, right? So you can usually use this bitmap to select between that and based on the number that is the like. Again, all the data, the instructions are really different because in this case it feels like, okay, for me that is relevant. But if the instructions are really different but say zero reduction, which is I think, you mean okay, so inside the prediction of themselves. Yeah. So it's like zero, right? Right, right. If come back, Right. Yeah. So in this case, so usually in this case we use something called likely predict. And so it's kind of a specific implementation to work around this kind of cases. But if we find that the case will be extensively like what you mentioned, we cannot avoid this and we have to switch this. If this is applied in this kind of optimization for something that you want to optimize. DBM So yeah, and that's exactly what's going on. So strict. So moving from the sections, can we do without on should everything being want. So the previous divided actually was to push it forward. You hand this to my with I have some money but I just find this is this should be fine so like supplied for as long as it has enough details inside it. Yeah. So I just tried it out regarding the right. Now that's one option if you tell this to go back. So now we just describe this kind of main problems that we have on the city level. Okay. And we provided a possible solution that could be like, you know, helping a lot in many cases inspired by the work of Monte Bibi. Now we're going through the main processing models that we are using to implement this. Okay. So what we did, the trick that we did there is kind of implemented in all of them. You know, any kind of processing model. We can use this kind of stuff. You can be aware of writing your code inside the operation to be branchless or to avoid other kind of problems that can be happening on the hardware. Okay, But now let's think of how we implement this blend that I have to do. Like the two pipelines that we that we described in the beginning, how we usually implement them in the. So usually we have different processing models. We call them either execution models or processing models. This is the way how we implement each pipeline or each plan that we have three main like operation, like the approaches and they are kind of similar to the same way that we did in the storage in the house. We have one extreme. For example, in the first now for the road, we have one extreme for the columnar and then we have a hybrid thing. This is similar to that. So we have an approach for iterating over each table and we have an approach for going for more couples together, like I mean finishing everything and then moving forward. And we have a hydrants like, I mean we can batch like I mean we can break down all the output that we have into batch and then start like, you know, in making these steps. So I'm going to describe each one of them in detail. But I just want to like I mean, give you the link between them. Like each one of them in the first two is kind of an extreme, and the third one is kind of an optimization look. I mean, the high grade design and typically we use this height like some in most of the case. Okay. But you have to know, look, all the cases can the hybrids will work, as we can see better in most of the case, if we don't know the nature of the workload, whether it's going to be a mixed workload or it could be an older pillar, you have to go for the average. Okay. So the first one is the single model is the very earliest one. Now I've got to describe it. It's also the pipeline model or the volcano. There's one of like very popular optimizer called the Volcano Optimizer. If you go there and the search engine and you search for Volcano Optimizer, you will find a paper about this, one of the very earliest papers about this. And it's really implemented in almost all the dependencies and especially the Postgres one. Most of this is having, you know, for that router model. Here we have the plant operator implements something called mixed function. Okay. So in next function, this simple based are like each operator only cares about its output, its inputs, so it needs the input. Okay. So it asks the other operators that this operate like this parent or right. But let's say that make it more clear. Let's say we have this plan. Okay. So if I'm here at the Roots, all I'm caring about here is getting my inputs. I know what I'm going to do here. Let's say I do a projection. So whatever input I'm getting from here, from my children, I use this for the projects. I do the So I don't care how this happens, like how this would happen. I just call a function next to get me, next input for me and then I continue working and this would be recursively defined view. So this one which is doing somehow selection for example, only cares about its inputs. So it calls next for its children. And so then we take the leaf so it doesn't have an X, so it has only the inputs there. Okay. So you think of this as a pull based approach. Each one is pulling the information that it needs from the children and we don't need to know how does this happen to me as an operator? I don't know what happens in my children. I just ask them to give me the mixed input for me. Okay, so each application here we return either a single topic or another. Like, for example, I keep getting my input, keeping my input to this children, this child here, not all of them. Let's say the left child says that, okay, I'm done. So I get them all. So if I get them out here, this means that I shouldn't invoke mixed, okay? And this would be recursively defined. Okay. And this was, I think, what would be happening recursively. And we somehow have a blocking nature here because each operator we have here is depending on its children. So if this one is blocked, this one would be blocked. If this one is blocked, this one this will be blocked. This one would be blocked. Okay. So we have a blocking metric. Okay. So an example for for what happens here. So this is the same query that this is a similar query. Let's say that this is like we going to table bottom. This is based on the idea, but here I'm using only one condition for filtering on this. So a plan would be like this. I think in this relation or I don't filter on it as I built on it based on the value 100 here. Okay. And then all will be used for building the hash table because I'm doing a joint here. So we mentioned that we have to build a hash table based on one of them. So let's say that we are building that has to will based on our and then after we do this election year, the joint would be using this hash table was the filter topics that we have process to probe the hash table. And then after that we are turning this to the projector, which is the root here. And then we are filtering based on the idea and that creates or whatever they have you to column based, for example, to return. Okay, this is a plan. How can we implement them as operators? So we have the you can operator or are we have a special operator press, we have a filter or select operator for this. We have a joint operator, we have a project operator. So we have different types of operators. This is kind of a sort of code for each one, but okay, so you see here, one of them is like they out to a certain implementation. This is the kind of a pseudocode for how we implement there would be here two types of arms as you see on the left side. Control flow and in control flow. I'm going to show you and control flow. This shows how the function calls are called. So which one is calling? Which one? Which one is depending on which one. The red one would be the data. So it will show how in response to the control flow data will be flowing. So I ask you for data, then you return. So there is a control flow for calling the functions. There's a data flow for returning that based on the function calls. So let's see what happens here. But before, before going through the flow, let's go through each box of them and make sure that you understand what's happened, what's happening. So in our view, this is a scan. So as you can see here, what it's tapping in or what we do is that we read it and we send it to who else after me, because I'm Justice Kenny, Right. So this is called elements, okay? And here and as we do the same thing, but for us and here for the select this is like a filter operator. Whatever I get from my type. Okay, I'm asking a big type when I'm asking it, give me the amulet for me. Okay. I do the prediction evaluation. So the predictive evaluation, which is typically based on this 100, whether it's more than this or not, if yes, I can make I send it to whatever it's after. If not, I'm not okay. And of course, you didn't say this. We can do a price list implementation, for example. But I'm showing you that branch implemented. Okay, it's fine. So as you can see, what you can do here is if you are here for the time, we have two things. We have to build the hash table and we have to province, so we have to close. The first one here is only depending on the things I'm coming from. This type, I like to type left at next, so I'm asking it to get me all the inputs I need to build the hash cable. So I build the hash table here at one. Once I finish it, I'm asking my right type to give me the inputs for me to use for grabbing the hash table based on this key tool. If there's a match, I make the house with you. I'm going to. Okay. And then the root here would be doing a simple logic. Say that for my client. Give me the next whatever you are having for this hash, join you to me so that I use a prediction and then I return that. Okay, so the logic is clear, right? What happens here if I start doing the processing here, I start something from here. Okay, This will be triggering the processing. So the root will say, give me the outputs, give me the input that I will reject the data for. So if we go there, this is the only time that we have. So it will be looking for this next year. Okay. But this operator, to get to the ultimate, the input that you need here needs to have to next to be called the first one here from the left. So now this would be recursively calling this and we wait till we finish everything here in table because this is a by line breaker if remember. Right. So we cannot proceed with this one because this one that but something like the output that input here from this right child will not receive any processing until we build the completely from all that input that we have here. So I will wait till I finished everything here based on that. But I'm returning this as a single proposal. So each table I'm asking for the next give me the next, next. So I finished everything, but this will be happening. So I finish everything and then I have. And then after that I go here and then it starts getting the input from that. Right? So here I'm asking for next. It is appropriate. So this will be going here and then recursively to get my mixed here for you, I need to get the next from this one. So this will be calling this once I get on top of here I images and this would be evaluated and in memory. There is a difference between what happened here and what's happened here. What happens here. I state in this do do I finish everything because I need all the inputs to build the housekeeping. But here I don't need to do that. What I'm doing here is that I'm asking for the first. Next you give me the first, next I return it to you and this will return to you here. At this time when I'm returning this, this can start asking for the next from that child. Okay, So that's how by blaming happening. By blaming means that I can proceed in my sequence of operations as long as I'm not blocked here. I'm using this on a couple minutes. Okay. So for each table on this pipeline, after I get it, I can make it to the next operator and I can start like getting the second one and so on this one and I'll do this. I have to be blocked like everything. And it has to. Okay, Anyone has any problem with this or like any question. So this is happens in the Isolator so it's both best I start from the root. It will trigger a sequence of mixed calls. I get all the input that I need. If there is a point that I have to block to get all the information needed to build the hash structure or whatever the structure there, I'll do that. After I do that, I continue this pipeline again. So yeah. Yeah. But assuming that the bags are Yeah, yeah. But they're in this case, I was just thinking that since we have a printer on rice, I know what you are going to do. So you're going to some. Yeah, that's the question here. I'm assuming that this is like the optimized logical plan. I'm just like showing you how we executed on that, like on the CPU or like on the execution. Okay, How we do that, of course, we're going to see also in other models, like I mean, what, what's your name? Shashank session. So, so what, what you mentioned here is actually what happens in the real time. Like we don't use this kind of my implementation. We are optimizing this on different levels. For example, we either combine this together because I don't need to go and ask for mixed and then you return to me and then I feel good. I mean, I can get it and filter it and return to right. That's one thing. So that's what's called predicted bus. So if you remember in 585, I don't know if you did this or not or 45 that one optimization and the plan say that you push the predicates to the leave, then you do the filter. So this is what we mean by pushing the pretty really. So you go there and you do that filtering as much as you can at the least, so that you don't need to like to maybe move the whole couple or whole pretty among different nodes. What's the advantage and what's the problem of this? And you want this. So now you know how it works. Yeah. What is the bottom substitution value of the mean? Okay, so how is this problem? Well, I mean, like severity, like blocking certain parts. Okay. Yeah. So blocking is one of the things. Okay. Anything else? One problem is that we don't get as much as you potentially could by Right. That's another thing. Right. That's thing. But there are one more obvious issue you can guess it depends on the how many number of by plane or people we don't address locker next we have let's say that we don't have a pipeline breaker let's say like we don't have a bottleneck all that let's say that we have only one pipeline this one say that it's only right there a problem even with this situation, don't reach. What's the problem actually, can anyone see as an advantage of this top down approach? Actually being a top down approach is one advantage. But how anyone does so in top down approach, I have a control what I'm calling for it. So let's say that this project say that did me that top ten. So I send mixed mixed next. So can I get ten and then I stop and no one would be doing it's next because each node will be triggered from the above. So if the root is controlling everything, it's like, you know, you have a controller there. So if it's controlling everything, then it's fine. And there is a usually and in is limit query. You know this limit query usually we have this limit. So this, this limit query can be easily artificially implemented here, but we control this from the top. But what are the problems? That's one obvious challenge. You try to think as a system developer, yeah, it's that equilibrium, right function cause also you see to get one problem I have next goal of a next goal of a next goal, I'm going to school it say that this is like this is only for one type of I have 1 billion of So how many calls do I need? Many, many calls. Okay. So in terms of low level implementation, this is not that efficient on the database correctness. Yes, it's fine and it can give me a logical control, as I mentioned. But in terms of that, there are many too many operational second to second operation. This is modification. Okay. There is one more advantage to this one as a developer and think of it other than this limit is more proficiency thing as a developer, a better benefit of this model, The usability of right. So I don't need to know what's my children nature. I'm having an interfacing give me the next and whatever time there will implement. It's mixed on the logic. So I focus only on my operator logic and then the input will be proxy by mixed call function. Okay. So there is an advantage of having this function calls because this will be kind of an approximation for me. Okay. But at the same time, there's an overhead for having I'm calling it for digital. Okay. So if you get this this is a volcano model. This is has been this for a while and people use it and still using. So even with the drawbacks you systems. Okay. Now we switch to the second model, which is the materialization, the materialization model. We do the same thing here, but in a different way. We say that we when we need a table, we go and next, next, next, next. And then we return it and so on. But this is as we told that we have one function called for each table. It's not like one function could operate double, so this would be so much. So what we can do here is that we say, okay, you do one function, call for all the inputs. So what you do here is that when you say next, I'm expecting to return me an array with one type of okay. And then once you ready, you intended to be okay, obviously this can be fine in all ultimate because in order to be, as we mentioned, we don't do this large or large processing like that. So when you when you do this material like this and you get all the end with once for each operation, it's fine. Because usually when I return, return two to 3 to 5 to, ten records, something like this. But if I'm returning 100 or 1 million, let's say, records in all lab, then I don't have space to store this intermediate outputs. I'm still didn't finish. I still didn't finish everything. I this intermediate to be in the memory right for the next operator and so on. So I need all storage for each query. Right. Because this intermediate outputs will be huge in this case. So this is not good formula, but this is good for both of you. So what happens here for the simple way that we have here, we can have an implementation right now for each of the tribes here. I'm returning that, so I get to the output as one array once. So here the array would be returned as a buffer. Okay, So I go here, I have the same logic. I look myself here on this table. This at this point anyway, will be the same. But here, instead of having this loop for each table and making here all the functions once. So when I see that, get me the next. If we're sitting here for a while inside this reading, reading, reading, reading and having them all one, all of them in one array, and then returning all patterns to you. So I have only one function called here, and then I do the hash table and then when I go there and this one I do the same thing. It the next from here, this would be for one. And then I will hear it mixing. So to get the next I need to be blocked. Right. But I go here, I finish everything I do, blue flow, then get on the top and then return it once a year and then loop and then return it once and then do that probing and then do this. Okay, so what's the problem here? Okay. Locking right. If blocking. Let's take for example, if the probe statement takes time, so run. But one couple and if the well predicate takes time to run for one, then it's a sequential one. You have you're consuming X amount of things like, All right, and then I'm going to write this, right. So this is a game is what we wanted to have in the beginning for reducing the starts, for example, and this waiting time, okay, we are not fully utilizing the CPU model that we outlawed, the CPU resources that we have. We are waiting, waiting, waiting till all the would be with. Another disadvantage is the need for intermediate storage to store this. Okay. If we have like so many queries running at the same time, all of them doing the same processing roughly all of them like requires one gigabyte for example at an intermediate storage, then I cannot afford like I mean 100. You have quit of the sample. Okay, so this would be a problem. Okay. But we have this advantage of, you know, at reducing the function call overhead. Assume that we are becoming large enough memory and we have a lot of enough cash and we are using this for all. TB. So the number of vehicles that are returning is still small enough. Then we are avoiding all that functional overhead that we need and this would be okay. So that's a wide. yeah. So one optimization that we can get here is that there's something called M lining, but this is like model implementation practice. And instead of having buffers to be written right away, like in, like in the memory or whatever, you have buffer like a, you write that there is a facility that you can use, that you can do this in lightning. So basically it's not like making everything go in memory, it's writing it onto a temporary structure and then will be forwarding this the other. So instead of having committing everything in the intermediate step to the memory, so like I go and make sure that I go to make sure it's I know split in like in temporary storage and then it would be like, I mean soon enough this kind of implementation, but optimization, but it's not reliable. People can easily make a like throw away the stuff that we have if that is like, you know, any problem having, having crashing into software or whatever. So all the information is also that you have will be gone. So that's, that's a problem, but it's kind of an implementation optimization that if you prefer to do it, you can called and like you just directly have an output buffer instead of E and it's just writing this. Okay, So obviously here we have to go for a solution that can makes this to so. Right. So what we do here is that each operator will still have a function called but still not meet like not needing to wait for all the topics to be returned at once. I can return them in batches. Let's say that I have an intermediate result of 1 million. Let's say return me 100 by one hundredths. Okay. So whenever I have 100, I start pushing them so that we can, you know, flow and you do this emission because the whole thing and the pipeline, if it works while I'm preparing the next 100 and so so this is that victimization and this usually implemented to interact with the properties so we have a lot of people is now using this but the crucial part here is how you determine the batch of sites, right? So this will be based on the hardware back to us. So you have to figure out the suitable batch of size based on your hardware. And that's why this parameter can be changing from one hardware to another. Like if you run your database on a certain cloud service you use for example batch side of 100 on another cloud service, you can use 1000. For that you have to optimize. And usually we have this protocol database tuning parameters. So usually database was responsible for this was always the one creating the database. And like running is a base management system and we usually call it database administrator. So this is the database who's not the typical user. Usually the one who runs the database is responsible for you as a user. Don't know what's happening. Do you just issue record one optimization? If you if you watch it die recorded lecture in the lab, we talked about that single instruction, multiple data thing to send an instruction. So basically on the hardware level, if you have multiple data that are doing the same operation, Okay, so there are some facilities that you can victimize the data assistant in the system representation, and then you issue one an instruction that comes on all of them impact. Okay. So this is very suitable for the simple instructions. We call it like victimization. That's why it's called victimization, because we usually use this was symptom as such. So this is like kind of the same example here. And as you can see here, we are checking the size of the banks. So here I say give me the mixed. Okay, then why I'm not getting all the reasons that I need in this batch size. I don't do anything. I just admit when I have this, like enough exercise, if I So I do the same thing. So you find that this is similar to all of you. I'm preparing for being a double back, so I keep looking until I have enough space and then I return it. And then after that I keep looping you to prepare the next batch for you. And then I like, return it to you once. Okay, so any question so far in a question. Okay, So the three things that we have described to you, the three processing models, literature, the materialization and the visualization, all of them are open. So I start from the root. I call whatever I need, and then this will trigger all the children recursively. Okay? Okay This is not the only way to do this execution. Well, that actually there is a push based approach and push based approach. You're like as a child, you're preparing your inputs and then you push it whenever it's ready. Okay. And it's kind of counterintuitive for anyone to see that. Okay. So if I if I don't need this and operate like on the root level, why do I need to persist? Okay, But there are some cases and even in the design, which is more efficient as I would be. So in this case, it assumes that we have the same two pipelines that we talked about. Since I'm having control on generating the input from the Leaf, so I need to be aware from the Eskom statement itself, how can I, like, you know, break down the operations, right? So how can I group them together? And obviously I can do all of this together in one operator and this one would be by itself in one operator. So because I'm, I know everything about this concrete, I have it as an input and I have a control to prepare the input from leaves until to you and the next steps. So if I have this ability, if I can run operators as much as I can together without, this function calls between them, I can do that. So for example, I completely can run this myself and this one can be running based on what I have here. So I can group all the steps together, just waiting for one input coming from this pipeline so I can group this way and to to pipelines. And it's it looks like having many operators have what, like two big operators and each one of this operators would be having to sequence all operations inside. So in this case, I have this information for the first pipeline, which is a simple one, an arm like I now I traced over them all and I know that I have the control and the input, so I can also, based on the information that I have from basically whenever I get a couple there, I inserted in the hash table and to get the has table ready for you to be used by the opponents. So here I compiled like for example, in this joint I break down that build the highest table to be here and I keep the problem. So here I can point to this whenever I have local I don't emit, I build hash and once it's ready I return the has to do. Okay. The second thing here similar. So now I know that I'm going to reach double. I'm going to take it to value. Once it's ready, I'm going to use hash table here and then do the project. So the only input that I need at this point for this type of work is the house, right? But I have everything given they can do the internet. So What I read is that it's double. I do the prediction, but I predict evaluation and then probe has table that I have. It's ready and then I make that like this to the project like that I can do even the prediction on the same one or I can split it in the role. But let's say for efficiency, I can do the projection and then return the results. Okay, so what's that benefit and this advantage of this think much easier to run in parallel activities if I'm right. That's the thing. No function overhead as much as we can view and even don't have so much things to materialize in between because I don't need to materialize the data. Whatever I read, I just build a vegetable based on it. And here, whatever I need by chickens, I do the protest. I don't need to, you know, the same thing. I feel I do the projection and then return it. So no materialization overhead. It's minimum as much as I can, like like capitalization and the pipeline breaking. Also, there is no overhead to you. There is one complication here. What is it is that I send back? Let's say that it's not if it's a limit coding, for example, and then returning this ten or like can the first thing like publish let's find this. Yeah. So this not be a problem. If there's a limit you can do here much work then but not necessarily need it other than this. Yeah, this will be fine. But there is one complication as a software developer identifying which pipelines, right? Yeah. So you see in the first one that I tried to with like a materialization one, I have this operator like interface having this next, I just implement my logic and then whatever I have from the children I will next look here. To avoid this next I need to be able to group things together. So here one operator is kind of the implementation of one operator is kind of encompassing. All the different operators have political operators and if there is another way which is having like I different plan, I should generate another implementation, right? It's like I have another player like it say we have we don't have that select. So this implementation should have this right. How can I do this? Obviously I'm not going to open my source code. It's very I'm I'm going to do this right, Right. Something like this just in time company. So basically I'm deferring and delegating everything to the query compiler. So I have here a quick compiler. By this kind of model, I need a quick compiler, something that can take that way and understand what are pipelines that would be full and generates this implementation on the fly. Okay, so this kind of query comparison is common, right? Because it gives you an optimization opportunity that you are optimizing every single implementation for everyone, especially if you have workload templates. If you have like different queries, like if you have like one workload with so many similar. So I can generate one work like one query template for them and they can use it across the whole world. Okay. So this, this approach is very efficient as long as I have the ability to compile to generate it and now it's right. This trend is not common in the old days, but it's writing them. People used to have this now since like people now started to be like compiling concepts from operating system, from compiler, from a picture. So database now thought to be a more complete stack in 2010. So now we have a compiler like a programing language, we have tabular, we have the cute that we have like this. We have so many things like so that's that was based like without any question. So part. So to recap here the pros and cons of this position direction whether it's to the bottom or bottom to the bottom. So it's easy to control here for the top bottom, right. I to mention it, we of course, we have a blocking until we return everything with a couple all attacks. If are using this victimization thing, there is an additional overhead for the next function because we separate the implementation for different operators. And there's also branching and here on this next application. So there is pressure actually because sometimes in the next when I'm getting like this from two different looks like more than two children and we have like the logic that we have to check between different operators, which one I'm running now. So now I have to go through FS as well because now it's not optimized for combining everything together. So it has some time operability of branch equals. But on the other side or the bottom from approach, we have Python control, like when the cache is we just assume because we have like everything in pipeline. So we have like everything like, I mean together on the same path. So whatever I get from the memory or from the this, I do all the operations once on top of this and they get a little bit. Of course it's not optimized this limit, as we mentioned before, but it's also hard to implement for all the operators you have. You cannot do this by yourself. You have to have an compiler to generate this. Okay. So just to recap what you have here, we had different execution models. We have different processing directions. Each one of them has its own pros and cons in you as a system builder, you have to think which one is the right for me and this is not my one solution that can fit for everyone. So you have to be careful when we do this. Okay? The details of this approach is are the people that I referred to you and I strongly recommend if you're interested in knowing more details, you go there, people. It's interesting with because the class would be about scaling and we are going to be assuming that we're going to start the representation next class way. Well, not not not yet For people who didn't scheduled their review presentations. And this the only time slots that they can choose from, but they are forced to do that. Okay. Be careful. I mean, it's a it's a part of like one. So if we don't if we have some people presenting next, we should be fine. So we're going to spend less time in the class going through this and we're going to solve this. If, if not, we're going to spend more on the quiz questions. Okay. So you guys next class. So.Good morning, guys. So before we start our class today, just a reminder that you did for the first report right? We are going to make the deadline for this deal. 11:30 p.m.. So you still have time? Still no IBM, as we mentioned. So we're done with this will be that record in due time that we have every at the end of the day, around 1130. I hope that you managed to work on this report. I know it's hard in that first one. After that, it will be easier because you are going to have your template or, you know, exciting the world how to find pros and cons, stuff like that. So it will be more interesting to that once you have to still do some effort but for the user. Okay. Another thing related to the presentations, I just want to make sure that I gave all of you reserving your slots. Okay. We're going to start with that presentations next week. Right. So we have to have everyone scheduled that we don't have any glitches in the middle. Related to the presentations, I, I told you we have a guest speaker on the February 13. I just got to check with him this and reconfirm confirmed this. So we have to ship the presentations in this day, on other days later than this. So I'm going to coordinate this with Summit and he will contact the people. We're going to present a visit. Okay. So no worries about related to the project during this weekend. I will drop some projects. I have a spreadsheet for potential open sources that you can choose from, and you can sense that I'm going to try to have as many as I can to cover all of you. Like all groups that say, like we have groups between 2 to 3, so we have at least 20 projects, like 20 open sources. Of course, you can take the same open source if there's another group taking it, but you have to do another idea. So there's no sharing for the same idea. But you can use a simple but I'm going to like I mean, try to have more open source as I can. And also I'm going to list some potential ideas for each one of these projects. You don't have to stick to this. You can come up with your own ideas on this. Projects and also you can come up with a new source code and a new idea. But you have to do this with me first. You have to contact me, send me that. I need to be an official proposal. But it would be a requirement because believe later. But at least we have a mutual agreement on what you are going to do. You say that I am interested in working on this project. This is what we have, and this is my initial idea. But there are two things that I want to make sure that you have when you do this with me. First, we got everything that this project is consistent with what we have. That's the way when you present your results, people will make use of what would happen if you don't talk about something that's out of the script. The second thing, I want to make sure that the amount of work is reasonable, to make sure that you don't overestimate things for underestimated. Sometimes you underestimate things, and when it gets to reality, you face a little bit. Okay, You'll find that okay, this project is huge. I cannot even go to the source with that and understand what's happening. So I have an experience and most of the open sources that we have in databases so I can rapidly evaluate whether this would be a good thing for you or not. Okay. Any questions related to that? You don't have to guess because it's coming on. So you've got to be in person. So what I plan to do that in that last call, about 15, this will be the beginning of the pre optimization part. We have two classes in this part, so we have one. I'm prepared with 13 on one hand and plus in the first class I'm going to spend the first maybe 45 minutes or one hour giving you introduction about the pre optimization and then he's going to spend the rest of the class with you giving his total. I do this because he's going to share with us some stuff about grade optimization by 5%. And this was the top bit I didn't want. He wants to start first, but this would be hard because if you don't hear about the optimization before the month going to be anything else out of the presentation, right? So you have to hear some concepts for me first to warm up and then after that we're going to have this. Okay. And I told you this because there's a really famous person in the area is responsible for that re optimization and rich in Amazon. So if you know about this, if anyone heard about Richard, this is like a cloud service for databases in Amazon. This is the main one for them and it's getting bigger and bigger now. And I'm about to talk about and he's he's also one of his goal here is to recruit people for internships. So he wants to get to know more about the students here at UC and whether you're interested in doing some stuff as a consultant. So it will be a golden opportunity for you with, you know, showing for Seattle and that class, you know, and ask questions, try to stick your name or your question to his mind, you know, this stuff, this would be helpful for you. Okay. And of course, you can try to reach out to them after the class if you have anything that you maybe like. I mean, propose an idea or anything that you want to work on or got any thoughts about this will be welcome. Possibly. But yes. Any questions related to that? Okay, Let's start now. Our class So today would be about quiz cuddling. And this is also part of the pre execution stack. So a quick recap of what we did. So we started with the storage, we did the storage, we started after that, we did indexing, so we built some indexing for different workload types and then we bypass backward optimization for a while. We're going to say that what we did already mentioned that we're going to do this later. We still don't have anything to do with optimization. We just described how the execution model is somehow happening there and what are the pros and cons of different things. Part of the equity execution in this kind of So if you have a plan and you want to execute it, if you don't have any problem in your resources, you just execute it. But what happens in reality that we have a lot of queries coming at the same time and definitely the details would be installed on a restricted set of resources in your machine. For example, if you still have a cloud service, you have some Amazons running up and you want to leverage all the resources that you have. If your queries are less than the resources that you have, then you will not have any problem. You just like, submit the quiz as you come. But if you have like some use or whatever you don't want to do, you have to be smart somehow to run this. Okay, so that's the main reason for queries. When the masses were created in the beginning, they didn't pay attention to that. We're going to see in the minutes why. But after that, when the elements becoming part of this become an essential part, need to be okay. So today's class also will be based on somebody that's from CMU. I've been to school and I will be sent to this two references right as part of the readings. But I added this one later yesterday because I thought that this would be also a good example. Okay, but no worries. If you didn't read about this, this will not happen. This would be fun. Okay, so basically we have some concepts would be different from the senior assistant class and we are going to show some things for this. Please. Okay. What do we do here that we focus on this difference? We have multiple queries at the same time and all of this is having their own tasks. Orders and they want to use. The main objective of both of them is that I want to execute my query. I don't care about others as fast as I can, but you on the other side, as a system, you have the responsibility to try to satisfy all of these requests from all backwards. Also, there is no restriction about how the cookies come to the system. Like, for example, the quiz can come and go or like finish at any time. Like for example, I suppose was one way. After one minute I have 100 ways and then up to ten when some of them are finished, some of them not. So it will be 50. So there is no restriction on the number of ways that we have at the certain time. This mode is called streaming mode. There's another mode that it's a very popular impolite quiz because I told you and we have like a bunch of queries we do over like on blogs or whatever. So it's mostly already. So we know them upfront. But there is a script with this queries we call this quiz, batch it quiz. This is bachelors. So in the back to query, we have all that quiz upfront. So I know there is a script and I submit them once. This would be easier for planning, but this is not common. Now we target the more refined. Okay, of course. What do we do in real time? So now we can also work with the batch. It's just a special case out of it. Okay, here, as we mentioned, we have some resources that are limited and we want to manage them most mostly when we talk about execution of resources, we talk about the CPU, all the memory slots that we have and the threads managing them that are usually about the main resource. And for simplification, people sometimes even focus only on threads. That's it. Because they say that there is a fixed unit of resource type, it's own memory, it has its own power coming from the CPU and everything. So people abstract this with threads, but at the real time what we want to do is that even we want to have some control on the serious thoughts and memories that we'll just be careful what we had in the last class. So this is like that quality execution plan that we had. Okay, this is a skill query. We translate this into a logical plan like this. It's a backup operators and we group this operator into pipelines. We say that in the pipeline we have a group or a sequence of operations that we can run and we can maybe omit some couples among the operators to run without any problem. So we have this pipeline breaking point so that we we still wait till we finish every dependencies we need from other pipelines. Okay. So we call this energy summit that we have here and a task because it's a bunch of work orders that we have in some cases or for example, in the second system. Quick step that I'm going to make sure we sometimes exchange the term pathways work order. So it's the same thing. So test or work order the same thing. Okay, How do we say here that this task can be even running on different instances of data? So if we have different partitions of blocks, I can even run this pipeline or task of orders, improve each one of them, one of them a segment, let's say different segment from 1 to 100, the second segment 101 and 200 and so on. Okay. So we can do this impact. The notion here that we have maybe like an extended one. The last question is the six. So now it's not only one have we have many tasks coming from many queries. And the problem here, how to manage this distance. This is that part that we want to go through. After we manage the task sets, we say that this would be run here. We switch to the execution mode that we had in the last class. We do whatever we do, okay? This is how we connect the different parts of the OR like layers of the division. Okay? And this is like, for example, you in this thing, for example, we have in this pipeline, maybe you would build a hash table if we brought the hash table unprotected. So if we do this on different parts, collecting different data segments, so basically each pipeline will be replaced by a task. This pipeline would be replicated on the different segments. So for example, if we have three CPUs, we can have like three segments of the building and also three segments of the probing can be running impact. Now all of these tasks are given to a system, so I want to do the building and they want to do the problem. How can they manage them? So how to run them? Which one running first can I run broke before build? Of course. Which one? I can run from the ability if I have only one goal or one threat, which one of these skills that I use? Okay, what if I have to? Which do I can choose? One of them? Because that's the main. What are the things that we are trying to do here in the step? Okay, so here we need to decide this question like the answer for this questions. We have a set of parts we want to decide for each one of them. We where to execute it. So we have bunch, of course, a typical, you know, so that can come to you. You can run it anywhere, wherever free, right? That's one thing. But we are going to see that this is not the optimum thing. That's the way. But second question is for women. If I have so many tasks and they have only them to those which periodically they assigned to each one of these tasks, so they just go random way, like first come, first serve, or so they'll do some reordering for them. And also how how means if, for example, some tasks can be assigned to many course like for example, one task can decide to only one thread for one core other task may be needing more than this. So who decides which tasks should have? How many resources, or how? Okay, that's the main thing. So I mentioned that notion of tasks and we want to settle for tasks sometimes we have to go more course the Grimm's and we still on the level of the operator. So now we say that we have to scan with all the tasks coming from this operator first without even going to other operators. Okay, So this is another advance of level two standard, whether we are going to go for task level or operate on this and operate here. When I say operator is scheduled, I have to finish all the tasks and this will be okay. Typically, as I mentioned in the beginning when we started this relational databases, we didn't pay attention to that scheduling problem at all because we used to rely on the operating system to do that. If you took out an operating system course, you'll find that scheduling and stress management procedures is part of what you study, right? Because this is actually the main job of the operating system, is managing the resources that you have in your hardware to serve your applications. This is one of this application. It's just an application on top of that. Okay. So what we do here in the old days that we just delegated this to the operating system and we can just like, you know, putting system beside whatever you want to do. So for example, one way we used to have in the course, the grace I think is still working to melt, but with improvement is that when we have like for example, end users asking for aqueous, we open one process from the operating system, each one of these. So each query is one complete process. Okay. Of course you know the difference between a process and through, right? Okay. So the process is more comprehensive than threat, but it is more of an execution resource. Inside the process of process is more complete, has its own memory space and more advanced management happening inside this process. Okay, So in this process, we assign one process to one quick, and whenever we have more, we keep assigning this procedure. And once we do this, we leave the operating system. How does this persist? So basically, operating system can say to estimate if this process picks, like maybe 10 seconds, maybe one megabyte, it's fair enough now to run it now because it's not giving the results so it will run it first. So there is no priority from the user perspective, whether this process or whether this is critical or not. Sometimes we have some queries that are critical. The other, but by really getting this to the operating system, this is hard to to do or to manage. Okay, So we set it now to do this inside a deep mess using this is kind of why is this more beneficial, why you need to give us more qualified to do this and this operating system and what is right right. So what do you mean by realistically positive ways investment. Right. That's that's exactly the point. So operating system has only access to the resources, the hardware, the environment. Of course, the operating system sees other applications. So it knows, for example, if you open your browser how much resources your assignment and only plans according to that. But mess, of course, I'm not saying the same resource. The view that I have as a operating system. I can ask the operating system what should be assigned to me. And then after that I do my own scheduling based on what I see that the operating system doesn't see. So for example, I see the data, I see whether I need to fetch the data from maybe remote machines or not, or I'm catching them. So from fixing them from remote machine, I need to assign more networking resources or like more like more time for queries that need communication from other machine. So I maybe I push their priority up or I know that the squares will be causing some starvation other so I push them down. Depends on what I'm going to plan. Okay. So I take whatever I have from the operating system and I complete this from my view, which is a data back with the patterns of execution, the patterns of users arrival. So for example, I have a data to have that with a template for the police, of course. And then I have at certain times of the day very few number of users, maybe at the at the middle of the day we have to be. So my decisions or scheduling would be different based on what I would do in another day. Like, for example, the Black Friday, for example, we would have a peak of queries of discounted orders or discounted products for the other days. Typically in the middle of the year, we don't have this, so I have to be more relaxed. So that's the way think a mass is more qualified to do this compared to the operating system of okay, it takes whatever it takes from the operating system and completed from there. Some of the optimization, the goal that we go for as a deep mass when we have queries are false. So typically the main objective for any device is go for this role. So I have a little quiz and they want to finish them as as much as they can. Okay. In the very small amount of time. So here we maximize the number of completed quiz. But this has a problem. This could be affecting the fairness because maybe I have a lot of small queries, very, very like a lightweight quiz I keep so that I increase the number of the quiz finished by maybe second. I keep putting priorities for the small queries. I finish all the coming spoke with, even if they are more recent than others. So I keep finishing them for this purpose. And then after that, the long quiz and this will be not fair. So the other extreme is to just focus on fairness and fairness. You assigned whatever query. I have you a fair amount of resources in terms of time slots or maybe resources for execution threads or other. Okay. That's another thing, another important aspect. It's very, very important for boys to be workloads that we're responsible. So if I'm doing a transaction on a bank, I need to make sure that I don't have so much latency latencies having just like or maybe like in stock analysis. So because like a second or a minute can make a difference. Yeah. So in this case, I want to make sure that response here is somehow very fast. I minimize the latencies. So this term is also very common in many papers. You see. And if you don't see it in this paper, this means that they are not targeting like their worst case. Their businesses are mainly for the worst cases. So let's say like I have 1000 equity, maybe like to quiz, take for example, one hour and the other would take seconds or minutes. So we call this quiz that can detect the average of the response time as till later. Okay. And if you look at the search engines like Google, the use what you optimize to avoid this case whenever you submit a query for search engine. The greatest to you and answer in very, very small amount. But I don't know if you ever faced this or not that you submitted it to a search engine like Google and then it stays for one minute before you start generating pages. No, it generates pages. Maybe your desired output is not in the first selected results, but we guarantee that is possible that maybe I'm older than you bit and I am. Like in the beginning for when the search engine started to with this situation like this, like we sometimes we like we submit query asking about something picks up almost couple of minutes to do that it was annoying it was when whenever you are doing the search when we used to have this Google to have a helper like to help us. So it's not just for but understanding things, getting some concepts. So if you just wait for I don't know if it's boring, it's so that's why these what you to the responsiveness also another thing the low overhead so I can find you an optimal scheduling policy that guarantees for everyone to run in a perfect time. But the gaps between these queries when I plan this could be having so much overhead. So for example, if I have query workload that can in total finished in minutes and they take scheduling before half an hour, this is infeasible, right? Because I can spend so much time in the scheduling to find the optimal vacation schedule for you. But this execution schedule itself can take forever because optimal scheduling is I'm in B hot problem, so I can drag in the time as much as I can. So we want to also make sure that I come up with solutions that can somehow wake them up for me not to put the bottleneck in the system, especially because this query scheduling happening during the execution. If I have a workload, I detect the optimization, I have the plans ready and they want to run them. So now that user latency is still blocking you, if I take so much time in the scheduling, this will add to the overhead that the user will have. The user doesn't know exactly which time is spent in the actual execution, all of the scheduling or the planning, if we only wants to give an answer. So as a as a system, you have to make sure that this overhead is minimal so that it doesn't, you know, since there is a bottleneck in the system from there. So these are the main optimization goals. Most of the state of the art scheduling algorithms that we have are not optimized for all of them. Usually they are optimized for some of them or even one of them. Okay, we call this heuristics. So I build up like for example, I build the scapula or poorness, make sure that all that with are having like fair resources shift. Okay, if I do this, I optimize for that and they have my use case to do that. I end up satisfied with this. If you have other excuse, you have to switch to another false. Okay. And this is common in many of the but with the most recent workloads that we have, we can have more mix IT requirements. So that's why having this policy and like the one policy driven thing is not that optimal. So what's the solution for that? If it's not, it's fine for me like to build my scheduler according to the risk. What should I do? And who choose among this? But it's that's I'm saying, so how can I do this? Do I need to like to do it one by one? Who's going to decide this? We optimize that. This is not a job for deputy optimize evidence is only application of the results. Yeah. Sometimes in the application you can. Okay. So that's one solution. But this is more let's say who to help to help with this. Like I, I really get everything to the user. I see that you have a list of options and you can choose from them and then stick with that. But what happens if my system is serving so many users and at the running time I don't have the option to satisfy all of this requirements for all these, I have to make some decisions on my probable choosing. So choosing to prioritize on the basis of the use case. Okay. That's that could be also relevant, but it's a simple I don't stick to one of them. I come up with my own policy at the running time based on the demands. How can I do this? I learned that's a perfect use case for machine learning, right? So if I have heuristics and this is the risk of hard to be satisfied, then you rely on some of the advances that we have in the machine, learning to learn what we have with our hands. So if it's more skewed towards like short quiz, you learn policy that can do that. If it's more skewed towards like read only, please, you learn a policy to do that and so on. So learning is once the use of you. Okay, So we're going to also, you know, minutes one of this exam. Okay. So either we talk to you, we have some heuristics and your mistakes are good. So why we even like so. Okay, so there is machine learning. So why don't we even rely on the machine learning in general? But then also just to stick to this, like sometimes we stick to this. The training time would be pretty, right? So this is an overhead for the machine. But okay, so if the case is obvious and implementation for the throughput and fairness, you are easy. So from a system perspective, I can stick to this as long as I'm satisfying my point. If not, I switch to more complicated. Okay, So. So till this day, we have a lot fewer risks implemented in our quiz because it's not. I would just go to a lot of the mistakes some of the used it's booth right right. That's yeah, that's perfect question and this happens a lot that's why on the machine learning approaches we put a compromise in this so definitely you are not going to have the optimal solution in everything. But obviously we can we can look there and see the throughput and prototype. This is theoretically proven, but we cannot guarantee both of them. In one case, we can have the counter examples without it, but at least we can put some weights for that or based on the machine learning, we can increase the probability of success by one of them compared to the other. In some cases, like, for example, in this part of my workload, I have more throughput advantage, so I should stick to that. So with third topic criterion and in the other part I can have that fairness corrected. I'm so yes. So that's why machine learning could or even when you were deciding on piece of data is as a big machine for. that's a good question. So over in the machine, in the machine learning or or in general. So the easiest if you prepared you can do two pieces. It had to see those cases and then you tried to come to. Yeah so so and the typical setup without any machine learning for example what we do is that we assume one of this is running by default and we use like for example, the throughput. We keep running backwards with this mistake and we return the results to the user. Usually users have their own feedback, so this feedback will be given to the system or in terms of satisfaction or in terms of complained or maybe bug reports. So at this time the system can switch to another thing. So it takes time to figure out whether I'm satisfying the, the main requirement that they need from the user perspective. But in the machine learning, we can we can somehow detect this without the human intervention. So, for example, I have many queries and I know from my past the training that these types of quality should be finished. Maybe in one minute. So when I see that the current queries with my new policies that I'm learning now, stick it, for example, 10 minutes so there's something wrong. So I if I think back to my machine learning policy already on the policy and then I do some modifications to the policy so that I can adjust. So in the machine learning, so it's more reliable in terms of the feedback compared to that, that typical. So it's like a piece where one policy like I think this policy is of late and then this gets a negative, right. Yeah. Yeah, yeah right yeah yeah. I mean if I have all the information upfront that this workload should be more throughput driven or whatever. So I can just put this as a configuration or tune this in my database. It wasn't working that was responsible for a number search initially from your competitors. What I saw was this. Yeah. So like, that's, of course, heuristics inside the system. I can put some thresholds if I find that there is a performance degradation at certain point I can switch to another and maybe I try random choices, but I satisfy this kind of performance of machine learning as well. But this is trial and error thing. So it's it's more of a learned approach that could be robust to solve this problem. Okay, So is this in the model? The database are relatively database level already at like as a put multiple databases together as a system, a couple to engender like would there be one model it supports multiple schemas. yeah. You need to learn tabular. yeah. So so typically what we do as we are going to see in the last system, we build a scheduling policy, build workflow. Even within the same database, we build a workload driven because we know, for example, from a template that this workload is somehow repetitive and we know that they have this pattern. So we learn this and we keep using the post for this kind of work with them. So it's not even for one database, it was for one. That is even I can have different scenarios and this model can be, you know, trained differently for different scenarios. So it's still based on the world. It's very hard to build based on the query level because this will be hard. You don't have so much information on the quality level, but on the workload level, you can have some amount. Okay. So usually when we assign tasks or we do the scheduling, we have similar approaches like what we mentioned in the phrase, we have boom and post based. So typically what happens in in any of the base system that we have, for example, calls and we have threads, it's effects. And what we do here is that we have two options as a operating system in terms of as a base management system, in terms of managing my resources either to make it more dynamic as much as I can. Like I have, for example, six threads and whatever I need to choose from, I can just list that call or the operating system worker. We call it worker because in the part of innocence that we have for running, we call it worker, database worker. And usually when we have this worker, we can let him choose one of these two choices. The first one is that dynamically choose that threads that you are working with. Like, for example, I have this six threads. So in the first maybe couple of queries, I use that for you to, I do some switching to that and I use it. I use it for running everything and the two queries and then after that I use this maybe in the others, I use the threads and so on. And the way we are using this thread, this code or choosing from others. So it's more dynamic choice. And another option is to take only one thread and benefits. We call this. Okay, I've put this only thread to you on this score and keep the query execution happening only on this score based on this. But what are the advantages and disadvantages of these two approaches that might have against your talking about which approach to the second one or the like, depending? Stephanie Okay. So you're saying that depending on right, right, right. That's exactly that. Okay. So basically when we do panning, we have two main advantages. The first one that we keep that cache level by coherent as much as we can. And the second one is that we minimize the switching overhead as much as we can, because when we move one thread and replace it with another, we pick like a kernel stack related to this that the operating system managed this thread with. So we take this and copy it down another place if you want. We do this switching a lot. So if we have dynamic environment with many, many queries so we do this context switching a lot. So we do a lot of overheads, which is costly because this will take a amount of time and also amount resources and hot stuff in the operating system like that. The stack or that cache is that the very high level caches? Answer because it's an easy for the operating system to operate. So we need to make it somehow efficient so they keep it in the most precious resources that you have. If you do this switching a lot, then you're compromising. This is degrading the performance of the operating system. You're trying to benefit yourself, but you're not helping the one who should be providing the resources to, okay, that's actually the main advantage of being on the other side. If we have like this dynamic pool of stress and we want to choose from, this will give us flexibility in coordinating maybe more resources to assess quality. Like if I have one query like assigned to the school, I can choose more threads, like maybe increase it or shrink it based on my requirements. Okay. What we do typically is that we stick to the we don't care about this shrinking so much because the overhead that we have for switching up is killing whatever you are improving here from this dynamic thing in brackets will be killed by the context switching overhead level. So what we have here is that if you have three cores, most of the efficient systems, even if you have like ten threads, I just stick to three because I think three works. Okay. And then I do the scattering within this three threads. Okay. That's that's the that's the why. Now was that large scale of the data? The hardware is not even efficient for us in the database system because even if you have resources for creating more threads, you don't have so many calls to do the opinion with you. Basically, you don't have like 1 million code, for example, on your machine. You have to mitigate this by somehow cloud service that can do this impact. So you have more machines on the cloud, for example, to have this more course to do this. Okay. But of course, this will be costing you more money because you're getting more machines. Okay. So typically, if you have like this system, the typical system setup of on premise database system, we just do, depending on the course that we're we have to approach it to assign these tasks to the course that we have was the benefits. And then we create one manager here, we call it dispatcher. This will be like a proxy for us to get all Beckwith's and their own tasks. And then I have a list of this. And whenever we have a free resource being, notify me. I assign a task. Okay, so we call this whole beef. this. So this is this here. This will be pushing back the tasks to the threads that we have. Okay. The other side is that we keep everything here, of course, still, but I'm not the one who's giving you the tasks or asking for your status. Whenever you finish, you tell me that you need more code and repeatedly, repeatedly, like you send this request and I return you that the the quiz or the test is that we have you. This boot based thing is more suitable for streaming based engines if you have streaming engine. So we know for default that we will have endless quiz or endless like task. So I don't need to put so much every year on the dispatcher to do that. But in the typical relational databases items or non streaming engines, we usually work with that post based one. So you have everything on your admission control or whatever. The system here that we manager and then you dispatch it to the others. Whenever we need that, we push it. Okay. But regardless of what we have, you just do setups. One key thing that we need to keep in our mind when we do the scheduling is whenever you ask me to process the task, don't put so much overhead on me to like to get the data needed for this task from a remote place. So you should be smart enough as a scheduler to figure out where the data needed for this query located. And then you assign me to the closest court or the closest to it. That's very important. Okay. We have two types of hardware memory outs. We used to have this uniform memory access labels events. On most days systems will be I'm going to describe here this was a system. There would be example depending on the implementation of the OS, because this seems like something that was a responsibility to deliberately do tasks to the phones. And also here we're the core here means like the worker. So here we are, the proxy. Here we have a scheduler that interacts with the worker. And once I have this worker I just sent to the operating system say that I want to assign this to this call. I ignore what happens on the operating system. I overwrite it because I know more. Okay. That's the thing. And our previous thing, of course, is to put the workload as opposed to what is the data, Right? Yeah. So how would that be controlled? By the database in the bush based its famous people. So this workers are the database. So this is part of the control. So basically what happens is in this worker, they just ask for new tasks. So they have 21 maybe. Q And then they take they take tasks, whatever they need. Yeah. So I'm not asked. So as a push piece, I have the responsibility to check the status here and then push a task whenever I see someone is I okay. But if I don't find anyone idle, I keep myself like, you know, away it would be finished. And then I keep asking after that I do this. You beginning like after, after, like after a certain amount of period of time. So that's why it's an overhead here. But in the puppies, this is the worker that they are doing this. Whenever you need something, you just go here and get it. And if one of these workers, for example, it's maybe a Twitter, a Twitter worker or whatever, so there are no more users doing read drive requests on this part. For example, in in this part of the world, like, for example, in Africa, like in the middle of Africa, there are no more reading requests. Like, I mean, maybe like what we have here, for example, in the and like in us or in Europe. So at this part, this resource will be idle. So I'm not forwarding some requests from U.S. and Europe to this. I'm keeping it busy with their own workers. That's the thing. So here in the put this, this one is the one who's defining that special. I need something. well, good is like a complete system of database. It's an instance of education in bulk. Yeah, definitely. We'll be welcome. Start with you. Keep pulling the actual voice level. Step one. What gives this list? And then if you have what it what these. These are abstractions. Yeah I can you voice living the workers the workers. The workers is a software. It's not just core it's it's a code for running the queries that we had before. So it's software and VB These software abstractions will be with support for students. no, no, no. So basically they just inform the operating system, their decisions. So we know that for this school or for this worker, we have CPU want. So we know that we know that I it, so I know that CPU one is assigned to. So whenever I want to execute something after the scheduling session with this missing based on push based or pull based, I get a task and then I go to the operating system. I see that is talking, including on CPU one. Now do all on that is an instruction being pushed to the actors, right? Well, yeah, yeah, yeah, yeah. So, so here this will be directly talking to the operating system more like whatever calling the API that would be useful to execution. But before I do that, I have to figure out what should I do, what I plan. So yeah, let's switch to base assignment. That is definitely a lot of work because it is like, yeah, but slightly different for any engine. Instead of like you said it is the of on streaming. Yeah. Because what it does is it seems that right because here so here's the thing so that whenever I asked about the stuff here about the status I ask based on my state if I don't have any queries I will not ask you. Okay. If I have so many queries I keep asking you. Right. So I have the control as a query. We manage it like a admission control part in the system so I can manage the rate of my requests for you. If I have so many queries I can keep asking you every one second if I have only one query. Or maybe you don't have any queries I will not ask you or I will ask you like maybe every 10 minutes or every 15 minutes. Okay, so I have the control here. So that's what happens in the dynamic situation that we don't have this like continuous query processing if content script processing. So I don't need I know that I'm going to be busy all the time, but if I don't know that, then I keep you. I keep the like the control here in the end. Equity manager so that we can do this business that I thought it was very good for me that way when was like, You should do that, but I'm ignoring that would bother like that. But I just never switched out. But then, I mean, so, I just, I do that. So it means all of them. So it means that the core would be assigned the threads and all that queries would be answered by this worker would be also true. This thread like the stable stone that for example, if you're doing the distances and if you. yeah, yeah, yeah, yeah, yeah, yeah it will, it will stay because I don't want to switch that way. So if the thread goes there and do some disk access and pull the bottleneck, I will not switch to another. That's why it's pending. In other other cases. Yeah. I can put the thread on hold or like maybe running it in the background first and then I do another active process switch. But this is really posterior the this starvation for or other applications because we will keep out. Right. So here we are. So okay so that's a good question here. We assume that the database application is a main system or the main application running at this time. Whenever we designed this, we assume that what happens in the operating system level that they take care of this. They know that there is a heavy usage on the score or the thread at this time based on this application, it seamlessly will switch some of these threads whenever the installation on the application. But so there is for example, another like maybe the what browser needs now more activity, so it will seamlessly switch all of this thread related to database. On another thing and we'll put this with browser that's but when I'm inside that the database defaults. I'm not switching anything. I'm assigned to my threads on this one. Okay. And I think if can assume that the database is only working on the select or resources assigned by the others and it has to be does not equal with that in that. Yeah. So mostly most of the cloud services, for example, now they don't want anything else besides what they have indicated. So this is a database Emerson's But it's just up and running. Okay. This scenario that you're talking about, about switching, as you know, this is happening on our desktop, which usually we don't run with you. Right. But if this happens, there is a solution on the operating system level. We don't care about this on the database system yet. Okay. So we have to I mean, see them. So here, one thing that I mentioned before, your question is that we have to make sure that things are somehow local to me. And there is a term that recently looked like we submitted like in the last ten or 15 years, we started to hear about what is that uniform, that memory access and the money for memory access. So the default memory access, this is what we used to have. And even like Intel, platinum, you know, that all generations of CPU's that we had before and different hardware provided, we usually have different of course, I think lot of them have they have their own that's and then we have memories. It could be like different chips so but when this memories interact with this cache on the defensive use, they are accessing them through one layer. And this is uniform for everyone. So this is called the system bus. This is the bus. So the system bus here, if I have this memory chip, which is very close to this CPU, it will if it passes requests here from the cache or whatever, like read something like maybe it like something that was written near here back on this memory. The same time, I think to access this resource, I'm getting access for my question here and here because all of us are served by the system. But the same thing. Okay, So it doesn't make a difference. So people didn't care so much about how to design on the hardware level, how we put processes or threads close to the case. Okay. Because everything will be the same at the same level of priority attacks. Okay. What happened here that to increase the performance of the hardware hardware providers like Intel and AMD, whatever, they try to make it more isolated hardware problem. So everything will be closely designed to be very, very mixed. They like sticking together at the same hardware chip. And then if we have many parts, we want to connect them. We use some Ethereum connection passes that will help us to fetch the data from remotely. The main advantages that if we have a coherent group here that's very limited physically on the simplest that reads overhead and right overhead here would be ten x, maybe like five X better than the remote one. Most of the applications that are not computationally expensive, they don't do this of fixing from other places. Let's say like I mean, you're doing your web browser mostly your browser will be hampered by this kind enough. Another application would be hampered by this, but that other application would be handled by this. But if your application is not computationally but in database, if we have so many things happening and we have to design for exploiting all the resources that we have, we have to take care of this situation. So now there's an overhead event assigning the queries to you. You have to make sure that the data that we have here are answering that really that we have. Okay. So you have to be aware what are the data that you are accessing as a schedule. And there is a like for example, a meeting table or whatever, you know, exactly which part are, but which part of this data would be the answer for this. Okay, we call this sometimes normal if if you just you see this term numa, it's called a non uniform memory. Okay. The first part of schedule is a static of which is happening also at that logical level. So if we do that, we optimize our planning. So I know that this query plan would be like this. We have these two pipelines and I know the number of segments. Let's say like I, I will create like ten instances for each one of this pipeline. So I know the number of tasks, I know the type of operators that we have. I know everything. So what I do here is that logically I say that the first ten tasks will be on the first order, the second deltas would be on the second call and so on. So I do everything in terms of decisions in the logical planning phase. Okay. And we do this based on your for example, I know the number of tasks, I know number of use. So I assign for example, I divide the number of customers. So to make it like load balance. So I assign here the number of until I tried to be smart. I know the ideas of the of the like the data I've got, I will access to answer this query so I can assign this like segment distant segments to this first part of this can segment to the second part and so on. So I do everything in the logic and once I do execute operation, I don't do any planning. I just know every, every thread or every code you with a pencil, it knows exactly what order list of tasks that would be doing this. So that's the is what's the problem you and to that but the execution and where it takes more time to write some more but I'm not prepared for any unforeseen situations happens in the execution time right I did my decision in the beginning and that's it That's my heuristics. Okay. But could be other applications running on the same like on the same machine with me. Like getting the resources so I have to figure out another scheduling order or whatever. This could be a problem for me if I do everything. Okay. What's the advantage of law? Yeah. So I don't do any sophisticated approaches or do any sophisticated like decision making that can speed up from my usual time. Okay. What happens now that most of the systems start with This is scheduling, but they also augment whatever they have in the scanner with an ability for dynamic studying, so they are not focusing on such scheduling. Main Okay, so I give you like 2 minutes or 3 minutes and then we switch to this dynamic scheduling. This would be the rest of the empty speaker. We want to look at this. You don't, you know, because you think it's the ability, which is if this is that so very quickly, I think that the end of the okay guys to go back. Okay. So the second type of scheduling is that dynamic scheduling. And as you can guess now, decisions are happening while we are doing the actual execution. So we have we start with some logical decisions and then why did we ordering the execution? We are taking some decisions for this can Okay. And of course, we know that this will be having some overhead because I have to reason about what I'm going to do next during the execution. There are three examples that I'm going to go through here for this despite the scheduling. But I would, if I could save an elephant so high risk, I'm not so hyper is a system. It's a complete system. It's almost steps. It's a complete system for in memory processing and it assumes that all the data that we have in memory and it's mainly designed for or lab queries most mostly and they have their own query scapula inside that. So they don't they don't rely on other scheduling approaches in the operating system we have to own and they are one of the early people who did this idea and people started to follow up other them. Quickstep is another always workload handler for in-memory processing, but it has another approach for handling that with scheduling. There's some details in that. So that also worth to you to have. So initially I was planning to have hyper and I was the last one. But look, so I found that it's also a good example for you to show you how different architectures could be done in the same time case. This is the learned risk. I know that we have. This is part of my work and it's considered that current state of the art for that in memory workload scheduling. So there is no other limit this kind standard for always in memory other than the system support to introduce this by to use the goal segment 2000 we need almost two years. Okay. It has a totally different approach using machine learning. So there are two things. The first two to approach is that we have the hybrid because then they are using it inside of it, but that it escape is completely done. And so this is another example on how we divide the strategy. So we have a static and dynamic scattering and also we have the investigative and learn step, but we have different criteria for having this. This the start was the hybrid. So basically here we are dealing with whatever we have in the partitioned databases. Let's say that we are having this hybrid storage partitions that we have before, so we have different partitions, horizontal partitions, and inside them we have columnar representations. Here we are using a bit of different, maybe we call it more than the partition or look like. So they have partitions like what we did. But inside this partition we have another level partitioning, we call it more. So this is the amount of topics for the amount of work that will be processed once at the surface. So instead of having partition completely like will be running one. No, they are divided into modules and here we are not having an interesting idea. They don't have inspection, but they are still pushpins. Okay, it's kind of a mix between push and pull, but I'm are not having it. There is no dispatcher that can fix the decision was taking the decision is the worker but still based on the needs of the operating system not continuously having all that okay here workers need to cooperate together because they see the same number or like the same set of tests, and they need somehow to coordinate that they are not overriding what they are doing or they are helping each other in like in doing the tasks once. So, for example if I have all the tasks now to be building, the has to show the workers should be smart enough to finish all the tasks related to that has table building first. If someone finished everything related to that has table building and has the ability to start hash table hoping it will vote on that all the other course finish. And then I start doing the project because I cannot do the project, it will actually be completed. Okay. So that worker has to see somehow progress of other work. So it's not a centralized view from a dispatcher perspective, but all workers need to be aware with what we have so they somehow have a method like a global meta data, like container, like a table or whatever it that logs the somehow the progress of the different work so they can see what's going to be happening. Okay, yeah, it was benefit level. What's the benefit of this. Yeah. So the benefit of this, we don't have this overhead of the piggybacking of the spectrum having like to ask a lot. And mostly when I transform everything into packages of morsels, I all switch myself to streaming seamlessly. So now I'm not using only one task. No, I'm trying to divide the tasks into a stream two. I only like all that. Like every task will be divided, for example, into one side of that that. So it's like a stream of tasks. So I'm putting myself also on a streaming mode, although it's not a streaming mode, but I'm, you know, I besides and if I do that, I have somehow to avoid this overhead of dispatching because I know that I still work for 1000 items upfront. So I don't need someone to ask me every time and this once happens. Okay, I already know that I'm going to work for once I time slots on this. So I remove the dispatcher, but I do my own tracking of the progress for other. Okay, so we have this one loop of the tasks and all the workers will be getting from this. Okay, So we'll see how this happening. So this is like a query. This not the same way that we have. We have this pipeline and this is a data that, for example, that we have for me. So we want to build the hash table on it. So this is one partition like what we did in the pipe. And so that was this competition. But inside the partition we have all over the presentation and here what we do is that we are not using to the whole package of data defense things here. We are dividing it into both. So this is one mostly this one, most of this one was not so at this more that could be spanning the whole subnets or could be spent in some columns. But based on your weight and based the logic of the operator or the passenger, but that the more so would be one subset of what you have you in the partition, whether for the whole topic or for samples. Okay so I think this model and I know for example the first test that first test you that they have is that as people and they have a three course view and they want to, for example, divide this to three, you know, types of segments like we're like three segments to do the work. So I divide this year to three segments and for the probing on B, I know that this will be maybe take longer. So I divided into four. Okay. But still I have three calls to do. That's why I do here is that workers are smart so we don't have to we have to finish this three first. Then we assign the hash table like segments that we need to build here in a different sort of model. And each one of these calls and we know we have to own morsels that we take and also they have their own buffer that will be dealing with that that cache of this processor, the resources, the kernel, the kernel meta data information that we have for the thread. So everything will be happening on this inside this buffer will be local. So I'm not reading anything from other places. So here I'm aware of that. No, my problem here, I'm not fixing anything or reading anything for my models here from other places. All this water will be sent to this court and will be happening at this. If I you know, Maqsood is a computer person guitar. Is it like at the time of grading at the table between. Yeah. So the physical representation will be the partition, but the most will be this table data that I will be assigning to you as a core to be operating the tasks on that. So let's say, let's say that I do the hash housekeeping note like the hash of the simpler example. Let's say that I'm, I'm having this data from and B and for A I have some tasks to read the data of E and then update the details. But this is another set. So each one of this course would be handling the re update tasks for this little more object. So basically if I have this in the first and most of the year, I'm going to have the tasks for reading the View and writing the View. There is no right to ask about the more so to hear from you, because if I do that, I will have to deal with the issue of remote fetching from other course. Okay, so it will be aware of what I'm reading. All right. So try to minimize the overhead of communication among the different course. Okay. So what what we are doing is that we did this last year. We have been here to be running on each one of this course. And also, if I need to write, I right here, this way, write. I do everything now. So are these schedulers specific to the storage, for example? The hard, isn't it the horizontal partitioning? Does it assume role based? Because that's a good question. So basically, whenever we design a system completely, we have to to be aware of what happens in other layer. So yeah, here we are assuming that you have a horizontal path, but we are not saying that this is a rule based. You can use the column so you can use the height. So it's the role and and the hybrid support for that. I'm going to give you also, as I mentioned before, most of the systems use the high. It's towards the outermost. So whatever we are going for the default option for design, we assume that we have a total partition and then that's the difficulties that so what? So for example, here, if we are doing this and we build the hash table and this tool code is finished, they are part of the hash table building, but this one is still working and I have to do it for this course to pick from this for I will hold on. I keep myself like I go here, I don't do anything. So this one will finish and then we'll start, you know, fixing three from the build for the property. Okay. So I'm the main message here of this hyperscale that I'm aware of my local vehicle management. So I know that I read, write and do everything for the data that they have on my part. And then also be careful when I'm doing something that can affect others. Like, for example, we are probing the same staple and others still didn't finish what have I have. So I'm going to be waiting. Okay. So he also we put inventory for government. We have this one in Spanish. We can, for example, one of this year and also if we read something from here and we like to see it's still fine because this is in the properties, in the properties we're reading, reading usually is let's or is having less overhead compared to the down. That's a general in anything. Reading is what he is having less overhead than like whatever we are doing this on normal or disk or memory. Okay. So you usually call this mostly driven, but we model is very brief term that we use. So to say that this is kind of a block based scheduling or partition people use also to say this partition, this here we are assigning one core, one worker per core, and we have one more task to you. So if we don't have the situation that we need to wait for consistency, then I have to be proactive somehow to take tasks that may be better scheduled for other like, for example, if I don't have. So in the example of the building hash table on building hash table, there is a requirement to do that. I cannot start that probing, but I finish the building. But in other cases I might stop in some other tasks that are available for me, but there is no problem. If I sub, then there might be some optimal. Okay. If I, if I if I'm taking as the poor compared to what others are taking could be a problem. But if I can do this because I'm somehow so it's better than. Nothing. I can do this automatically thing with partition reading for example. And if you look myself, I okay, so sometimes we do this what stealing? So we know that this task should be assigned to core to for example. But as a cool one item so as there is no harm to use it. So I steal it from others. Okay. If there is a big problem like consistency issue, which usually really happens, if I have a transaction manager, I have to roll back like all what I did. So I keep a look of my look for myself in this note and then check it is inconsistency. Issue happened, but usually, as I told you, this is for all actors and usually this must be a request so we don't have this issue. So it's mainly about I read some data from remote places that could be taking no higher overheads without fixing that. But compared to being idle, this is much. Okay, so on the previous late atmosphere during a previous, so the buffer of each view applies. What should you be asking based on onwards? Right now let's say, you know, but it's not, it's not. I think in one CBA on the line what what do you expect. So here this buffer is actually the hash table has to be it has to be in one place, completely in one place, even if I'm partitioning it, I know that this partition on one place you will be having all the data related to this partition. Okay. So if I actually any data here, for example, probing the table here from the partition that we have you, it's still fine. But I know all the data that they had here and this plus this buffer related to this partition. Fantastic. Okay. So now let's let's take an example of a model that is a good proving that actually I'm free. And so there is no brief on the asking, but could there be a placement? But seeing it would be a 52 or one? yeah, yeah. So for example, the the house table is built and I know. So the thing when we build the hash table, we build it locally and then we globally merge. Okay? So that to have a consistent touch too. So now I tried merge it. I know for sure that if I'm looking for this key, it should be answered only from this part. If I don't see it here, then there is no key. So for example, if I'm looking, if I have a press or like asking for a key that should be in this partition. Yeah, it's still fine. I can answer this question by probing the table. See if the partition of the if I don't find it here, then the key is not in one of these partitions. I know for sure that at the end this would be a consistent build has to. So there is no randomness in building. That has to be either. You do this locally, I believe once capacity which would be in a certain number, of course there is a certain memory based in a certain memory in the system. So each core will have its own buffer, it has its own memory space and every every cache level and whatever. And there is a shared memory that we keep most of the data that we have globally so that like there is no system doesn't have this short memory. There is a segment, but there are some systems rely heavily on certain memory and other rely on the should we call these systems kind of shared nothing because they are partition things in fact. Okay. Okay. So is it okay now we understand now the concept of what stealing that I can do some stuff. Support memory. It's just being okay as long as there is no harm in doing that. Okay. As you can see here, there is no notion of priorities. I'm dealing all the tasks as the same thing, but there is no notion of produce. Whatever. I'm free and there is no problem. I think some of this stuff is undefined. Okay, this is not the case. This is suboptimal in many cases. Okay. And also here. So running queries also can be blocked after running a long running query and it could be starvation problem that some of this cases would be having problems in. Like for example, the short queries would be finished like becoming first and then then if I finished in a lot of them, like in the beginning, Okay, so if we do that, this will be an easy starvation problem. Okay. So what we need here is that we make some positions smarter. This bill, we try to take as much as we can as, you know, hardware management and efficiency for pitching, pitching the stuff and not having overhead. But we don't have any print. So there is a paper called on product candidates called selfie tuning scheduling thing. So I put this optional reading that you have, which should be one of the things that you may be like I write to report about this. It's an extension for the system and they are putting somehow the notion of priority in this kind of work, like in this morsel driven stuff. So now they are extending it somehow to do that. Okay, Now another scheduler here is the quick step one and the quickstart, which the one is similar in terms of the notion of what we have to do, that we have tasks and we want to schedule them across different workers. But this one has a centralized structure. So this is like having a centralized dispatcher that could ask about the status, the status for the different workers, and somehow here they are smarter than the other systems they are using, machine learning to predict the resources, not for the policy. So they have to fix it both. Okay. But they are using the machine learning. See that I have a task to better assign like a worker or resources for it. I have to predict based on what I have seen before, What are the resources needed for that? Okay. How much memory, how much maybe CPU cycles? And then based on the statistics, if I have two options of workers to assign this thing to, I should go here. They are not preserving the normal notion. So it's fine for me if the worker that has better resources and even if they are far away from me from the data to assign this, it's fine as long as I'm having a better chance to provide the resources. Okay, so this is the architecture for the system. I'm going to go through it one by one. So this two parts are the components. As I mentioned here, the replace the past notes are always a work or so if we have work order. This is similar to this, but this is a quality manager. Whenever we have coming. Okay. That way that we will come to the bus is usually by something called admission control. Usually this is a part of the A system and this admission control has all you want as one thing to do to start like to admit that we should be starting now for, rejecting it or putting it on hold. This is not for scheduling, only this for anything. Like usually like I mean there are some back malicious so we we this is the job of that question or maybe queries that take so much resources and in order to so that's that doubled deputy manager and that mission control for that to take that into account. So what we have is that whenever we have a new query, we have some resources requests for this. I know that with the existing data and I will be having this kind of cooperative, stressful, I have a map resources, and then this will be going to something called policy poster and it has something called law controller. So this load controller would be estimating whether I should start having this query executed or not or putting this on hold. So I keep this waiting. You know, I have some resources for it. If I have some resources, then I will give it to the query manager. So added to the queries that we should run now. So if I have queries now, we have in plus one query for each one of these queries. I know exactly the tasks that they do with the data that they fix or what should be done, and then equity manager will be responsible for putting some priority on how we should start executions. For example, if I have to quiz running and I know that one of these queries will be almost done either crystal or so I put my suggestion or like, I mean, I give some like your proposal here to this policy enforcer that you recommend this operator from this query to be running the tasks from display to run them. This is not a final thing. I as a policy enforcer, I have the right to reject it or split something that's based on my policy specification. So here, if I have different policies, let's say like first come, first serve or I have certain very first or whatever, I have policy there, I have to see if whatever you suggest there is, like, you know, matching the specification that they have. And then if I do that, then okay, I can guarantee that the task requests that you have and I signed up to that with like the equity workers that we have as a helping tool. I can use the learning agent. This agent will be estimating for each task the resources, as I mentioned. So basically you said just that I should run, for example, task one and two for equity one and don't run in the beginning, task three and four, property two. So what I do here that I take plus one and two and three and four like that have built and I estimate the resources for all of them based on the learning agent. I see if this would be matching my policy specification. Like, for example, they be not causing starvation like one of them is like eating all the resources that we have. And based on this specification and the prediction that they have here and they suggest to assign the workers the forces that they need to run now, you'll finish your tasks. You as a work as equity manager, you need to buy me that you finish your tasks, but you don't ask for anything. Maybe I have some tasks that I already scheduled before providing you suggestion, but I need to finish so I provide you some tasks to finish after I finish everything on my site. And you are not suggestion suggesting anything, possibly bringing you like I go there and I tell you if you need anything also to be running. Okay. So it's somehow complicating in the interaction, but it somehow making sure that if you have a certain policy it would be match of that. The beauty about this spot that you can build this in as an abstract framework. So here you can extend the class policy and you provide the policies that you need and whatever policy there is, like interfaces or how you achieve this policy. And based on my user specified policy, I can do whatever. So for example, I provide different implementations for this first come, first serve, so Trotsky first or any kind of optimization, but they have you, I believe the policy and force are enforced one bit, but of course I have to enforce one of their I have different options, but you select one of them and they have to match them. So this part is just like a hardware thing. So let's set this policy. And Paul is just asking this format to see the baby for the threads that we have for the different worker. So this really happens. And then like sometimes this will happen at the in the beginning. But this interaction here when, these three policies usually happen again. That is in the scheduling face. And this interaction here would be happening only at that We mentioned when we like when you want to start mission control or control, but it's not part of the software, this is part of the interface. So another layer, different modality is that that's before that. This is even before deployment. If I have a query, a skill query, once you provide this with, I started translating it somehow I'm providing an initial idea of whether this query will be happy or not, and then I ask that Mission Control to pass on. Even sometimes the mission Control can take decisions without even looking at the query. For example, I know that there are a number of users now in the system is and important session about that, so I'm not taking any other. So I could be even very early compared to what we have like optimization, Godzilla, changing the optimization modes. Maybe like but back to me with the indirectly important balance is the quality in can also change the policy with anything. No. So here you can implement a policy for fairness, policy for throughput, but at the running time you choose one of them only to be satisfied. So you don't switch between the learning agent is not for choosing between. There is only for the results of prediction. yeah. So like somebody should square like queries that requires meaning. For example, like I keep iterating over the whole data that they have and there is no specific operation to do like for example, I don't do any aggregate query. It's like submit the political goals. And usually we do this to keep the resources. Maybe some of the attacks happening like this to to have a put them into the system. But we call this malicious for it sometimes. So we have like that early detection or this kind of templates of the squares and get to the any questions so far. Okay. So that last part here is that this gives skeptical Yeah. So in the background is not meeting names right these are listed some very good by the sample so that user and it suspended one of the and then really go back to the yeah yeah of course yeah. So this goes without saying so after after I suspend it's here and they have enough resources after a while I automatically without giving back to the user or whatever, it will put this into action and even give it to you already over queries come. Okay. Because now this one is already held for resource restriction. So now starts to go back and looks okay. So this is an example of a nervous query scapula. So basically all what we talked about. So there is the basis that they have some policy, whether I define as a user or it's already built in, I do different ways to match it and try to improve the performance and minimize the overhead around what's here. We found that still this is not satisfying all the workloads and values that we have. So we have some workloads that even very complicated to satisfy was one heuristic. Okay. And we decided to use some techniques for machine to improve that. Okay. It is now this existing things that we just spoke about, they will adapt to the changes in the workload. So if I start with one a restrict and that will work with changing, then I seamlessly don't know exactly what's happening. So I sort of started degrading the performance. Okay. And also most of this theorists, even if they are doing this dynamic problem, they are not take into account the full information that we have about the execution, but it just is still using something about that amount results. Maybe like, for example, in books that we use this agent to predict the resources and then based on that, we do something. But this applies to books that can be like overcoming this limitation a certain limit, not like completely them. So what we need here is a totally different approach to learn an end to impulse. So I give you a quiz and then what will happen here in the square is based on the execution patterns. As we go, I learn some stuff and then I predict based on what I learn, what I should do in the next step. Then. So this is very complicated. I'm not going to go through the details. I'm going to give you the high overview idea of how the framework works. But there are so many details about how to do that. Okay, so I'll leave this for you. Like to read it and if you have any questions, I'm happy to answer them. This is part of my work, so I know every single bit about it. So if you have anything about this angle, okay, so what happens here is that we usually use something like feedback based machine learning, online machine learning techniques. There are different ways, like we have this multitasker learning, we have active learning, we have the very common one is the reinforcement. If any one here, the box reinforcement, then okay, so you know the concept well. So reinforcement learning is very popular now because interactivity is reinforcement learning based. And even if you don't know about this before now, everyone suddenly delves into this reinforcement learning and now everyone becomes an expert. This one. Okay, so a lot of techniques in the reinforcement learning and they have categories and they have their own use cases. I'm going to just describe the framework, not like the details. It is the okay. So what happens is that I see that we have this place. So okay, because this is this is taken from my presentation and so in my presentation I used to represent backwards in this way. Not like that back in the beginning. I usually represent this as execution floor, but I start from here and I finish the execution. So usually we plans, we have the route. Even if we have the execution starting from the right so we do scanning, we then do joining and so on. Here I usually go with the flow, so I start from the least and doing. But this would be the route. This would. Okay. So typically what we have here is that we have the pretty plans. Okay, This is like very abstract. I'm not going to give you that that notion, but let's say this. We have this and at the same time we have this execution environment is it's an environmental. You could be abstracted with that course or the threads that we are building for this course, could assume that you have to read at the beginning. We have the quiz. We don't have anything executed. So the information that we get from that environment is, no, we don't have anything. So in the beginning, what I do is that I have different steps to predict decision. So what I'm doing is that instead of just saying asking as tabular, you tell me what should be done. I ask the machine learning model an agent to give me an idea of the task and that idea of CPU that should be measuring distance. Okay, so that's it's prediction. Okay. What I do here in the beginning is that I find that in the first states that I find something called good encoding. So I get to information that they have from the execution environment, which is now in the beginning the number of threads is executed. The number of tasks I did this information and they get the information about the quiz. What are the squares, whether they are on the quiz update, quiz the number of tasks, if huge information and I encapsulate that into ambiguous embeddings is just a representation for like numerical representation for the machine learning to understand what happens. So machine learning understand the logic, you just understand the numbers. Okay, so I find a way to represent all of this information into like visual representation, like what we have on the operator level. And this would be equal weight. And in level we have this embeddings. And what we do here is that after we do this embeddings, so now we have this different operating level embeddings, we send this to a new network on your network that will be predicting the decision. Okay, we call this agent. So the only decision that we think is that for this CPU, I decided to have this for one difference and for that set for that. This is the second one. The third T2 would have four five from here. So I can now start getting cooperators from different with if this optimal thing I'm not because I'm in the beginning, I don't have so much information. So I speculated somehow based on the information that they get and they give you a prediction, okay, what I do here is that I go and run, and after I run this, I record the information that they have. I run this operation and that this time then I have this information, the partial information, and give it back. Also in Escape, too. So now I have this information coming from the environment. I already executed this. I have this partial of queries. I keep doing the same thing, creating a new encoding based the information that new information that they have. I have this partially executed. I have this business of that threat and they have this like possibility workload and so on. And then after that I think I have a decision or three and or six and so on. So now I keep running them and we keep doing till I finish the whole pack of police that they have at this point. They had a complete information of what they did. So I know now that this is finished in five second, this one in 10 seconds. So I keep this information and the intermediate decisions that I took for myself to train the scheduler and a few to learn from my mistakes. And after I do that, I go and train for the encoding and scheduling prediction to make sure that this will be performing better and maybe in the first workload and the second workload. I'm not doing good, but since I'm using this feedback along the time, I'm getting better and better and maybe at that workload and I'm okay. So that details in how we do this in the run because you know how we do this steadily prediction. Okay, so these are the main questions that we have and elicit that we like answers through our design. So basically, what do we need to encode like how can we represent the execution information? How can we present that query information? Which way of the presentation is is it read the vector presentation or there is a three representation or there's another way of representing and also how how can we do that? Scheduling predictions. What are the decisions. Is it like only the CPU side that thread it or only should I predict the portion of the pipeline that should I run example if I have a complete pipeline for operator, is it good to run the whole operations together or run first 2123243 or what? So if we have this decision to make, we have to make sure that we take into account all of this information how we predict them in the machine, learning. Okay. So a lot of details there. So I'm just to give you an idea, if you're interested, you go there and you read about this in the paper, as obviously you can see here the process that we have a flexible thing. So after the workloads are changing, the models are changing, the prediction is changing. So I'm thinking something new into account. So that's better. And also one thing that we noticed through our experiments that even we can come up with scheduling decisions that are not human, like, for example, it's not even a first come first serve or it's not fairness or it's not true. It may be something that can mix all of them. Maybe in the first two operators, I'm like, maybe try to preserve that throughput. And then after that they might be the fairness. I don't know exactly how this happens, but the machine learning approach have figured out that this should be minimizing the whole workload that we have. So sometimes, even if we don't have an intuitive approach, it's still better for optimizing the whole workload to give it another reason. But there is one obvious overhead of training things and also the decision time itself for taking the machine learning decision. Like, for example, I, I want to predict this prediction is happening through a machine learning neural network, right? So this new network is happening through numerical representation. There is multiplication operations happening. And so this can take hundreds of milliseconds if this query a workload is about updates like OTB, this will not be suitable. This will be only suitable for lab queries that can take for example, our store. If I have a query that can take one hour, I don't mind spending maybe 30 seconds planning for this one hour to make sure that instead of running one day times one of them. So this is not suitable for everything OTB workloads are not suitable for this kind of. So this is really for the long running workloads, of course. Okay. So let's recap you what we had. Yeah. What are some of the feedback in all. Yes. That you should fact basically something. So we I didn't go through the details of how we do this. It's called rewarding and reinforcement learning. And basically here the reward should be this it's like reward and punishing. So rewarding should be increasing. If I minimize execution time and punishing should be increasing. If I increase execution. So this should be the case. So we usually use execution absolute execution time. Okay. We're able to stop it. I mean, so when the situation is optimal. yeah, in practice there is no automatic like it's it keeps running usually. But we put a threshold on the number of patients that we will use for the reason the order that we are selecting are predicting one. One more thing during the visit. here we are not in this decision. You know, the first prediction of the three and six, another one. And no, I mean this kind of just abstraction for the visualization. So what I what I do here in practice, I'm not always I'm like, I'm not predicting only one operator. I predict a subset of operations. But for example, like, for example, if you remember this pipeline that we have, you, we have like three or four operators together, maybe I run the first two that on this one I just visualization, keep it simple for you. Just the decisions on the opportunity. Yeah. One caveat to you that we are taking the decisions based on the operator and you want gets why I'm running on the operating level. I'm not talking about that because segments when you mean operators, I mean like for example, this is the scanner so I'm not like, so in the tasks that we had before, we can maybe like this segment could be scanning the first 100, scanning the second 100, 100 here I'm standing, I'm like predicting the whole this can this is somehow not an optimal thing, but I'm forced to do that one. We are more data but we do don't already give us ambiguity about that becomes I know this is related to the overhead that I told you if I did the decision for every segment I have to ask is a new your network for every segment and they spend 100 milliseconds or every segment. This would be killing me. This would be similar to the context switching that we do here. So I have to compromise. I have to do this on the operational level. Okay. And I think the decision after a while, like for a more coarse grade level, like on the operator level, another extreme that would not work is that to people do this on the level. Right. But this will be having the same drawback that we had before. This will not taking into account the execution part. Right? Because I plan everything, but I have to still be on the same page with what happens on the execution time. So I have to be operated by operators. Okay. Okay. So to summarize here what we have on the second. So we have two types of segment. Second dynamic, we have also your risk based and based on what each one of them, we have the pros and cons. We collaborated a little bit about this as the main lesson that we have from every class. It is no obvious winner choice. You have to be careful when you design your stuff. That's the case In the next class we're going to have, we're going to be returning to the execution somehow. But execution was very clear from the big drive execution. This is the typical one for the hybrid storage out. Now we have this corner thing. So how can we use this was it sounds like that. So we're going to spend some more time on this. Okay. So yeah, so this is included in the inputs that you what's that is not unexpected to be. It's a different system. It's running. It's going to be by a batch, so it's running this. So what this project, when this started, we wanted to improve it but we wanted to implement also provide back flexibility, found that it's not optimal. So built a completely new thing. And then we have this integrated list so that the original author of this project is I don't know if you know him or not his genius, because he's now.That's. Good morning. Good morning. So couple of things before we start. Today's last regarding the project, I really like with the list of projects that should be potential for you, I'm going to discuss this with the D.A. this week and probably will be listening to you by the end of this week. Meanwhile, if you already have a project in your mind and you want to report it, just go ahead and send me an email. Was that the initial idea? This is not a formal proposal that you are going to submit, but at least the project, the open source, the main idea? Maybe a couple of sentences. I don't need to know the details of that project member like who is going to be who? But I just need to have this email that is from one member of this group, like as a representative, just to make sure that everything is okay. That's the thing. That's if you have a project, if you don't have a project in your mind and you prefer to wait. I post by specialists. This would be listed by the end of the. Regarding the reports, I we have before reports again for that presentations. But I had a question on because I don't know if all of you saw my answer or not. How can we select the repeat presentations? I already mentioned this before, but I'm going to like reiterate on this nowadays. So you already have review reports. You select a paper out of the list that I provided. Okay. You're not allowed to use the same paper that you selected for your review report to be presented. So I have a list of three papers. For example, you select one of them. The other two, you're allowed to select any you want to do. So now there is a case that many of you will be overlapping because maybe you're selecting the same papers and you selecting this for report and then you select the same paper for the presentation. It's fine, but I'm not recommending this. What I recommend here is that you try to avoid overlapping with others. That's for your sake, not for my sake, so that we can present many materials during the class. That's. That's it. That's the whole look, if you want to present one specific paper. It's fine. Of course, you're not allowed to share your presentation with others. This will be heavily penalized. Okay, that's it. That goes without saying. But if you want to represent the same paper again with your own word, with your own material, that's fine. But I don't really recommend this. That's for your own sake. So me already providing you a list for that, the paper that you select for the presentation. So I'm assuming that you already can see when selecting favorites. So you should check this. I take this list I think two days ago and I find it still incomplete. So many people didn't put their voices. So try to do this as soon as you can. Maybe by the end of this week. It's easy to do that. Okay. Again, you were allowed to select one of this things that you didn't select for your review report. You the person. But it's not recommended to have over 11. That's why you can select anything outside of that list as long as it's in that covered topics before your presentation. Again, the reason for that that you I don't want to have this case that you present material that maybe you spent time to prepare that presentation so you know about the topic. But others didn't do any background checks for that. So we are not going to follow what you are saying. Okay. So the only reason for like restricting the topics that you choose from to only the topics that I covered for your presentation is this is okay. That was clear. Okay. Please, if you already have your choice in that and the sheet and it's already based on one of that paper that you did your report on, please change this. Okay. This is not going to be a. Okay, go back to the reports. So for who joined very please be already contacting me. But in case if anyone already joined by then know what to do, you're allowed to submit the first report along with the second report without a late submission. Please. That's only for Rejoint. Very late like life lesson. But it's okay. Other than this late submission policy, is it still applied? Okay. Any questions regarding this? I'm assuming that you already started working on the report. Again, also another clarification in the syllabus. Initially we had this deadlines for the weekly reports on Tuesday. Why it changed this to five days because we had already one shift from that, the first report. So I don't want to put so much pressure on you to do like I mean, to report from like in four or five days. Okay. So now our weekly report will be submitted by the end of Thursday. Okay. The portal will be available for you for the little mission paper deadline. But we know that the submission policy will be affected by the late submission which would be applied. Okay. So Blanca, still any confusion or any question about report, about presentations, about anything from the institute? All right. No, not right now. But it's recommended to do this by the end of this week so that you might if you have something in your mind and you want to choose, you have no chance to put this first because people see that you already choose this, so be it. You know, we go for another selection. Even if they'd said something that was the. Yeah, yeah, it's fine. You can use. If you want to wait till the end, it's fine. But now you're limiting your selections, right? That's your own choice. Okay. That with a very selected review presentation, you can pick any one from the syllabus, but you haven't done a report on right or I didn't cover in the class. So I this week I didn't put the the reading list before today's class. But I'm going to mention here and I'm going to put this also on the upper class, a date. But typically when I am approaching a certain class, I post I post the list of the readings that would be covered in this class. And before the end of the week, I post also the list of the readings that you choose from in the report. So now you know, notice two sets of papers you should avoid the ones that I'm using in the lecture and also the one that you selected from this potential list for the porter course. Okay. Anything else you're allowed to do? Do not advance which ones you use for the lectures. Yeah. I usually post this on on Beata before breakfast. I mean, if you're going out in the piazza, you will see the very first book I'm already covering for the weeks and also in the beginning slides of every class here. I mentioned here. Today's class materials will be covered in this paper. So now we know in this presentation what are the papers? So avoid features of paper that and then later on in the class and cover one. that's a good coordinator's. Okay, that's a good confidence. Okay, so what I'm going to do is that I'm going to fit that. So but was this kind of situation please try to like put your selection by the end of this week as much as you can. Then I said this list before preparing the class material. I already prepared some of the classes, but I can change it around. Or at least if you have something that I know for sure that I'm going to present, then I will contact you. I'll let you know that you should have changed this, but at least like try to put your preferences in advance by the end of this week. Keep up with this conference because obviously all connections will appear because this is my first semester teaching. But it's good to know that the text. Okay. Any other questions? Well, I'm going to read something I really don't. So what type of exam would it be and how should. Yeah, so the exam will be a take home exact. Okay. The specifics of the details are still not clear. Like for me, I'm still developing a build on our material to cover before the exam, but the site will be for the exam. Like you have some questions about approaches, techniques, topics that we discovered or like we discussed about potential pros, cons, maybe I ask questions about the second situation and then ask you opinion about what's that suitable design for this from your perspective, and you have to justify what you have stuff on. Okay, It will be an easy question. Very rare to have like maybe, you know, numerical or something. We don't have interpolation, we don't have any like having zero or one answer for that, but at least your answer will be will be right. It's in the right areas. Okay. Well, that is a question that's submitted or yeah, the format will be the same for both binaries. Yeah. For yeah. Yeah. Do exactly the same. It's not like we have a midterm. We have final there's like a two point exam. So what we are going to have in the first like in the first exam, the topics that we covered in the first exam will not be in the second. Okay. It's just a check point for your knowledge. Okay, regarding the date of the exam, but I already changed this to Thursday. But I'm not going to be here on this day. I'm going to coordinate with with me with you about this. But anyway, this will not affect you. This will be a second exam and the details about how to do this exam would be possible. So it will not be a problem. So for who already has exams for other classes based, it shouldn't be a problem for you to plan around this. So why don't bother about any like hectic situation. Okay. Want to be bound by, right? Yeah, exactly. Yeah, it will definitely would be limited submission. Okay. But the details for that will be, you know, posted like that depending on how complicated exams by count four would say that this would be like a couple of hours, 3 hours something because it depends on what I'm going to cover in this. Okay, any other question? But any time doing that is allowed you to wait any time during like Thursday. So I'll give you think big any time. It is specifically that I'm going to have that from any specific time. Yeah. Yeah, there will be a specific time. So what's the point? A call? Well, you don't have to be here. it doesn't remotely. That's a remote. Okay. I mean, I don't have to proctoring here, you know? No, this is an OB exam. Anyway, you have to. You can take the materials that you want of. Right. One. Okay. Any questions so far? Okay. But to start today's class, it would be about victimization. Okay, so that's what I'm talking about. Before the class, I, I usually post this paper. So if it's not in the on piazza, they are already there. And so, as usual, we have some materials from the unit that is systems course there and are covering specifically three pieces of paper that talk specifically about how we implement the big transition. Okay. And databases with some specific examples about algorithms and operations inside that. That is, we are already implementing them and they are some of them are already open source so you can check them with you interested if they are not open source, they are already described in detail. So also you come to the so some of you, this is a very, very enjoyable part for who loves the writing code. This is a low level implementation discussion or like class about this. This is how to implement face this, how to go and read the instructions about being friends, about the eyes, about the support from the hardware, the compilers and stuff like this. This is not like I'm in high level design thing, although we have some discussions about what are the tradeoffs and approaches, but it's a still very low level implementation. Okay. And of course I'm not going to go through every single detail. I'm going to show you some examples so you can get the idea from the examples and then you can, you know, think of the other operator implementations or algorithms in the same. Okay. So I already touched upon the visualization before. If you remember very briefly, and I say that we usually use this digitalization as an optimization or of so usually we have some support from the hardware to do some instructions in part, right? For example, you can a bunch of couples who have a set of doubles and it's coming them one by one. You have an option to use some specific instructions to load, like to load some packages of this proposal and then you apply the same operation that you want on all of them in parallel at the same time, you're going to be like, you know, not like involve it in, in the details of how to do this as long as you're providing the right instruction. Okay, under hardware like the compiler, there will provide the proper instructions that can handle this for you without any involvement from your side. So yeah, that's, that's basically what we can convert the algorithms scalar implementation to this part and you leave the hardware or that you have to do this parallelism for you. And why we do this obviously for performance. Okay. So if you have like already some codes that you can paralyze your algorithm on, like for example, you have 32 course you can listen within this core, each one of this core, you can do more parts. So for example, if you have support for doing operations on board, right, it's the same resistance. So this means that I can process all in one just and if I assign this to one of these course so that parallelism degree here can be like, I mean 52 primes for each one of them would be parallelism, it would be parallel. So here we can have this theoretical upper bound for 128 performance game optimization. But of course we are not going to reach this because when we do this victory zation, we still have some overhead as we are going to describe in implementing that in in practice there is an overhead for switching data between registers like switching part of data from memory to registers. So there are a lot of overheads there. So it's not like a pure or X like performance thing that we have. So typically we can end up with two x, sometimes 2.5 X at the most, you know, ultimately that would be but still very variable optimization ratio that we can get if we make use of this because it's a free thing for us. We don't, we don't need to like think of the design heavily to do that as long as you have it with you. Design does implement it in a different way. Again, this parallelism, but so again, so the central section is deprivation for safety inspection, multiple data. Okay. It's like so explained here in the term. And basically what we have is that most of the hardware providers that we have, the Intel AMD or whatever, like meaning like hardware architecture provider that they have really their own semi operations, are all of them having the same set of instructions? No, but the major set of instructions are really common between all of them. Like, for example, we have instructions for scanning, we have instructions for storing, we have an instructions for doing logical operations and automatic operation. And so the syntax could be different, the capability could be different. Like, for example, some hardware providers provide this registers of 32 bits, others can provide 60 or 64, other can provide like 512 and so on. So you have to see what are the limits of your provider and to work with this. And this will bring another issue. What is it exactly the possibility if I write something that's quite reasonable to a certain hardware architecture and when I move this to another picture, then there will be a problem. Even if they have the same ability, the syntax of the instructions will be different. There are some papers that can hide this thing, but they are not the standard. And typically in most of the department says we don't do that. We would prefer to use that data hardware support to avoid any indirection overhead. Like, for example, if I'm using an API that can hide this differences between the architecture, they are again, it provides another overhead for doing this. So now this would reduce my efficiency that I'm looking for. So at the end I'm not going to have any benefits. So I decide to go with whatever hardware I have to supporting and then stick to it. And then we deal with that portability issue later. Okay. So that's basically what we have. Just like in the warm up example here, what we mean here by by having this simple like execution, that if you have like two victims or data that we have, you let's say that we have tuppence an X and tuppence and Y and we just select one column, for example, an integral column for each one of them. And I want to go to operation for summation for each corresponding couple or like column in this and this. So if we do this in a similar way, we just do a for loop. And then for each of you we get to something that's very straightforward and we use what you call this single instruction, single data, because we have the same instructions executed in each one of this. So we get this first one was the first one, and then we get the results. And so but now if we so back to that failed instruction signal of our data, but now we want to switch this to the Senate thing. What we do here is that the same force good that you have here, using different set of instructions, will take maybe all parts together, put them in one register. So this register is like 128 bits. So if we have like four integers, I can put all of them together into a huge like 32 page. So I can look for couples here or couples here and then do the submission once for each one of them for the same time. They also finish this. I got another set of couples in the register and then I do the other operations. So basically your batching your competition. Okay, so that's you, right? There's no problem here or any complication. So we have different types of construction. What I show you here is a very simple maybe additions and as such, but we have like them in the citizen instructions that usually help us in building the basic primitives that we need to build the image that is open. So obviously, since we're moving with data and we deal with how to like move them from one place to another, we change them and then we write them back. So we need some bit of movement operations. So basically we have something instructions for reading data from memory, writing data from like writing data to memory, and also reading data from registers and writing them to register and also we have some automatic operations and usually we do this operations once we have the data loaded in vectors or like or this registers like vector representation register. So we have the critical things for the additional subtraction multiplication division where again, so we have different versions of this. Typically when you have an ADD as an instruction, you will find at 32 base at 64 bit, add 128. So this means that we have different lenses of registers that we can do the same operations on. We have also logical and instructions. Logical instructions are very useful as we can see later, like in some examples for doing some logic, like implementing that back condition logic. Like for example, if those or implementing any other checks that we need to do, we can do this with finding or in exploring. And so a lot of like instructions is doing them and they are very, very efficient because they are hardware friendly, they are dealing with zeros, ones. So that are the most efficient instructions compared even to the arithmetic one. So when you take the latency for this account, usually they are like very fast compared to the ultimate one. There are some tables that implement the automatic operations using the logic instructions. Others, they have their own hardware to do this operations. So it depends on the hardware, but so as a good practice, if you can implement whatever you have using logic and instructions as much as you can, then that's preferred. Then we also have some comparison instructions. This is helpful also for checking the conditions less than greater than, and we can do this also on parallelism degree so you can take different items at the same time. We do shuffling, as I mentioned, also between registers and between registers and memory. We have other specific operations and instructions that very rarely to be used like by common developers, but in some of them they are they are still useful. For example, we have this conversion. This is for transforming the data between different architecture, specifically that 86. So this is like that. This was like the typical architecture that we had for a while. Now we have more modern architectures, but it's still used because of that compatibility issue. So we have to be compatible. Also was the version that that we already designed for this instruction. So that's very rare to be used, but it's still useful. The cache controls and the cache control here is that sometimes you can provide some control on which data to be careful what you did or not. And what do we do? Typically when we develop the algorithms that we leave this for the heart, like for the city, or to decide what should be carried, which should be not. But in some cases that if you know how the data will be changing for the pattern of the data that you have for processing and you know, for example, that I know that the first ten pupils will be always in the memory, so I can put some specific instructions to keep them in the memory or like to keep them in the cache, not the memories, but keep them in the cache, because I know that all the other tablets will be checking against them. Or maybe they are very useful for me. So if you know this, you can specifically use something instructions to pin this data to the cache. If not, you leave it for the hardware or the CPU to figure it out and then after a while it will cache it anyway. Okay. But in some cases, if you know that it will be been there, it will be beneficial if you use this in instructions and not leaving this to the CPU because some cases when you have more operations, these operations compute the like the cache for you and you cannot guarantee that this data will be cached. You still have to play save somehow and put them with the specific instructions. But if not, you need it for the CPU and most probably there will be no issue happening unless there are some corner cases from that cache pollution that these are the typical instructions that we have in. We can summarize the pros and cons for using this in instructions very, very briefly here on a high, high level, of course, if we do this right, we can have much better performance. Again, improvements, as I told you, like we can have to perform on whatever device. Okay. But at the same time, this is a serious manual process because you have to implement things like, for example, this for loops, you have to change them, like to victory implementation. Yes. But with the correct, the rights is decided mostly by fighters for the photos like DVD or I will describe this in just a minute. Yeah. What I'm mentioning here that this is the most efficient way of victimization that you have to implement it yourself. There is another way of like just letting that compiler to do the victimization for you. I'm going to do this like describe this in a minute. But what I'm emphasizing that even if you want to do this victimization through the compiler, automatically, you still have to investigate which parts that you emphasize for the compiler to do, to do that, the visualization or which parts are not unsolved. So it's still very hot on the other side. Even if you do this manual effort, you still have a very challenging debugging problem because victimization can be left to think about. Like, do you have an access to are the registers that you are playing with? If you did want instruction wrong in the wrong way, then you pollute your data. You overwrite other data for other operations. So there is no issue in terms of the runtime, like there is no runtime syntax error, but could be misbehavior for the data that you have. So your your code is running fine and you usually get the wrong answer. So that happens because you messed up with, you know, the register values that are changing in a wrong way. So when you want to debug this, you have to be very careful because code can be easily very difficult. But this doesn't mean that no one's working with is like almost all departments. And now we do this because okay, another very important issue, reading the data alignment. As I told you, we have different sets of instructions that can work on different little instance, like let's say registers of 64 bit registers of 128. Let's say that I have support for like my data is like 64, like doubles. Okay, so we have 64 bits. Okay. And I only have support for and instructions on that 32 bit letter. Does this mean that I cannot use victimization? No. You can still use two registers together. You kind of like do some concatenation, do some workarounds to do that. But this will put you up pressure to do this like you got when challenged. You have to think of everything, how to put this like two registers together to represent one to the client and so on. So this is called this alignment problem. Okay? In most of the cases we don't find it in practice because now most of the hardware support the like that, like the registers was like larger a number of bits as much as we can. I think now we have this 512 bits registers as a standard. Most of the hardware support this, but in some real cases, if you're using a special hardware like for example, GP use or whatever, like sensor, like real time sensor, they still have limited hardware support for registers and sometimes you need to write some good for them and you still have this alignment problem. Okay, so you keep an eye on this problem. If you fix. Now going back to the how to implement this, this is the same question that was asked year. Usually the victimization can be done on different levels and on each one of this level. We have a total between that and the program of control. So the easiest way is to let the like the compiler decide this without any intervention from your side. This means that you don't have any control. I just told you like I'm having discuss control or whatever, just so that you want to make sure that they are filled with the data that you're processing. You don't have any control on this. You leave it for the compiler. But another approach is to put some control. Like, I mean, you put some hints for the compiler, say, Hey, this part can be optimized if you if you like, I mean, have capacity to do this, and then you let the compiler to decide whether to go for this or not. If it doesn't work for this, you don't have any control, but at least you provided some hints. If this approach is the one that we spend most of the time talking about this explicitly, you write them and using the instructions. So now you're forcing the compiler to use this instruction, but the compiler doesn't have any control. And that's just like, I mean, go and like, you know, execute what you have. Okay, So this is an example of what we have done before, but you just leave the code and the compiler will do that because there's no free. Okay. At this point, as long as your code is simple enough and is written clean to be vague, trust. Sometimes we write it in a way that can confuse the compiler and we think that it will be victimized. But in practice it will not. Okay. Of course the compiler is implemented to have safe execution, so it will usually prefer to have safe results compared to having efficient results. So if you write good like this, will this be victimized or not? So this is the same submission problem that we have over X and Y letters. We just know over them. And then we did this on mission one, but then if we implement it this way, will this be a big prize or can anyone get positive? So let's just. Yes. Okay. So he is saying yes, all of you agree with this? No. Okay. There very specific reason for the partner is like the address of this. Right, right, right. That's the thing. Yeah. I believe I need to you. So. Yeah, but it needs like a good observation. So basically if you write this using your arrays, like arrays, handlers, it would be victimized automatically because in arrays, basically this like, like if you just, like, write as a into the array simple and then x this basically will be reserved on the stack or whatever. Like I mean safe heap like place and then this would be isolated for a different for the different data structures like for different stories because y so now there is no problem. And Victor, I think this because I know where I work, can I get X work, Can I get why I'm then where I should put my results in Z. But if I implement this using the pointer implementation at this point of that compilation, I don't know if this addresses point to the same location. Maybe you as a programmer like an invited to for example, when you have a value for x, write it and like you read and write from the same place, right? Somehow if I tried it, this would be wrong From my perspective because I'm running things in parallel. I'm assuming that they are safely parallel, but you are making them right because they are referring to the same thing. So as a compiler, I will just discard whatever you have you as a visualization and leave it that is okay so it wasn't like this needs from you as a developer attention. If you don't want to put some effort to write the explicit victimization, you have to write it in a clean wing. Okay? Because you cannot get the benefits out of the both words are the same, but you have to to do some sacrifice. This is like the nature of them, right? That's the thing. So this is clear. So what we do is that we usually we do this systematic factorization through the compiler for the for loops, for the frequency access items and so on. As long as we place it was a pointer or any dynamic changes in the source. Okay. The second approach I mentioned is a compiler hands. So basically you put some directions for the compiler to say that okay, you can victimizes still the compiler will check this if it has a problem. So an example for that for the thing here, we still mean this was pointers but I can use this restrict you want so restrict key words here will make sure for the compiler that they are unique pointers. You are not referring to the same thing. So you are basically telling that compiler that don't worry, I'm responsible for passing this pointers in a safe way. So they are. You need somehow, even if you don't do this actually in the previous source code path, you still have the responsibility for that you are saying. But they are restricted. They are. Now if there is a problem happening during the execution, this would be your fault. Not for the compiler. Okay. Restrict it is used a lot. You will find it in the in the in many like DB messages but people sometimes don't smoke in the Istio standard but still working people use it. I know a lot of people who are going to using this trick even in like me in Google, but they use it a lot. It's one of the documentation that people still use another way of putting hints as to put this directive. Anyone use this? The people drama. Usually when you like, you use a C++ editor, like when you create a project, you'll find this fragment and sometimes the do you do this. And if you're just like learning programing in the beginning, you don't know what this. And when you ask, you to or whatever your teacher is able to ignore this, it doesn't mean anything. You can simply remove it because it's not going to affect your source code anyway. This is just a direction. Okay. Something that you'll get a hint for the compiler that if you want to like victimized, it's fine. So for example, this IDB here means that ignore any dependencies issues. I'm saying that okay, there is no dependencies here, so you can do this right? Even if there is a dependency, you have the responsibility for that. Sometimes you put a directive say that if there is no dependency, do it. Then the compiler would be checking this. But you are explicitly saying that no dependencies, you ignore it. Okay. So that's a kind of control right on that victimization. But here you're not fully optimizing it. You're trying to optimize the the common operations that we have in programing specific for database. Okay. Like if we have a full loop, if we have something that we access like very frequently unsolved, but we don't have any specific control for database operations, like how we implement scan, how we implement filter, how we implement like data movement or stuff. And so, Okay, so that leads us to that third option, which is the explicit visualization. Now we use that CPU intrinsic and, and assumptions that architecture, so this goes precise the instructions of architectures that the hardware provides to you, and then you implement the algorithms using them. Okay. So for example, that's one, one examples of how we implement, for example, the same like folder that we had before. So what we do here is that we when we have the data inputs, we copy back to registers ourselves. So we force the compiler to put them in the register. That's what we do here. So now for each one of these X, Y and Z, we have a 128 register that will be getting that all batches of papers that we have like for the support apartments for couples so that we looking about here. So we have a reference for the register that would be used for getting this data. And then I know that this register would have four couples right? So now I like to change my loop here to have this two divided by four, because now I'm taking the first four together, then taking the second four together that supports together. And so so I dig them and I like so is doing instructions here. Ignore this one first. Now let's go for this. So now I'm using this loaded for couples from X and Y and do some additions on a certain level because here we have like 32 integers and then whatever it looks that we have here, we stories. This is like I'm storing it in the register or Z that we have for you. So we have this also. So now we have for 32 bits added. So we, we stored in 120 minutes and then we move to the next four buffers. And so, so that's how we implement this because it's using this redundancy. I listed maximum type as equal to work as we're doing for calculating that retirement. So let me skip the next I mean, when we're everything I we've already got for five plus one equals two at least three. Yeah. So then the numerator increase by I plus equal to four, then 20 by 1 to 130. I meet you. yeah. Yeah. For this one. Yeah. I mean this could be already divided by four. Yeah, yeah, yeah. I can divide it back at the back. The divide by four. Is it the next. Yeah. So I'm dividing the maximal died here either I guess four or. Yeah. I mean I mean most of the so the course that I'm having here, I'm not very optimized. It could be different ways of implementing that. I'm just showing you the idea this is amazing. So here we are moving. This was offline So, so we're moving. This was a for a site of four, you know, so we have four double slides. So we move this this a main so this is an example of that instructions that you have. So now we have a bunch of design instructions. You use them to implement your lot. So you can easily figure out how is this complicated. So now we just for adding X plus one, we did this. Now if you missed something you might need to put why and why not? We have a problem. Or maybe you you made something you like 32, you have 60 or so. We have a problem now reading two or but now your planning is to be four. So you know this did not align. So you have to make sure that you doing it in the right. There are some fundamentals here that we can implement using this and forensics that we have to. So the intrinsic stuff we have like adding subtraction, logic, this are very basic and can be used in common. Any programing, language, style. But now for databases we have some instructions or fundamentals that we use when you use and frequently have in our development. So we we should give some highlights for them. So this instructions are already there for a while, but this paper and 2020 put them in a nice way. People usually are little bit from Germany. We have like good skills for implementing low level databases. So the provided there are nice use case for how to use this like intrinsic that we have here to build this fundamental constructivism. So it's specifically we have this masking for remote loading store or certain places and compress and expand and don't scatter or certain places we call selected data. So we're going to go through that one. But so masking, so I'm masking here like it's very clear from its name. So we have a vector or a register and we can mask some values like we do the calculation or we do the competitions that we have here on some of these values and others not. This could be the case and in some situations, right? I can maybe I don't need to do that. The addition for the whole topics that they have, I want to do this for the autoplay or even pulse. How can I control this was in forensics using masking. Okay, so just to provide them a vector that can provide the mask for you to hide the things that you will not do the competition on them, and then just leave the others be complete. Okay. So for example, here, if you want to do this addition, for example, or pick the one vector to okay, so we have here, this is a vector what This is vector so that we have some values, you might say. So all of them are the same. So all of them the same. But here I'm selecting some of them to be computed on. So performing the addition. So I'm putting one and the corresponding indices in this vector. So this means that this operation will be done as an error, this will be done as an and others will be left as is. But they provide me also another option to do something important for this to say. Maybe I want to leave them as this or maybe I copied something from, you know, another memory slot or whatever to fill it. So for example, if I have this memory source, I want to add all the information that they have here being moved here and the places that will not be used for competition. So I'm concatenating them somehow. So let's say that this is like the default value. Like I'm I'm saying that if I'm not adding then put that great for the students as mine. If I'm not this, how can I do this if I don't use the same VM instructions or if I want to do this sequentially, I can do this on two steps. First, I do the calculation of the competition on the selected places, and then I take the other empty places and then put my ideas now using send instructions, I can do this at the same time using one atomic instruction. So they provide you have a way that you can do that. This is here for the selected places and it also move data to be completed in the final. So for example, you whether we have you three by two by compartment and then we have minds to be measured, you end up responding, okay, all of this happened at the same time. Imagine if you do this sequentially, then you have to do the addition and then you go to the memory copy nines or whatever, like default, the value that you have there. And then you go back to the register and do this here we are hiding this latency for you and you do everything here. Okay. There is another variation of that instruction. I told you definitely this was the use or if this was the default, if I don't do anything. So you have multiple different options for okay, that's one. An instruction. It's called domestic masking is very common. Usually we play with masking a lot. You can imagine How can this would be useful like I mean for selecting things for doing I can implement if was was this must be right I can implement other operations using this. Yeah. Means that we can do this or this method fails sales entries this weekend. What do you mean by still that feeling of feeling like not succeeds? It's not an optimal choice for us. yeah. So? So that's an instruction for hiding places. If you logic doesn't require this, you shouldn't use it. I mean, it's not like that requirement that you have to use digital instruction like for example in each add don't you don't need to have masking. Right. It's only use the if you want to mask some of this, if you don't want to do this, you don't have to mask. There is another instruction by. If that doesn't happen, you must in vector. So you can just do that by looking for all of. Okay. So another useful one is the premise. This is like very useful for shuffling. Shuffling means that I want to move data from one place to another in. A specific order that I provide looks like I mean in a corresponding to the like. I mean, I don't want to move the first one to that first place in another array. No. Want to move the first one to the second place, the second one to the fourth to place in another area. And so how can we do this? We use mute. So whatever we have, so we have this symbol that we have here, an index vector for the places that I want to put the corresponding elements that we have here. And so, for example, I have the A and the first one and I put here and this and it's on this victim Will was responsible for muting the shuffling the three. This means that I have to move the data here from the three place. That third place you do the first one for the zoo, the one here for the eight would be going to the second one. So this is the place for the final destination? That's right. Okay. So this means that I have this in the data I select for the first place here is in the output here. But that third place. And so I take it from here and then I do this for zero. I do this, B, I do this. You okay. There is another instruction variation that can put some special values here that doesn't like have any mapping. So for example, in some cases I come with another vector that kind of specify whether I want to do a permutation on this or not that. For example, maybe I don't need to produce all of them. I just need to buy the first order first. So there's another variation on this, but I just use the simple. Okay, so this is another way of using a selection to move data. Okay, Now you can play with it that have maybe three or four registers and now you can stop all the things among these four registers and two steps or four steps based on the instructions that you have already. And then they should really function. Because if I wanted to take that along, let's say in different parts of the register, then I need to selectively know which to discuss to pick this talk about permutation within a given for not doing this. Given that it is a bit awkward to me. Yeah. How can they call it in case I said what? I want to point something. Right. So that's your responsibility as a result. But so what we do here is that like in MapReduce, like anyone knows about MapReduce, it's similar to that. Like in MapReduce, we can do this some phases, like you can shuffle them locally first and then shuffle them globally. So once you do this within a register, then you can do this on a mixed level. Globally. Sometimes you can do something between maybe you. Okay, there is another nice implementation that scale this commute instruction on a level of resources. Like if you have like a different now we have one inspector for shuffling and then moving the four times together. So there is a nice guy for doing this, but that's not standard. This like I mean the standard for Intel and most of that and instructions like you do this on the register level and if you want to do another level on top of this, something among different registers, you implement it also using this. But on a higher level, like, for example, you shuffle like you put one index vector with all the indices and one of this like zero in with your review and the other one would be 1111 and the other one would be 2 to 2. Something like this. You implement this on the. Okay, I'm just saying this is like the the most straightforward way. That's why it's hard somehow to implement using factorization. But you have to think of how to do that. But at least you have the ability to do it. That's the thing. Okay. Another interesting thing is the loading store, which is when we say usually we were to need probably for every couple of constructions, we're going to use it because this will be loading and exporting data from and to the memory. Usually we don't have everything in the cache or the registers, right? Just registers are limited. Like for each hardware support, the number of registers that you can reserve through the source code and doing your program is really limited. Like maybe we have it for the size of the L1 cache, but whatever are so let's say like it's a market of hundreds. But now if you're running out of this registers and you want to switch data between registers and remember like I mean now I finished this implement like finished execution of this set of registers or the data and this registers and I want to move to another set. So now the results that I had from that first round of competition, I want to store it in memory and then we are set to this registers and so on. So we want to use this load and the straightforward way is to load everything sequentially. But also there are some cases that I don't need to load everything I might need to load, some stuff I select. So this is how it works. Like this is a memory and we want to load it from like we want to load some data from the memory to this data that we will be used for addition or subtraction. So we use also a master here that tells me what should be loaded. Okay. The way this works that for the first one, this would be performances of it. So each entry here in the Mask will be controlling, which will be in the victim. Okay, so here I'm saying that whatever you have in the picture means in the first page or like in the first and the first result of this code limit in the registers, we have this, but all the lanes register usually have it. This is not okay. It's like what's implicit in our class and say so or likely. But. AP So here I'm saying that in the first place doesn't leave it as is in the second place. Now I have a control of the memory, so I know I give you the memory. So now say that the first place like this one is already activated. This means that load starting from the beginning of the memory, whatever you have here. So I load this YouTube so the cursor would be moving up and. You would be ready for that. Okay, so I'm saying here that this zero as is now the next one that's ready for me to be loaded loaded here. Right. So instead of just sequentially loading u v w x I'm having in my control to select some of these parts to be loaded, baby, it's useful for me to keep this data. For example, I'm adding this was this or adding this was this. So I need to add whatever parts of what I had before was new data. So in instead of just writing this back to the memory, maybe most of it, and then reading all of them together, I'm just doing selective load to override what I need and then like complete normally. So this is selective play it right? Similar logic for selecting a store. Now I have this vector of output, like let's say that this is the submission output. Now I want to write back to the memory the typical way, as we say, that we write all of this together to simply right now I want to write some of them a certain place, so I select what to be written. So here this means that for the first one year I've done selected or being written back, this means that the second one will be written back. So I give you the memory slot. So I'm giving B here in the first lock. Now I move the cursor. This one will not be written back. This one would be written. So it is still just like it depicted here. I choose to ignore EMC and then wrote okay, so this is how we control this. So now when you are developing your operations, one of the most important thing, how to design this message. This is your control, right? This is your logic. This is how you implement your logic. If you implement this or use it in a wrong way, then your logic would be wrong. You are writing things to right like the wrong places or you overwriting some previous results and so on. But this is your response. So it's clear. So let's make sure that our destinations and so this so in the instruction, so you're providing a pointer to the memory that you start writing back into it or start reading from. So you have a point to it and then I start writing back or start reading. That's the but you have to provide this memory. But otherwise I don't have any information about where should I write back or should I read from. Okay, let's take a break for 5 minutes and then we as a set added class. Let's sorting out the last five. Did you manage to submit your reports of. Yeah, I did on great stuff. Okay, great. So I was wondering if you have recorded Zoom for previous lectures because I checked on Blackboard and finally there is the lectures. I mean, the first couple of lectures, I think we don't have a recording, but in something from the third, we have an elephant in the blackboard. There is a blackboard and we'll find in the content. Usually you will find a PDF and a video ended with underscore got in error. Yes, but you know, it's it's quality is not that good because you know, alternating anyway like if you have any questions you can come to my knowledge and cover any parts that you don't get in the previous lectures that's on your syllabus. Right. Like the same process. yeah. Yeah. I usually to the same days of the lectures Tuesday and Thursday, but from 4 to 5. Yeah. Thank you. That's when I'm actually You talking about that. These are my windows. Same addresses. So what if they end up pointing to addresses? Like the data would be the addition of X and I would be stored X. Yeah. So let's say that here. I'm saying that I have data from 022801701234. But if I want to do the victimization, this means that I want to do zero one, two, three in the same time. But if they are pointing to the same location or like for example, Z is like, like if I get the value from X at least zero and I'm writing it in place one now, if I'm doing the parallelism 0123 now there's a problem, okay? Because I'm writing two place that you're reading, right? That's right. Let's think about this and you have to address in a studio, right? Yeah. Yeah. I'm 45, okay? Because I had some issues when I was reading the paper. So. So be thinking in that Max by, for example, that you who, if the thing is not exactly a multiple of four, then we would have a way of checking for that. And then. Right. So what we do usually is that if we like, for example, if we have data that ends up as six points and we have support only for four points, we usually have two registers like four and four. But in the second one we put the control like a normal character or like in the last two bytes and we have to check it in the last register. Whether I'm ending my date and it's still just reading garbage, you know, that's the thing. So like, okay, it's just six by eight. So it was willingly supported. This doesn't beg for four for two. Yeah, no, but if we have it like not for either. You can check for the last two, like to ignore it or you can use a selective operation. Like to make sure that the ones will be in that first two bytes and the zeros will be in the last two bytes. Doesn't look like this is our final like my transition slot time. They decided now. Yeah. I think it has as a as a possibility. yeah. You catch up with them after the class. yeah. Okay. And also for the review reports, I didn't see any like, clear, like Manno. What's the things that my class has to. This points for the reinforcement. So so and then sections of like guidelines for the report. I already mentioned this like you have to justify what you writing about. Yeah. Yeah, that's the main thing. Yeah. Okay. I see. Yeah. So as long as as long, as there is no zero or one it's more of a critical thinking from your side. Okay. If you say that I don't like this because it lacks a support for something, then you have to justify it. Why this? If it's reasonable, then that's like, okay. And also for like first or one priority, like for the first review report, I feel like for like we are going to write the concept, like the things that you don't like. I feel like there's, like I'm mostly like in the paper, so I feel like writing this. Well, I mean, that's how it's hard, because usually in the paper they try to sell their paper. They say that we're doing this good, good, good things. We usually hide the back end. So, yeah, somehow have to figure out what are the bad things that they are finding. Okay. Or do you think that that should be maybe they are not having any. Okay, but at least you should express your issues with what the courier has it like. It is a must like. It must be like three points will criticize then that should be the case. Usually any paper will have more than this. Okay. So like even if they are very good review on paper, right? Yeah. Yeah. So try your best to find our pros. I thank you very much, but look at my mindset. Okay, guys, let's go back. So we continue with this, like in the instruction, which is that compress an expense. So after we do this selective load on the stored, there is one important structure and instruction that we want to use also in a frequent way is to compress the data that we have somehow. So let's say that you're doing this addition for order, even like for like a large number of data have been, let's see, like you have this for us only then all the register that you have, you would be having some empty slots. And the one I want to work on, the output, I want to reduce all the empty slots that I are going use and compress this into one vector, for example. And then I continue my computation or whatever next steps using what I have. This is like. I mean that comes up with that I get. So we provide you this. I send it to using the compressed unexpired. So for the comparison, you have this empty vector, okay? And you have this index vector. This is like your controller and this is the value. This is about the vector that you have. The basically what I'm going to do here is that I say that I'm that selects some of this data here to be put here next to each other. So for example, here in this first one, we see that this will go here like it's not like you put this in one place, it will be written by default. One does not to the end, but based on what you select on. So here I'm selecting this like I'm going to write eight and I'm going to skip this, skip this, then I'm going to write it and then this part will be zeros. Now I can take this vector with another vector input and then I can put one in one or maybe like, like 114 for other vector, like in another place to put this in this place. So now I use this one output vector for compressing the data from two to time, but you're running zeros. That is like notify first and then I know if I already and so, yeah, like in the second factor that I want to press, like I say the second. Yeah. The Yeah. Because when you write it read it back right in the back. Right. Yeah. So there is another instruction or like another masking. I thought if you got ten select the places that don't want to write them, but at least you have here this clear control controller that can by the places that you selected. Okay, this is like the input. Okay. You can think also of this on the other way. This is the expand. Now if you have this input vector and you have this ending cycle, this is like selecting one. This is how you spread the values. So let's say that it's like the values that and they want to spread it in different places. So what I can do is that I'm saying here that in the first place don't do anything because it's and then they put it here. And then the second one that what would be left as is and then the next place would be or BQ. So I'm spreading and B and ignoring C and maybe I'm as predicted in other other registers. So if you can think of it as another variation of the selected little don't store, but it has a different way because here it's not working on the memory slots, it's working on vectors. This is just a y thing and usually we have other wrappers instructions for how we use them. Consequently, like consecutively on different registers at the same to spread values at the same. So that's another way of compressing and expanding. So the main objective of this is how to, you know, reduce the data that you have so that you can continue competition position. Okay, okay. Also that get around, select the getter and scatter. It's another variation of this comparison experiment. But now we're working on the memory thing. So basically what we do here for that, so we have this memory memory spot and we have this and spectrum. Okay, so what we do here, this is together. So I can gather I'm getting data from the memory. Okay? So what I do is that this is the individual. I want to know the things from the memory. So each place here in this end expected would be controlling the because that I would have everybody. So let's say that this is like the old would say that this is the old and they want to override them. So what I'm doing here that I see that we take this value from that place, the index tool here, from the memory, which in the first place, and this one and this five and this three. So I'm selecting the places that I'm gathering data from the memory. That's the thing. Similar things here for that selective scatter. So I want to write the impact in different places in the memory. So let's say we have one place here for the individuals to control the corresponding place and the value vector. So here I'm saying that right this in the index to write this, see it in one right this year and firefight this year. And three more people are there like for example the 420 not budget needed them beat them as this we have them for other reasons starts to modify or we don't want to modify them. We leave them. Okay so this is like any selective going to be selected this time. But now we see from this different constructions we have different ways of controlling things, expanding, compressing, loading and writing back to the memory, moving things around in registers, doing addition, subtraction. We have a set of tools that we can. Okay, so now how can we use this kind of thing for why do the existing this is the this is a paper. It should be written using. It should be written. This is a way. Yeah, yeah. This is part but any other question before we move to the. Okay so just to recap, so we started from the basic intrinsic and then we built some primitives. Now we want to build the operations that we use internally and we use this different constructs that we have to build a database operation. So of course we are not going to implement every single database operation using this victimization. But there are some common operations that can lend themselves to that visualization to be implemented efficiently. So I choose only to select from the high statements. I'm gonna just show you example of how we implement this operations. But there are some other operations implemented in this paper, which is the state of the art by piece or using victimization and database operation within the segment 2015. And it has different operations. So I'm going to show just to show you the first order. So the first one is a selection scan to remember this branch in this code that we had before, right? So we want to scan things and we filter them and if they are satisfied, acceptable condition, then we return this as an output. Yeah, that's a typical thing in a scan. And we did this in a branch right? That's another objective of doing a branch. And good because if you want to implement this in a victimized way, you don't have enough or conditions to be implemented using registers. If you want to implement something using s conditions, you have somehow to think of how to implement this in a branch this way and then you provide an equivalent representation using factorization. Okay, so prejudice can be useful by itself for avoiding this prediction that we have before the branch. This prediction. And also another advantage of it is easy translation to the factorization problems. So in this example here, how can we we do this using the victimization? So for example, we have already this skill practice for the school. So what we do is that for each we could use anyway, remember this in a way, and then we play with that controller. The idea to override things. If we are noticing that I'm hitting back or like most, explain the condition. If they are transferring the condition, I'm moving the controller so that I don't operate it. And so, Okay, so the equivalent representation here, you can say to that there is indeed a section that simply this is not like that, but actually intended point you see here, they're going to have this and 128 something. I'm just like having to sort of quit for that. So what we do is that we have some instructions for loading this, using some whether I'm going to use this straightforward loading or selective loading or whatever this or different instructions that you can play with to implement that. And then you have also some instructions for doing the comparison, using that logic, using the at the shift operation. If you want to do something or not or you want to check the things, you can also have this condition comparisons. You have the larger list and so on. And after you do that, the results that you get, you pull it back if we have something that fight. So we supported back to the outputs or not, and if that's fighting the condition, then we keep it as if not, we like increment that. This can be also easily implemented using some adverse and okay. So when we see this working in action, like if we have this different piece. Okay, so what happens? What happens is that we see that we have the sixties here. Now we do the comparison for all of them using muscle here and then this mask would be with a set it see it to do another compress for the other for the it offsets. So whenever we check something and we found that despite, then we have the offsets for making it. So after we do the comparison, let's say that the spectrum looks like I mean, for all of the six things we have, this and x one and x two and three and it's four, let's find it. So now I know this offsets, that will be simplifying the filtering condition. Now I want to move after that to do my competition. So I want to select only this places. I want to do this using compression. So I just select the topics that I will put in competition with. So I do the compression, I get one, three, four, and then I continue competition after that. So now that register, that would be at the end you would be having the output or satisfy us. Okay, So basically you can play with loads store logic operations like comparison here and compress. So all of this instructions can be played together to implement this, like this. And after that you get to you that the offsets and then you continue your competition with whatever logic you. Yeah mean we do this more what are we manually designing like the currently the mask is that that's the mask here is kind of the output of this component. So I'm getting my output. This is like the output on other operation, like the comparison. So I'm using this as a mask for another operation. So I'm using it as a mask for me to do the compression. Okay, so I'm the mask. You would be an output from the comparative. It would be used on once. And then I'm thinking this use as one, using it as a mask or another instruction in the context. Okay. But there is one problem here. It can still reduce the efficiency of what we can. Anyone think of our corner case specific to that and the comparison that we have. So we already played this trick of copying, copying any way and then overwriting this to avoid the missed prediction. But there is another problem that could be happening that we cannot avoid even if we do this tricks because it's not related to the Miss prediction. Let me give you a name. But what happens here if we have too many pupils that are not satisfying the condition, but we have very few number of pupils. So despite the condition as a prediction, we are not going to have a problem because you overwrite things. There is another issue that could happen here in our factorization. This just gets zeros, right? So we have a lot of data that will be zeros and this compression will be meaningless because now compression will be happening frequently. Actually, most of that there is just as good or empty or like having zeros not once. Right. Will this be efficient if we keep that so screwed or like our own implementation as this. So if we keep it as is, here's what happens. So we do the comparison, we implement, let's say so we do that compared to the like reload, we do the comparison and then store and then we return things with other parts. So now if we have like so many things would be empty or not, let's find it. So most of the comparisons that I did are going be wasted. And also most of the places I have here will be wasted. And also after that I would tell to you, or let's say one hard for each 100 couple to topics to work on quantity competition. So now the point there after that that will be doing the victimization will have a picture of like let's say for each 100 slots we have only two that can work and then other 100 will be having to submission. So it's not like wasting the cycles that they have you. I'm giving you the ability to work on 100 in Poland or like 40, but the 400 in parallel, let's say put this for for, for 100 times, but you only give me maybe two or three spots to work on to continue working after that. So I'm not fully utilizing it. How can we avoid this situation? We have things very big back section, right? But this is not during the execution. This could be for selecting the done in the beginning. Yeah, but we have before you guys. Yeah, but this happens during the execution reckoning and index will not be known. This index will not be a problem here because this is during big. I'm not having a problem in selecting the data I'm working on. I'm having a problem that my intermediate output during the execution is a sparse right. I want to make it more efficient. How can I do it before they push it for the next page? It's a bend, right? I make a situation that it's attempts. How can I do that? Then you have right? So I would not move to the next part till I gather for you enough data that can fill the registers and you can run this better. Right? So if I have like a different set of instructions after that. Now, before moving to the rest of this instructions and I know that the data here would be somehow sparse, I hold on, I go back, do more operations like fill this registers and then send back. You remember this technique that we had before that we we hold on like in the pipeline when we we have like I mean, different operations running in parallel or like the overlapping and instead of just waiting for for many blocking parts, we can just keep moving there up and then we do the execution and then till we fill the registers, we just send them back. If you're working on full register, after that, you can stop doing that instructions to you and then continue working. Okay, So this is exactly the situation here. Let's say that we have this example. We have this aggregates in structure. So basically at each table I select if it's larger than 20 and, then I increase that count. Only this actually. But this will be a problem here if we do this pipeline, because now we can stand still to the aggregate at the same time, right? We don't need to be blocked. We can do this, but if we continue doing this, can filter aggregates. If the output of the filter is sparse, then the aggregate operation would be inefficient. So what they can do here is that somehow I can break this pipeline, I can make it to pipelines and see still the same filter. That's one stage I put stage and then after you gather enough from this stage, you can start the next stage, which is about aggregation. Okay, this happens during the execution. So this is your responsibility as a developer. Now when you implement this, you know for sure that, for example, this selectivity ratio is somehow higher. So I know that this operator will be sparse anyway. So I the implementation to use it, if you don't care about this situation, then you write this straightforward in transit instructions, then you are going to waste some type. Okay, so Question Yeah, the splitting of pipeline depends on the cardinality of the. All right, so is it dynamic integer, some things that can group all in one. Sometimes I have a student, right, Right. That's why I say that you usually have different implementations for the simple IT with some heuristics the optimizer will be selecting among them. So this this implementation is one way. Another implementation is to keep everything that one by line. That's all the way. Who's deciding which one I choose to optimize based on some statistics, like for example, I get some statistics from this beta that they have from the previous quiz. I know that most of the cities, for example, or the country or for the cities would be less than 20 this. So I know from statistics from previous studies this could be not true. It could be. But at least I use some statistics to die of my institution. If it happens, then I will get so much performance gain if. Not then I will not have incorrect answer. I will just the greater performance. So it's still fine as long as you could. If the mother is doing a great job in estimating which one should be one in an efficient way, then you're fine. Okay. So what we do here is that we optimized this as to pipelines. Okay? We call this relaxed cooperative diffusion. This is an implementation for their victories. The operators in a way that you put this in two stages. Any pipeline that you have, you break it down into stages. So now we have pipelines broken based on that that we talked about before. Now inside the pipeline, we have a stages that will be broken based on the House pass because they have. So we have two ways of chunking the operations up with them. So this would be inside the operator itself. And for example you have this stage. So how can we move the data between the different stages? We keep our there. We use one of these registers in a way that can be used as a buffer. So basically we don't use typical load and store from registers to memory because I know that this data will be not written back for sure. I'm going to just accumulating them, so I need to read and write them very fast. I put in registers and after I finish the accumulation and then I pass them to other parts, the final output will be written to the memory. So now I can use load and store from and to the memory that's the thing. But here I use buffers. So between the different pages, I'm not writing anything back to them. Okay. I'm using buffers. So that's one sort of quick for that. So so basically here you do the comparison. I'm like, you load you your based on the component and then you put in the buffer. You check that once. If the buffer here is more than the maximum special that they have for the one day, I continue to aggregate, but if not, I keep doing the buffer, you putting the buffer here and then reach this stage after it will be full. Okay, this is not the actual source code. This is just a sort of code. Just look the logic. Okay, so it's clear. Yeah. Compared to the code operations was sorry it wasn't is an advantage of this. It does this advantage of this. But it's very complicated. Now. You have like a lot of now it's easy for you here to figure it out because it's simple pipeline. Now once we have like a little larger pipeline, it will be hard to define this breaking point and two different stages. The second part is that it depends on the workload implementation, but so if your workload is somehow different from one time to another, then this operation could be important. The third disadvantage here for that is that we use a lot of registers or buffers if we have different places. So this is a waste of the resources that we have. Yeah. What's buffer like this variable buffer exactly did register the register, but I use some of it. So this one specific type of register, we call it stream because none Right. Three specific type of register, it's like it's the size is limited and the number of this type of buffer is supported by the hardware is smaller than the usual register, so you have to use them wisely. This is like, I mean, something supported by that, the hardware that you have. So you use one of them, right? So data, basically this is kind of a temporary register that somehow use virtual memory if it exceeds beyond the size that you need, it doesn't write anything back to the memory. It's just like you like to do whatever. So it's more of wound up cache or you usually have this buffer in the cache recommended. The beverages are kind of like a ready register, right? It's an array. This is not very long. And then you can store things of it, right? Yeah. On the right to stay in buffer will be considered as part of the plan. Or will it like all the sites in the plan, they know it. So it's an implementation. So if you have this like relaxed operation Fusion, you implement this. No, we have the right, but it's not like something that you you choose to write to or not inside the implementation because it will only put the buffer in print expects it to be most of the later stage within the point of reading. Yeah, of course. So so basically here I know that what I'm writing due to the buffer would be used in the next step. So that's why I'm, I'm breaking the buffer time, particularly here to be used so I know when I'm doing the implementation. So this buffer is not generated on the fly. Of course if you have a compiler that can generate this on the fly, it's fine, but it will be making sure that what you generate is used so the buffer is needed that you are using for the intermediate ability and from better for plus we'd be forced to use it in the next spot so there is no there's no waste of resources or no one is deciding it. You as a developer, you already implemented this, don't. Okay. Yeah. This is done in this big box by the base of the in this case. Yeah. But in the sponsored speech it's not like this past data and this is very important, you know, so you as a developer, you provide these different implementations, the optimizer will be choosing among them based on the statistics that you get. So whatever you implement does not for sure use the for all. The cases will be selected by the optimizer at different. Okay. But for example, if you were writing this aggregate pipeline, like the filters like scan, filter, aggregate, you as a developer, you write different variations. One of them is writing one pipeline. The second one is having this two stages, one that you provide that the optimizer, the different options that compare uses or snippets for every possibility or mutation combination for each of them. BE Right. So that's that's why I told you we are not using victimization for every single blueprint. We use it for the common things like the aggregation, the scanning, the filtering. We provide common implementation of efficient optimization like this, relax the operator solution for some of the ones that I know for sure. That would be if there are some cases that could be happening very, very. And I don't provide this implementation, but it's fine. I'm not sacrificing a lot. You don't have you don't have to consider this for all the possible no situations that happens during the pipeline for the constitutional ask any questions. Okay. So this is just like I mean, describing do what we have. So this part would be like the are component here is two stages then to get this to pass for making quite a buffer. That's about what we already discussed. And then the final pipeline, this is the fine line. Number two will not be the same thing. So we have this. Okay. The second example. So now this is an example of how we implement victimization for game. Okay. The second part is the table. This is a very important structure, almost the most important one, like it's used there for indexing, used it for joint use or most of the common operation. I'm assuming that all of you are familiar with hashing right and hash people. Basically, it provides you access in order, one based on the hash functions. So what we typically have in the scalar version that we have a has table, you use what you have key and below. When you search for certain topic, you search by the key. If you find a match, you return available. So I'm looking for the payload, but based on the Q, this usually the key is like 52 or 64 bit sides and it's a value. So basically when you build this hash people with each efforts use a hash function, you get an equivalent hash tag and then you know, what is the highest do this place and then you go and you put that the corresponding time. When you want to search for this key, you use the same function to get it. Okay? So it's a very usually hash function is the most important piece in this thing because that hash function can control the collisions. You know, what's this collision that A1 here the about the collision. So basically where there is no guarantee that you will have different excuse. In fact, it's the original proof that at least 30% of the keys generated by a random function would be like this is according to this is specifically if you have like large enough number of keys this according to something called the a person box. It's a theory based on a sort of. So now we have at least four pairs of the keys that we have we'll have to look at. So for each key or like for each like key slots will have three entries would be having the same thing with having the same times. Right. So what how, how can we do this handling for the collisions people keep linking whenever you have a collision, it's just basically a small list. So you go and search for this keys and the smallest or you allocate large enough data like hash table so that if you have a key collision, you put it in the next block. After that, we call this probing in this probing, right? So if you don't have any of this knowledge, you just stop me and ask me for what the mean. I'm assuming that you know this different Haskell. So problem here is that you have, let's say, notice. So I allocate maybe three and All right I'll have or a hash table size. So now if we have any collision at certain key, I put it in the next empty. So if I have a collision when I search for this key, I check the first key here. If it's not mine, most the property will be the next one. If not the most probably will be the first one on. So if, for example, I checked it for a certain size and I don't find it, this means that it's not there. Okay. So this called linear problem. So I search for the key in the first place. If I don't find it, I continue looking for for this key and the next slots. Okay. So what happens here? If we have a scalar implementation, we just go for the key and then we keep looking for. Exactly. Is it, you know, the three? No. Eight? No, I'm looking for K once or maybe in the first place It's Q one. So I get it. Okay. What's the bottom here? See the sequential thing. So now if this linear problem happening to be very, very long, then I'm not getting any benefit from the question because the other one would be dominated by the sequential escape. So people either to optimize the hash function to avoid collision as much as they can, or the optimize how we do the searching or the problem. So that's one key optimization that we have here. We can search for this for all the values that we have sequentially. It's still just one by one. We can search for all of them using some simple instructions that we have. Okay. So one way to use that we do this horizontal visualization, like, for example, here, and instead of just having like one key slots, one key slots, we have four keys. For example, this is like I'm in the size of my register. The number of puppies that we have in this register. So now when I search for a key, I search for the four anyway. So if I have a key, I get the hash value. We are not searching for one by one. I go and search for this key in the four slots. Anyway. What happens if I find it in the first place? Will it be a problem? Because would it be appropriate to check the other places? No, because I'm doing this in parallel. So it's not a wasting for me. It's a it's a thing. I use it. Okay. But if there is a chance that it would be in the fourth place, if I do this sequentially, I'll take time to search for this. If not, I'll get it at the same time. Hope if we do this because. Okay, that's one thing. So I implemented you using one one register or for couples and then we use a comparison in instruction and then we put 21 and mask when we find key. Okay, So now I know that this place, this back to you responds to this place. Then I did the payload to complete. This is the corresponding one. Yeah, but that's one way of doing victimization. Search for the keys. Okay. There is one problem you. But the problem is there are multiple collisions on the same set. Don't use them. So scalar values will be one. Not really, because this would be handy in any way. But there is another thing here. As an operator like performance problem, not a performance problem. It's a storage overheads. So usually here this is the typical hash table implementation that we have. We usually have one keeper slots. Okay, so this is very specific to the victimization. So if I'm, if I'm with Optimizer and I want to switch between the scalar and the position, I have to implement this not a weight as it so I have a redundancy and keeping things redundant and giving them like synchronize it's an overhead. So this is because this is not a typical or I should change my scalar implementation to check for times per property or best loss. So this is counterintuitive. So we can still use the same property that we have here. But now and it's not searching for one key at a time, we can search for 40 point right? Of course, this only limited in some cases that requires to search for four keys at a time. This could be happening, enjoying it or like enjoying processing for whatever it does. So many cases to do that. And in fact we don't usually do that has table retrieval lot. Yeah. In the indexing case, but we usually do this in the intermediate processing of doing operations, aggregate them, so on. So in this case we have to to do the hashing in patches anyway. So let's live doing this one by one, go for vertical visualization. So now we have a hash table implementation now searching for all keys at the same time. Okay, That's what happens here. We dig for you. You get all the data from the memory you have for this case because you have this hash people hoping that in that cache we have it from somewhere in memory. So we do this kind of gathering. So we get together all the corresponding values that we have. You we give it one and register and then we do the comparison. So after we do that comparison, whatever we find, the match we put once, if we don't find matches, what is it? Okay, But here and this zeros. This means that we don't find it. We still have to search for it. So now we increment their next index for next operations. So now if we have other kids, that needs to be checked. I keep this too. And they filled this to 20 keys and keeps us okay. I can be easily done because we know. We know we have this selective load and selective stored seconds so we can using masks, control whatever we. So I'm here utilizing this. Now, after we fill this with new keys, we go and search for and then we'll find the results in. So now in summary here, just to give you two examples about how we do this victory session in databases, this is an effective optimization when it's really useful and therefore for who did this and tested it in practice, it's really good, but you have to use it wisely. If you aggressively do the victimization will have an overhead for switching data between registers and memory. You will have very complicated source code heart to the to the bug. It's easy to be messed up. Like if you have like imagine yourself like writing this an the for they are mindful easy to make this register. It's very hard but it's useful if you implement this for the selective operations that can be useful for it and not all possible things. Okay so next class will be about that critical violation. So quick compiler is another way of doing optimization for the pre processing. Okay. And Somehow we have some ideas from visualization, but when we do the compilation so we will see some terminology from what we have. You also end violence and then we would describe some trade between this two approaches. So we're going to start the review presentations next week. So I'm going to start the first part of the lecture and then after that we'll have to get more questions. Okay. All right. Like I did one question regarding to the recovery execution parts.Guys. Good morning. Can you hear me? Okay, great. So thanks for or trying to attend. You mind? As you see, the weather is not that good. So we are going to have this as the new requests. I'm already here in my office, but I don't want to push it so much to to attend in person. So I like to do it here over Zoom. So to make it interactive as much as we can, we are going to continue our tradition. Like, if you have any question and instead of just raising your hand here, because I might not be paying attention to all that in here, like attendees here on the on the right side panel here. So, I don't know, maybe you're raising your hand or not so you can unmute yourself and then you like, ask your question. I'll try to answer all your questions also as much as we can, but if you don't have any questions, please mutual. Okay. So at the end, we are going to have presentation today. This is like I mean, the beginning of the review presentations, right? So who is going to present today? And please, I'm U2, so the multi. Okay, no problem. So whenever we were going to be ready for doing the review presentation, we're going to do this over Zoom. So you have to share like I already gave you permission. So you have to share a presentation here and start doing this. It will be maximum 20 minutes as we agreed on before. Let's plan for FY 15 like 218 minutes as a presentation and 2 to 3 minutes for questions. Okay. Okay. So let's start today's class. So we are still in the query execution part. We're trying to improve the performance as much as we can. After we get the logical plan and also the physical plan from the good optimization that we mentioned before, that we are going to bypass it for now. So this is the second optimization that's commonly used now in many of the database systems, which is the query compiler. So just a quick note. See here before we start the details of the presentation, we have most of the materials class for the class today will be coming from the CMU advantages of a system course, and we specifically focus on two very important papers about the query optimization using the query compilers coming from very prestigious like researchers. I really respect them. The two papers coming from ICD unveiled to me. So I'm going to cover the main ideas and you are really encouraged to read the whole paper there to understand the details of the query compilers that they are implementing. There are some other resources that I put for the reading, like for the review report. You can also select one of these papers to do the review report, but you're still encouraged to read the other papers because they are really, really important. Okay. So we're still we're still here on the low level optimization part, but now we're taking a little bit different approach for optimizing the weirdness that we have. So we're going to focus on how we generate the source code itself. So in the last class, we talked about the victimization and then the victimization. We tried to rewrite the source code that we have for the queries using a different set of instructions that could be very friendly for the hardware. It could be like even hard for us as a developers to write like longer programs using them, but still very friendly and efficient for the hardware and infrastructure. Now we are trying to delegate some of this tasks to that device itself. So if that department has its own query compiler, that can take that, but can take the queries that we have and tries to find an efficient representation and instead of just rewriting everything from our site as a source code, then we're still on the same level of optimization that we are looking for. Okay. And if you remember, we mentioned that there are three different approaches for increasing the performance during code execution. The first one is to reduce the instruction count. Like if you have a program one way you like its performance is to reduce the number of instructions that you have in this program, because this will be counted as extra work if you have like more redundant instructions that need to be executed. Another approach is to reduce the number of cycles, but in instruction. So you have a set of instructions, but you're trying to avoid any branch prediction, trying to avoid any memory installation. And like any stuff like this that can block the instruction during the processing. This is also another way of improving the execution time. And also that third one is the straightforward approach of paralyzing the execution engine that will work. So in that victimization, we're kind of not reducing the instruction count, but we're trying to make it efficient somehow. So in terms of the cycles and also for the tricks of the pipeline and the like, the class before that, when we described how we create the operators that we are building in our plan an efficient way. This also reduces the cycles in instruction, but we are going to do today is somehow return to the first point and focused mainly on reducing the instruction time. So our objective here is to have like, like very, very efficient size for the source code that we are using for that execution. And by default, if we do that, we're enabling quick execution setup for that big mess. So it will efficiently compile everything, efficiently generate source code that will not take so much from the memory, from the cache, from from the resources that we have here. And by default, we inherit some performance again, benefits like and compilers. We often do this by coding specialization. So here we try to generate the source code that's very customized related to the tasks that you want to do. So, for example, you have a query, and this query is reading from a specific table in a specific batter. And then after we read this like data, we we do some query execution steps on top of it in a specific pattern as well. So instead of just writing a source code that can be generic for any possible case or any possible query or any possible data access pattern, we're trying to generate the source code for this pattern only that we need for this equity and then we link it are compile it and link it to the like the existing binaries that we have in the departments and then we run it. Of course we are going to have an overhead to do this generation, compiling it and linking it. But we're expecting more performance again if we do that. Okay. So the whole process is having so much overhead compared to the performance again that we have, then you don't need to do that. But in most of the cases, as we are going to show, this will have some benefits. Okay. Another thing here that most of that goods that we write as developers, somehow we were kidding, as we mentioned before, and the victimization class, we care about the ability of the source code and how we make it debug able, how we make sure that everything is in order so that when we have some corner cases, we somehow like find them quickly or like debug whatever bottom that we have in terms of performance. But for the CPU performance itself, we don't care about that. So that generated by the compiler will be very efficient for the performance gain that we have during the execution time but will not be readable for us. So it's not easy for us to do, but. Of course, it's much better compared to the victories, the code that we had in the last class. But it's still no doubt the one that we are looking for when we do straight forward thinking, when we write the source code. Like for example, this query compiler generate some source code that will not be following design pattern or inheritance. It will be very, very efficient in terms of linking them instructions and the statements that we are going to execute, but without being any attention to the modularity or being reused, like having good reusability and so on. So it will be very hard as well to the budget. So pay attention when you use your query compiler, because somehow when we use a pretty compiler, if you read some source code that will be embedded in that source code of the DB mess itself. So you end up with source code that's not entirely clean to debug. Alright, okay. So that's, that's one slight drawback. But given the benefits that we can get out of this, we somehow bypass it and we continue doing that quick comment. Okay, Sorry. Sorry. So for the query compilations, we are generating the code like at the Irani time, right? Yeah. Instead of like using the existing like executed, we are generating code. Okay. Right. Yeah. Yeah. So, so when like in the typical as if we don't have a query compiler, what we do there is that we are preparing our source code to handle the different cases. Like, for example, we know that if we're like there is one big F or switch case, for example, that will handle scanning data that has like because it has like integrated types or flow bytes or related bytes or w, Okay. So that's basically what we have in the typical DB mask if we don't have any query compilers. So in order to run our using software, we will have to go and check for for everything just long switch case or long if conditions. So this will be adding so much overhead during the running time. But okay, so what we're trying to do here is that we generate the source code that only specifies that case that we are needing here. For example, we are reading from integral to the types. So we just generate the source code for reading from integral to the types and then compile it and execute it. That's basically what we are trying using that with the. Okay, so is it clear? Yeah. So and then for like query plans and interpret power relations, these parties stay the same, right? Yeah. It's the same here. Okay. Yeah. Okay. So again, like, try to explain this using an example here. So this example is trying to to join data from different tables. Basically, we have two primary tables and each one of these tables has its own ID primary key and also have a value column. And there is another table C that links the like the matching tables from AMP. So basically we use this somehow for like having intermediate joint output or anything that we want to get from these two tables that that have shared information together. Okay. So this is A, B, and C, So on the left side, you we have a simple query. Let's go through its syntax and to understand, understand you what what it tries to do here. So basically we have A, B, C, and we are trying to join a, C, and B, as you see here, B is somehow defined as a nested part in this query. So basically B is evaluated using another nested query here, and then we are labeling it or giving it an alias like B, and then we use it somehow for filtering after that. So here, if you look at this query, we're filtering a based on a certain value and then we are like building B by filtering it based on a certain parameter value. Pay attention here to the question mark here. This means that this will be a parameter and then we do some projection. And then after we get that data here from the projection, we will join it. First we see based on the ID columns and then we have the output here joined with the filtered tuples from a based on another idea columns from ANSI. Okay. So it's a two level domain operation, but now we have an institute here that generates the intermediate output for B. Okay. So what we do here when we do this query, execution or planning so that before the execution we generate source code like this, if we have like a typical query plan, like if we we use that pull based approach that we talked about before with our pipeline or without anything, we just to have this simplicity. So for example, here and we will just read, we just as Canada tuples from a and then we pass it to the filtering operator and then this will be selecting whatever matches that. That's the predicate that we have here to be ready for the drawing. And the same thing will happen to you on the right path. So we have the same thing here. We, we emit that filter. Like this can doubles from B and then we felt on them and then we do some aggregation and based on the values that we we have is we aggregate the value here, we go by the ID and then we aggregate the values. And then after we do this, we join the results here with whatever we have here from the scanned tables, from C, and then we get the doing a bit. Okay, so this is simple pull paste implementation that we we talked about before. Okay. But let's focus on only one part. The predicate like, let's say that here you see this predicate that we have the parameter here. But what happens when we do the execution for this? Like when we do the filtering for that in this, for example, here, when we select this here based on the predicate, when we evaluate the predicate here in this part during the actual execution, this is not simple because if we write this in a generic way, in a way that will be interpretable for for any case, we have to be prepared for all the cases, as I mentioned. So you have like I mean, a big switch case or whatever there for the different operator types. Like if you check the quality or if you take the greater than or less than or whatever. And then for each one of these operators, we will have a case for the different types and so on. So let's take an example here for the operator by quality operator. So what happens, you know, when we have one topic, only one type, Let's say that the current topic here that has this value. Okay, So we have this like this B value at 1000 and it has this idea. Okay. And we have a parameter here that comes as an input, which is like, I mean, nine, nine, nine. So this is like I mean, the parameters that we are checking again is this current table. Okay. So what happens here? The source code itself is not having the next statement to be just checking the value. No, it will check first the operator. So we have to call the function that determines the type of the operator. And then if it's if it's M a quality operator, then we we go here and we check. If not, we have to check whether it's like having on one of its sides having the here assumption on the right side, for example. So we when we go to the summation operator now we have to check its details. So now in summation side, we have a value for the parameter and then we have the constant. So we have to replace the parameter values and check whatever, like any schema information that we need to do, like to access to get the information that we need from the tables or from the tables and so on. So for each tuple predicate evaluation like this, we have different steps, checking the operator, types of checking the data types like having multiple function calls. We are juggling among them. So this will be so much overhead for only one predicate evaluation. And you can imagine if we have like I mean, a billion couple, we have to repeat these things a lot. Of course, we can have some optimization, like we can cache some of this information, like for example, the the start of that, the table that we are scanning from that the tables or having some of the cached parameter values or whatever. But still we have to go multiple steps to do that. Okay. So that's the very bottleneck that we we talk about. You have to get rid of all of this generic stuff that should be working for everything. And you have to generate something very specific to use, like your specific cases that you have. So we want to have a source code for something. Let's say that, okay, if you have this predicate, you just to go on and check the quality based on the value of the parameter after some like the summation was one. Okay. That's basically what we want to. And like, if you look here at the at the bottom, if you see this figure. So that's exactly what we want to do. So if we have this predicate that says that, okay, your value is one, if it's correct, then you filter like then you satisfy the predicate and you filter a table based on it. And instead of just having this different steps, checking the operator type and then checking that quality of the constant was the value. You know, you can just have this inline function that can check the value if it's equal one or. So here we don't have any F that can check that if the operator is equal or greater than or not, or whether the the value at the value of the attribute is integral data type or float and so on. So that's what we want to have. And this is the output of the query compiler that we are hoping for. Okay. So basically this is not just done for for the predicate evaluation. It's done in many, many places in a wide way of doing the query execution. Like, for example, when we access a data, it shows the message that we are using for reading the data and storing the data that can have like a multiple step so we can actually generate specific. Good to do that. Also, in some dimensions, we have stored the procedure like in Oracle. So they are preparing some functions that can be used over and over again. So now you can customize building this function based on your case. Also that could be operated execution. We are going to see some cases like that as well. And this is the predicate evaluation that we we talked about. And also for logic, when we look the operation, the performance of the different operations based on the type of the operation, you have to log specific meta data that you need. And if you have this in a generic case, you have to like, do check every time what's the type of the operation and so on. So you can easily find so many parts in your source code as a deep mess that can be customized for the key execution plan that you have specific. Okay. Any questions so far? Okay. So let's move. So we have two main types for doing this good specialization. We roughly describe the main idea of that. First one was which is generating source code, like typical source code, like what we used to write, but using the compiler, like we the compiler generated and it's sort of writing on our side, like using militarization, any other optimization tricks. Okay. And once we generate this, we run it through the conventional compiler and generate the native code and so on. So basically what we do here is that we run the compiler, generates the source code, and then in the background it will be linked like will be generating that machine code, the bytecode and then linking it as an object code to the rest of the executable files that we have in the department and then running it. Okay. This will be good as long as you're very efficient in writing source code or like your compiler is very efficient in writing source code that can that can bypass the overhead that we have in typical good implementations. Like, for example, you're compiler can be naively missing the direction directives that we talked about there for the victimization. For example, we we can like I mean this as equity compiler, what's more efficient it could be like, I mean, having another overhead of the complexity of the implementation, but more efficient is that we let the compiler directly generate the intermediate representation presentation like the but like that bytecode or the machine code that we can just use because any way the query compiler will be doing it automatically, we are not going to be involved in in doing it. But of course there's another drawback on that on the other side that if there is something happening that like in terms of the the correctness or there is a bottleneck or there is a problem in the performance, it will be very hard to debug it because we don't have the original source code. We have only an intermediate representation. So what happens is that sometimes we can some of the DB masses can start their first earlier versions of the DB mess using the code generation. Once it's becoming stable, they switch to the just in time compiler. Okay. So that's basically the different two approaches of doing recompose. So before going through some details of this tool, let's see who where's the compiler located in the debate mess. So assume here that we have this skill server and then we have queries coming from the users here. Then first we do parsing, so you write your skill queries and then you'll get this squared is passed as a tree. This is not still the plan, the actual plan that we are going to run, but this would be an abstract syntax tree that will check if there is a syntax error in your in your ethical statement. And then once we have this syntax tree, we're going to check whatever we need information to like. For example, transform the queries that you're writing to be understandable by the machine. Like, for example, you right that query in a way with, with many aliases, like I mean you have like what we had in the nested query you write and this is a security statement and then you put an alias for the output. With B, now you are the only one who understands what's B at this level. When you write the source code or like write basically statement. Sorry. And then when we try to translate this and execute this now the system of like the diplomats needs to understand these aliases and what they are referring to in the original schema and so on. So now we read some information or meta data from the system catalog and would bind. This was the abstract syntax that we get to get an annotated abstract syntax tree. This sometimes is called the logical plot and this is like I'm in the different sequences of the operations that we have that we can break into pipelines. Now, once we do this, we pass this to the optimizer to get the actual physical plan based on some cost estimation. We're going to go through that optimizer later. But this basically is trying to determine that at this part of doing the drawing we're going to use has volume or nested loop joint. So now we know exactly what should be running what what are the parts of the source code that should be running the implementation of the operators that should be running? Okay. Once we do this, if you don't have a query compiler, you ready to run now to implement version of your operator that we said that it's determined by the optimizer. If it's in the traditional DBM mess, it will be having switch cases or if statements, as we mentioned, if we have a compiler, we're going to have this query plan and then we pass it to the compiler and then we generate the source code for this operations. So if you have the compiler, you will generate the source code on the fly. If you don't have it, the optimizer would be choosing them from the already written source code that you have in the beginning. And once we like select the source code from the traditional DBM. As for compiling it during the running time, now we generate the the native code that would be used for running the actual quick. Okay, this will happens regardless the design of your compiler, whether it's bytecode generation or just in time compilers. Okay. So that's basically the architecture. Okay. Professor, can you explain the role of system catalog here? I see that it is connected to Binder as well as optimizer writing notecards that Can you please repeat it? Yeah. So basically the system catalog, you think of it, so you think of it as that. All the information about the tables, about the meta data, about which IDs are used as primary keys, which IDs are used as like reference keys and other tables and so on, like foreign keys. All of this information can be used in different parts in the system. So at the binding level, we can use this information to replace the alias names to the actual like, I mean, table names that we have in the schemas that when that like when the equity execution runs. When you say that I want to join the data coming from Table A, he knows exactly what's the address of table and goes and get this information from table eight. So this information coming from the system capital and also during the optimizer, we can get some statistics about these tables. Like for example here, we know that this table A is having integer types and it has, let's say, 1 million tables and table B has like double values and it has like 100 tables only and so on. So I can use this as statistics during the query optimizer here to check and select what's appropriate type of the implementation for each operator to use. Like for example, if we have a few number of tables in each starting, like in each table we can use for a joint nested loop joint because it's easy to do to be running for a small number of doubles. It doesn't have to to build a hash table or have so many complicated steps. But if we have like so many topics like, I mean, we have a table, a having 1 million table and table B having another million topics and we want to join them Now, it would be hard to do the nested loop during the nested loop joint. So what we going to do is that we want to build a hash table and then we go for the hedgehog. So how can we select between nested loop, join or hash join based on some statistics that we get from this system. Okay, so this information can be used in different parts in the system. So that's what we mean by the system catalog. And there are some research efforts now on how we can like improve the performance of having the data retrieved from the system capital because somehow in large databases, this system catalog can be very, very large as well, because we can have very detailed information about each staple, the types of the columns and everything. And then when we generate that information that's needed for the optimizer or the binder or whatever we have to do many, many trips to the system catalog and retrieve this information back. Okay. So there is a resource by itself about how we improve the writing and reading from the system. Okay, Is it clear? Yeah. Okay. Any questions so far? Okay, so now let's start talking about the first type of compilers here, which is about good, generous does so many systems. I think the the oldest one can be, I would say kind of like can be routed to the IBM systems in in eighties and but it wasn't like that efficient so people abandoned it for for for a while and then they switch it to just typical switch cases or if statements there. And after a while since we started to to revive this idea of the query compiler and we started to feel that the importance of it once we have this language because now in the language we have this so many topics, as we mentioned, we can have we can have a scan for billion of problems easily. And now if we do this checks for the operator types and the branch goods, the punching that we have here for the different data types or whatever they are, this will be painful. So when we started to think of that quick compilation again, people started to revive this ideas in different ways. And most important system at this time was like you. And it was simple. It was just like generating another source code that can somehow capture what you need in your operator logic and then leave it running for the compiler or the execution engine to do the next two steps as any typical source code in the departments. Okay, so they are focusing on idea like templates. The main idea here is that you have a parameter like this or you have a query that basically can be repeated over and over again, and the difference will be mainly in, in some parts in the prediction that the whole query is having the same template, but changing and prompting somehow we called it sometimes we call this queries as parameter quiz. Okay. So if we have this template, it can be easy for us. If we can generate the source code for one template at the beginning and then we keep reusing it somehow. So that's the perfect course for the query comparison. So at this time we don't need even to run the query compiler for each for each evaluation. We can run it only for the first predicate. And then once we prepare this template for each for each like predicate that we have after the first one, we can just replace the parameter value with whatever we have as a current value, and then we continue using it. So we don't need to generate the whole source code, we just generate some parameter substitution. It's of this predicates after that. So let's take an example here for for how we do that. Like for example here, if have this scan over that that happens in the certain table for the whole number of tables that we have. So first we get the table and then we evaluate the predicate and then we if like if it satisfies that the condition that we need, then it will return. This as a as a satisfying table for the next operator. What happens here? As I mentioned before, if we have like this typical DBM as implementation, when we get the table, we just to go to the schema. We know exactly the information about the table and that type of column, the the types and whatever they are, and then we want to know exactly who it is. This table is starting to so we calculate an offset using that start of the table on the table size, and then we return the pointer to the start. So now we get information about this table. So now we started the execution of this couple. After we do this like three steps. And then when we do the evaluation, we have to know exactly what is the type of the predicate, whether it's quality less than or greater than or whatever. So we have to traverse this predicate tree and we have to traverse any operator that are type three and so on. And then once we do this, we would change everything. We return this to the evaluator and then we check if it's true or false. Okay, So we have something like let's here for this two statements only. We have like at least seven steps to do because that's the typical. But now if we have tentative blank, so what we do here is that now we don't have to do all of this checks. What we do here is that for that w we know exactly that this column from this table is an integer value. So we know exactly that tuple types, okay? And we know that table was this will be the same predicate over the same the same column in this table. So we know some information about where it starts and the size of its table until so we instead of just repeating this questions for all of this predicates over and over. We do it once and then we generate one statement like this that can calculate where the table will be starting. And then we don't need to check here. What's the type of the predicate. We know that this is somehow I'm an equality. So we we generate like an equal equal operator in the source with itself. So we don't have like as in general keys that have as general keys at like less than or larger than or. Okay. So once we can read this year we have this parameters that we will be only changing if we have different predicts. So now we have double size, we have predicate offset, we have a parameter value. All of this information is used here. So now this will be changing. Somehow. Some of them would be fix it, some some of them would be changing over the break. But the snippet of code that we have here will be not it. Okay. So that's what's generated by the by co-generation combined. Okay. Any questions before about what we expect from this diode? Combine the computer, generate the code on the right side or compiler took the code generated on the right side and write to the binary code. No. So. So we have this generated by the compiler or this is generated. The right side is generated by been rated by the compiler because this will be changing from one way to another. We don't write this for each quick. This would be generated from one source so that compadres are actually our dbmr compiler and then generate the super particle like like the right side. And then the GTC or Clang just took this code code into and. Right. Yeah. So the query compiler is not the same compiler that we used to end in programing languages. It's okay for the database. okay. It's a combo. So the compilation for the database has a different function than the compilation for the source code. Okay. Okay. I see compilation for the source code. Yeah. It has that the job of generating this into the machine or whatever the code that would be low level that we don't understand. But okay this would be generating the specific code that's customized to this quick. So that's what we mean by the quick compile. I see. And then and then like the actual like song or TCC come seeing like doing the wrong time and the compile code to the binary and then we link it. Yeah, exactly. Yeah. It's all happening in the runtime. Yeah, it's all happening. Okay. Okay. Okay. I see. That's the why query compiler is still having some overhead. That's stuff. So you have to evaluate whether it's valuable or. Yeah. Okay. Any more questions? All right. Okay. So this is just like I'm showing what we we mentioned here that we have this like parameters here that will be changing from one project to another, but we generate the source code that will be fitted for all of this, predicates for this type of query using this practice. Again. So let's, let's discuss some of the pros and cons here for this approach. So one important thing here that this compiler would be taking care of is generating source code that is compatible for us to be used with other parts in the source code. Right? So basically here, since we are mostly developing this deep mesh systems in C++ or C, our compiler should be generating source code for the quiz that is compatible with this source code that we have in the deep mess so that we can, as we mentioned, we can create the object code and then link it and so on. So if we do that, we can still use any part in the typical mess because the source code for the queries will not be having that all the code for accessing the data in memory or everything. No, it has to be linked to the other parts in the Libyan mess and we can use the buffer pool. We can have any network handlers, we can have any filter handlers, we can use any other techniques for handling the transactions for the controller. And so so we still need to be part of the whole picture. So this compiler will be generating that query code that can be used and invoke any other function in the departments. Okay. So that's why the source code here is is easy for us to debug as well, because now we can we can like use this like debugger, we can put like any check points at the source code because so skewed is linked among the different parts. So what we generate is also linked to the old one. So we can have like this debugging sessions and so so that's why it's easier as well to go through it. But on the other side, this is still not the efficient way because you, you're automating it anyway. So why don't you just go for the low level, like completion, like, like for example, what it's mentioned in lab, Like now that you can read that just, like what? Like the bashing code or the pipe that we used to have from that clang or that easy or whatever, why don't we do that really? This would be happening in the second approach. But this will have, of course, as we mentioned, that you cannot easily debug it. But now let's see that we took this approach. Now this is not the most efficient way because you still have to compile the source code at the running time and generate the bytecode and link it so this is all overhead to do it and you think of it as is still a considerable overhead because we do it for every query. So in that because now the parameter values will be changing, but the code itself is already generated once the template itself is already generated, once. But whenever we have a new predicate or a new query, we have to build the parameters and then we generate the object code or whatever and then link it. So all of this will have an overhead. And if you have like different users running different ways at the same time, this will be also overwhelmed. So this is not that is way to do it, but it's is still it's still an efficient way in many cases because as does the PBMs delete the binary code after like it depends on the case. Like I mean if it sometimes you have some privacy settings if you if you want to delete everything after you finish or there is a session after you finish the session, everything will be removed. This is like something determined by the settings that the user will be said. Okay, you know what? What, what PBMs actually use this? yeah. There are a lot of them are the most popular as opposed to most of us is using this PostgreSQL this. Yeah and they have also another version of that the the just in time compilation as well Oracle is using it. I think so Snowflake Redshift although they do some customization techniques for that but the same idea is used like I mean over and over again everywhere this is a mainly for like a lab system or it's also the same goes to well, you can use it for everything but here. Okay that's a good question. So what do you think Will this be Will this approach be good for all TB or lab or lab? So in terms of the performance, it's all lab. But if you look through only for that, you can think of. Also, it's still good for all TB a little bit more than all that because we generate the source would like it like once and the transactions that we have in the all TB will be very fast and. If we can read this parametric thing only once and then keep using it, this would be very efficient. Okay. But if we we are not having this case of the parametric queries and we have to generate the source code over and over again for all TB, the overhead of of doing this generation will be like killing whatever performance again that we are looking for in the alternative, because in all TB could be running in milliseconds and the overhead for generating this could be hundreds of milliseconds. Okay, so if we do this for each square, it will be hard. But for all lab. Yet if it doesn't have a matrix at the wish and we have like the large or lab like workload, this would be much better because the overhead here will be hundreds of milliseconds. But the lab is like finishing in seconds so it doesn't make a big issue if we use this query complete. Okay. So it depends on the workload button that you have if it's parametric or not, and whether it's on TB or lab. Okay. But typically, like transactions has like a lot of like sequel queries that ID like compiled or at once or they typically going to compile it like differently for each SQL carry doing a one transaction. No, I mean, so if you have one transaction that has like different, whereas at the same time it will be combining all of them together together, okay, because they are Oracle. So this is like I'm in a different discussion, like, I mean related to the transaction and how we isolate things. But if we have like one transaction, one like one big beginning and end transaction and you have so many queries inside them, we want to like them and run them anyway together. So it doesn't make a problem if we like compile all of them together. In fact, this will be improving the performance much better. It will be complicating the queries implementation a bit, but it will be having better performance. Okay, I see. Thank you. Okay. And also one problem here is that if you if you think of what we did to you, we generate this for each operator one by one. Currently we that the hycu implementation supports only the pull based approach without by blinding without the idea of like I mean like combining different operators that are not blocking each other to be running. So now it's generating the source code somehow on an operator level, not like on a set of operators. Okay, but this is orthogonal. We can re implement the same idea that we have here also for pipeline so the same generation can be implemented using pipeline and there are some systems doing this, but I'm just showing you the idea of the lifecycle generation using only the pull based approach. Okay, So let's take 3 minutes to break and then we switch to the second sorry, where we continue to discuss this hurricane pollution stuff like next class or that would be like different topic and so it will be a different topic. But the Okay but the the slides also has another like ten pages. So we expect it to start out. So we're going to still we're going to still like doing this. Now I'm just okay, 3 minutes. Okay. I thought it's like presentation time. So we're still with the okay, the presentation. Okay, then let's go back. Okay. Now we so this first approach is the third generation. Now let's start some discussion about the second approach, which is that just in time we competitive. Okay. So as you as I mentioned already briefly, that this is like a targeting the low level generation and this is not readable by human and it has its own efficiency or like somehow like bypassing some of the overheads that we have in the typical generation using this. And one of the most popular systems for that is hyper. It was like proposed in 2011. They are proposing a version of how they are doing the compilation using this idea of just in time compilation. So they are using a specifically the LLV. So I'm not sure if you all like you all like know about LLVM, but this was like it was a project at UI you see before for generating some dynamic compilation facilities. And I think after that Apple like incubated this and they took it further and further. So in the beginning it was like targeting the first one machines. But now I think that it is going beyond that. So it's more typical compiler for any actual machine could like virtual machine. So any, any real machine, any on-premise machine, you can use this compiler. Okay. So 11 now, so that provision is for low level virtual machine, something like this. But now it's not referring anymore to the virtual machine. It's like just a branding name. So you can use it as is. So it's one type of the compiler, okay, But it's very, very efficient. So hyper use this to generate this intermediate representation for the source. And typically you get like you can generate similar tools like the data could generate and you can generate this for the pipelines, whether it's push based or pull based. Okay. So they have also in their paper this notion of the data centric and operator centric, like whether you are generating the the source code or the intermediate representation for that, the query source code or the operating source code based on the operator level or that data like that, that the operator level could be like you have a specific generation for the scan operator, joint operator, aggregate operator and so on for the data center level means like I mean, the size of the patches that you use, because if you remember in this system, like hyper and similar systems like this, they use this code, this notion called more so that this like I mean batches that we had before. So now we can generate this for different patches of the same time only for one patch, for one tuple in the batch and so on. So it's based on the level of the generation that you want to go for. Now, we're going to have only one like example on that, the push based approach. So the same query that we had before, since now we can have much control on how we generate that, that low level like the representation of that query code that we have or the operator code that we have. We can here generate this by blind operators source code once. So we can read everything for each pipeline once. So if we if you look at the same example that we had earlier, in the first part, we can break this down into a different pipelines. So this first pipeline here is just like I mean, scanning on filtering. The second pipeline here is just scanning and filtering. And then here we we have a projection and building the hash table in one in one pipeline and then here filtering and probing the hash table and another one and then using the results here also after probing this second has a second hash tables located. So here we have different hash tables. We have a hash table here to be built. We have a hash typically used to be built and then we probably two times one time here on one time. Okay. So we can generate since we have this pipelines ready for us, we can generate source code that's optimized for the whole pipeline or the like. The whole pipeline. We can, you know, like if we combine all the, the pipelines together, like each pipeline, we can read the source code and then we optimize all of them together now and generate that the low level representation code right away. And it's just having a source code that's readable pattern. Okay. So this is an example of what's generated there. Here we're just having the soda code for the parts that they are generating, the low level representation. So for the for pipelines, one, two, three, four, here we have this different parts that will be generating that low level representation for it. So for example, in the first pipeline here, it will be generating one like intermediate representation that combines the code for filtering and then building the hash table based on the idea of eight that's filtered. Okay. And this would be used later in the like in the subsequent pipelines. And then the second pipeline here is just doing this equality check. And then we we do this aggregation based on the IDs and this would be used in the third part and here in the third pipeline here, we just like do this projection and then we build the hash table for this joint deadline between three and four here. So we have built a hash table here and we have built the hash table here. And then the force pipeline, which is the final one here, will be using that hash table that we built from the the pipeline one on pipeline three for probing and then generating the final output. So you see here that that four parts that we have are somehow connected with each other. So when we generate one intermediate representation for all of them, we keep an eye on on all of them together. And what are the dependencies? This source code will not be readable anyway by the human. It will be generated the right way on the machine level. So it's fine if we complicated as much as we can. So here we take into account all the dependencies and all the pipelines and then we generate them once. Okay. Here, as you can see also we still take into account the parameter values. So once we generate this, a good snippet, we still have the parameter as an input. So this will be also the parameter. Once we have its value, we will be translated into the low level representation and then would be parsed during the execution. So there will be no pipeline paralyzing. Right? So it's all run out. It was so here in this part it's still pipeline, but the code itself is generated for this pipelines is somehow generated together here. This is not meaning this not like I don't mean here that we just like, run all of them together in this sequence. I'm just like I'm showing the source code parts or like the parts that will be generated and the how they are connected to each other. Okay, So this doesn't mean that one would be running completely first and then two completely first after that and then three. And then for no, this means that since four will be depending on a somehow for will not be finishing its execution till it's finishing. And then the same thing for three here, but two can be running independent from, you know, eight from one. So we can run one and two at the same time. Right. So we don't have like anything a blocking any of support. So again, so the idea is simple. You just generate for the different pipelines that the low intermediate representation for that the generated code and you take into account here the dependencies. So we know that there is a reference in for example, if I generate a hash table here and I call it hash table AC, and then I use this reference here, like I use it to somehow I use the reference AC here when I generated because I know that this will be running at the same time. Was this like I mean, after we finish this, this will be running. So I can use the same reference. So I generate one reference for that can be used across the different points. Okay, so now we are not module it, we are generating this as one big snippet of code that would be running in in their own way. Like, I mean, one two can be running in parallel, but after we finish one and two and before we finish four, we kind of still use the same references that we have from, for example, one to be used the was for GCC. Some barrels are coming here. We can easily like, guess what, what we have here as pros and cons because here in the pros we have way more efficient approach to execute the query operations. Now we we we can support by outlining we can use the references from one pipeline across the different other pipelines. Okay. And this would be very efficient also for oil to be workload because that could will be much, much more tightly coupled to that where implementation itself or like the operator types that we have. So in this case it would be very efficient. Now, it's not that good for all lab workloads because now you have to generate larger sequence of, of, of instructions, although it's still in many cases if you have that parameter workload similar to that I could generation, we can still have like good performance again. But for the general case it's not that good. Okay. A very obvious limitation here in this is that that completion time can grow significantly if we have like larger number of joints or predicates or aggregates, because now we're generating the code for many operators at the same time, if we have one operator that somehow affecting different pipelines at the same time or one reference or one one hash table is used in many, many pipelines, I have to compile all of them together at once to make sure that the same reference is used across all of them. Okay, so now that the completion time will be a bottleneck somehow because that the more complicated pipelines I have, the longer compilation time that I will get. And now this will be a much higher overhead in the running time compared to the dive generation since are doing this for each operator. As we mentioned in the pull based approach, if the compiler supports running this or generating the source code in parallel for different operators because they don't depend on each other or like they are not used as references inside the the subsequent cells among each other. Now we can run them in parallel, but here we cannot run them in parallel if they are depending on each other as we do in the pipeline. So we have to wait till we have all the dependencies are in place and then we compile everything once. Okay. Any questions so far? Sorry. So for the Dirac code like completions that they did like compile like joints, aggregations predicates, they only compile predicates not for. No, no, they compile everything, but we can compile them in parallel because somehow it's modular, because it depends on the operator. Just focusing on the logic of the operator. Here we are focusing on the logic of the pipeline. So here if we have like one difference from pipeline, one that's used in pipeline for I have to compile a pipeline one, two, three, four or one four, for example, if they are dependent on each other, I have to compile all of them together. I see. So the compilation time will have its multiplication compared to a data would generate mixes. Okay. Okay. So now let's have a high overview on a comparison the victimization and complete. So now after we we get the main idea of both of them, we can somehow evaluate whether they are like both of them good, but have been bad. One of them is good, one of them bad somehow. So let's recap them somehow in a quick way. So victimization, you implement the source code yourself. You, you, you have a control of the like of that, the performance optimization. You write everything on the single detail. It's a very complicated to write it and maintain it as a developer, but it will guarantee so many like performance. At the same time, if you are completely running every like rewriting everything, you're using victimization, you're still adding another overhead, which is that's the code switching between the registers and memory and memory on the registers and so on. So it's it's suggested I'm strongly recommended that you optimize only the main operator is that frequently used somehow in part like that, like the scanning, the filtering or whatever data that's used in in parallel over different data tables to be just the victories. And the other parts that are really doing like extensive operations, you can just like optimize them using parallelism and so on. You don't need to rewrite everything that's more the victimization. Okay, for the compilation, we don't we don't make so much overhead as developers. We left it over for the compiler, but we have to make sure that the compiler is doing it in the right way, whether like doing it as just the generation and this time somehow it's not efficient in terms of the performance. Easy for, but it's easy for us to debug. Or we can have this just in time compilation that can be very efficient during, the execution time, but it's very hard for us to the budget. Okay, so both of these approaches, they have the same objectives. They kind of speed up the performance and they are compared to the very basic growth oriented masses. They are very, very much better than the like we can have like easily 100 X faster than this. Okay. If we want to favor one approach over another, if we're just looking for that performance game Victor ization is somehow a little bit better because you have a control over some structures that can hide some cache. Mrs. Things like you are having that. But the best way to control the source code execution because you're talking at the level of the registers you are having something instructions that will be executed at the level of that. Just so if you're smart enough and you were efficient in developing your system, then you can hide many opportunities that can have like potential overhead, like the cache misses the cache that branch missed prediction, so on. Okay, So that's basically the main thing. But on the other side, the compiler, yeah, of course can, can hide more complex this, but the source code that's generated there may or may not be optimized because it's automatically generated it's it's not optimized for the principles prediction that the memory stalling operations are this stuff so it's a still somehow reducing the number of instruction that we have here but it cannot avoid the double the cycles wasted during the instruction. So this victimization can somehow do that. Okay. So different systems recently started to somehow investigate having both of them together. So basically we use a hybrid approach. This is like typical typically like I mean, similar to anything like if we have like I mean, one extreme and another extreme. So we can try to to have like a hybrid approach combining the benefits of both of them. And if you see here in this figure, this like I mean taken from the paper that that's cited in the same user base class and it's basically showing you have the layers as systems that do use this quick compilation and what's the technique that they are using. Yeah. So here on the x axis, the are either like I've been doing this a of time or victimizing. So the x axis here is just whether you're victimizing or not. And y axis here is whether you are doing the predicate interpretation like the typical DBM is doing the switch cases over the different types of or the different operator types and so on, or this compile. So now you see here some systems like System are from IBM, for example, here this is like, I mean very old system. So doesn't have any of this, it doesn't have any victimization or interpret like or compilers. It's just like I in a typical system but system like hyper that we mentioned here it's focused mainly on the compiler doesn't have this victimization on other system like vector wise it's mainly written in simple instruction, so it's mainly victimized. There are some other systems. Yea, as you can see, they are in the middle of space somehow, so they are doing a bit of victimization and a bit of compilation. And the same thing here. And as you can see which part they are biased to. Like they, they are mainly here for example in, in the space of like you mean having less victimization but high compilation features or having here a high victimization list compilation and so on. So a pollutant system is the system developed by anti Bevel in decimal, and it's somehow using this hybrid approach as well. So basically, in the hybrid approach, what you do here is that you you let your query compiler somehow generates some of these victimized implementations, but this would be limited so much because it's very hard to generate source code with this victimization like instructions that we had discussion about them before because, you know, you have to carefully think of that masking logic and how you can make this. And it's not easy to do this over, over and over on the fly. You have to think think of it like how you implement an if statement using masking or switch case using masking. If you handle some of the very frequent cases, that's fine. But you cannot automatically generate this for everything. Okay? You have to like overwrite a lot of the features in the compiler to make sure that it's generating some of this victory statements and that it harder and harder. Because now if you you're generating this using compiler like let's say like we are having a compiler that generates in this by could yeah. A generation that has this like relation feature so now it generates the source code and now inside it it generates the the victimized implementation. Right. So that way if you have a problem, you have to double check whether the statements that you have as a vector ization are correct or even the logic of the comparison is correct for this. So you have two levels of chicken here. You check the the semantics of the compiler and you check the semantics of the victimized statements that you have. So that's the harder and the problem becomes a harder and harder if you are generating low level, intermediate to presentation. Now, if this intermediate presentation belongs to a victimized statements, then there is not even an easy way to do some reverse engineering to like to understand what's happening. So it's becoming harder and harder if you are combining both of them. But there are attempts and in combining both of them, and it is still an open space for research. But okay, any questions so far? Okay. So let's just summarize what we we had here. So we're compilation. It's another different approach for doing this optimization. It's now becoming an essential component in many in many businesses. So no one is is ignoring it extensively. The best query completion implementation based on many, many people and a survey had been done before, was from system called Esko and it was in my version 2016. They covered most of the cases. So it's also worth you to check it if you if you want to do that. And the hybrid approach combines that. The advantages of the tool like approaches that we we had year. But as we mentioned, it's still a hard to cover all the cases. So you have to be careful when you are using it and implementing it. It's still like a very early stage of research. Okay, now we finished this very low level implementation tricks. We're going to go back somehow. We're going to still be in the execution part, but now we go to the algorithmic level. Now we we go through some of the operators and then we start investigating how we implement them efficiently. So there are some algorithmic tricks. They're not implementation tricks like the victimization and the complex. Okay, This will be starting from the next class will be the joint operators. Okay. Any questions so far before starting the presentation? Okay, so let me just stop sharing you.So the basic class would be split into two halves. The first half I'm going to give you the the content for the joint algorithms today, the first part of the joint algorithms and the second part would be three presentations from your colleagues. Since we don't have so much time, we're going to start directly so basically we're going to cover today the joint algorithms. And the algorithms are very essential. The most you will get like exposed to any of the details of joints before or not, like the details of the joint algorithms anyone look at inside the joint algorithm or implemented the joint algorithm before. Mostly use this from high level using the database interface. We just call this a through your skill statement, but you don't know exactly what's happening from inside. So this is one of the most essential operations that you can see on the database. And typically every year and every day, the press conference, we have multiple papers about this research. It's not a big problem. It's still very, very active because we have a lot of challenges that appear like every day from different problems, different context. And whenever we see any small change in any setup, we go directly to the joint, to the joint and try to improve its performance, because this is a very critical thing. I just to say that selected from table and B, the first thing that the optimizer will be, and that is is how to join the data from this table. So this is not efficient. Regardless, the setup that you have, this will be able to make for your performance. Okay. So today's class will be focusing on the first part of the algorithms, which is mainly about the joints. Mostly we have the materials from this course and covering three main papers in the has joint algorithm. The first one is considered that the most influential paper in the history was somehow very old, but people keep citing it that using its implementation, it's very simple. But in this part, all of the people coming after them to how to implement joined, you know, efficiently. Most of that algorithm that you see here on here in the second and the third people are just variations on the first one because the first one did a very good job, like it took care of everything and some optimizations related to the hardware and the different cases that we can see. And joining the beta appeared after that. So they did some incremental optimization over this, some of this. This is somehow annoying, right? Okay. So before we go through the details, you just to recap for what's a joint, the joint is an operation to relate different tables or items from two tables or more. We called this binary operation or binary joint operation. It's just joining two tables or we call it multi way joint. If we have different tables doing using the same operation. So if you have ambiance, see you have different ways of joining the table like I mean joining Amby and then the result would be joined at see. So in this case you just depending on binary operators I upgrade that need that binary joint over Amby and then whatever I get as a result to you I join it to see all. There are some operators that are not that common that can basically join amps to the operate on the three tables at the same place. Okay, this is somehow complicated and to do it efficient is really, really hard. So I'm not going to cover this in the class. And most of that advanced course is even not covering this in details. It's it's mostly a matter of resource for people who's doing like advanced studies or so. But in practice, most of the DB masses focusing on the binary operators how to do this efficiently. If you do this efficiently and in practice, that happens if you do this efficiently, efficiently, you even can do this multiple tables joining very efficiently compared to the multi weight. Okay, there are many ways of doing this binary joint and we can use whatever idea that comes to you mind to do that. It's just simple. I have a set of doubles from one table, I have another of from another table and they want to see the matching topics. Let them in with a B are match on the idea for example like that couples that have the same idea or maybe like, I mean, they all share the overlapping in the same range or whatever. The most forward way is just like I take one table from your ticket with everything there and they keep forwarding. Then you get all the matches. This is very good and guaranteed that you're getting all the match tables from the two tables. But the most like obvious problem here is a performance. If you have to like two tables that are very, very large, billions of tables and you want to do this nested loop joint because you just doing Mr. Loops, taking one topic from here and comparing it with all comes from another table. This obviously would would not skip one step forward approach to improve that is that you try to use multiple threads and for each thread your sign chunk of the tables and each of them running in parallel to like to check it should happen. Was all the topics there that that will work but still limit to the number of the threads that you have. So if you have ten threads then the maximum parallelism degree that you can how to start? Okay. And typically we don't do that because now to perform one efficient joint operation, you have to employ all the threads that you have. So now if you have like many, many user requests or queries coming to you and you want to utilize this problem, you're going to be bottleneck here because you use the all the threads that you have to improve the performance. One query on Now if I want to be smarter, I should combine that efficiency of the algorithm with the threads that happen. So I assign one state to each one of this user and inside the thread I'll try to be as efficient as I can in doing the choice. Okay. The two main approaches that we usually have in this space is that one based on the hash functions and the second one based on sorting that, okay, we have so many works inside all of them. We're going to focus on the has going here just a note here that the message loop joint that I described to you now, it's mostly not used in all, obviously because it needs like do not scan everything which is like I'm in very hard in case of like billions of bubbles. But still, for some cases we can still do this if we have like selected number of topics that are matches and they have some prior information that will be like looking for more range if I do that. And typically we can combine this. Mr. Loop join with an index instead of just like we are not just scanning all the tables from one table was all the comes from one other typically in a lab setup or to be sometimes if the workload is using some data that's already there and we use it previously. So in the offline phase, I build an index because I know that some queries coming in the future will be also doing some joint operations or doing some indexing on top of this data. Okay. So when I prepare this index of line, when I'm using this index online, I'm not having any overhead, I'm not building the index. So the index is already built so I can make use of this area. So what I can do is that and instead of just having one table and checking, it was all that comes from the other people. I take one table from the index of the relation. If I have two relations and like tables, me and I know that there's an index on B, so I go for A, I did one table from eight and then go to the index. If I'm checking on the ID and this is the index on, if I go ask what is this idea? And then it locates you to a certain place, then most of probably all the tuples that I need to go and check in Table B will be around the place. So I just go based on the index recommendation and then check the composite. If it's a range of query, I ask that the index for the minimum and the maximum. So now I know that this is the start of the minimum, this is the start of the maximum, and then check all the patterns inside the range between the minimum and maximum. So now I took advantage of an existing and so the index is there. I don't have any overhead in building it, but if the index is not there and I have to build it on the fly, this would be a problem. So this approach is not going to work. It will be actually much worse than the method looked on because I have to build this index and this index could be built by some sorting. So it's at least in log in. And then doing this in like in scans or for like checking this index. So depending on the number of the topics that you have, whether this is very large or not, this will be a problem. So message loop join is good. If you have a small number of tables in each relation indebtedness, a loop joint is very good. If you have an existing index before and you want to use it. If none of this case is existing, forget about the message like don't do it. But it's not smart anymore. So there's one advantage for the index of just a loop going here other than what I mentioned. You can you guess, but it's not that important. So index admitted to doing just like going and checking one topic with all the couples and doing this. What's the main advantage other than like being good for the small number of couples and having this? Obviously, I don't need any external data structure that can help me during the joint algorithm index, considering that I'm already saying that this index is already built offline. I'm not doing anything during that online, so I'm so Boston property, I'm not building this index. For me, it's already built for other operations, so I'm using it for free. So it's not considered for me as a joint operation, as a blueprint. So it's just that a three step I'm taking care of. Okay, So it's not an existing overhead. The the structure that I use, if I have to build it now, it's an overhead for me, but if it's already there, I'm using it. It's not an overhead. So basically in the setup, I don't have any extra of structure that will take space from my site to use or like to build during this online operation of the joint. Okay. But as I told you, this is not going to scale anyway, so it's not going to be worth. Sorry, Professor. Yes. Could you explain again why I OLTP PBMs does not usually implement hash joint, so I was just about going to do that now. So in all TB So, so now I cover the message joint and then this in this loop joint. Now we're going to focus on the first solution, which is the line. So if you think of the hash, join Now I'm trying to do something similar to the index in this loop joint, but on the fly and that has joint. I built a data structure like a hash table. Okay. And this hash that will serve as an index for me, but it's not totally sorted and it's the most like existing problems that we have in the indexing that I have to sort them for into queries. And sorting is an expensive operation. It's m logging in, hashing. If I build a hash table or we can consider this hash index online hashing out this, this would be an order F for each end. For each table I have. I just need order one to access the hash table and then certain on average we're going to see that there are some collisions and we have to handle this. But let's say that I'm the average case. The number of collisions are very few and constant. So the complexity for this is order F So building a data structure in order M and using it for helping me in the joint is not that bad. Okay, that's good For large number of cases like large number of doing checks because now I build it once and they use it multiple, multiple times for each square was for each way, which we have. And instead of scanning billion of stop as I go to the hash table and get some like you know topics could be like very, very few and only to be the number of topics anyway that I'm selecting. They are very, very few. And most of the cases, all B are served by like small. It's small retrieval costs. So we don't need to like to spend so much time in retrieving that like the items, using building data structures. And in order to be we usually have an existing index to serve us like to, to like, to return the whole couple. So if you were all to be is very typical, like retrieving a record and updating something and get like getting it back. Usually you will have an index and you can use this BENDIXEN This loop don't in this case, because the query and the table that will be able to double to the return on this query answer will be very, very simple. So we don't need to build a hash table destruction and building the hash table as we are going to see is somehow taking space. So if you're not utilizing it, it will be extra overhead. So if I build a hash table for very few number of tables for each relation, then I'm keeping like large number of slots in this hash table like empty and not used. So that's, that's a problem. But if we have considerably large like table in front of this operands and the joint now if I'm building the hash table, I'm utilizing the space that I'm like, like specifying for this be okay. But going forward, we're going to see how this is implemented. These. So we usually have this dynamic between the high speed and sort this joint and over the time there was like I would put that. So in the beginning we thought like the beginning of the 2000, we saw that the hash edges of the fastest operation that we can use and this will be any sort based joint and then wait, we like we found that if you combine this with send instructions that we did before about like the victimization thing, now you can improve the performance of the sorting and it's comparable to the hashing. So there was a debate at this time whether, okay, we should stick to the hashing or we use the same instructions was the sorting. And then after that there was a tradeoff. Now, once the data starts to be big, so the sorting and passion, both of them need to be partitioned somehow. So now we have an extra partitioning overhead that we combine was was both approaches. So in this case, which one is better? We don't know. And then people start to say, okay, now it's going to be or instead started to be even faster, like if you compare. So it was simple without saying with some instructions in advance sorting algorithms, we can even have sort of this joint algorithm much better than the hash join. And then we keep doubling between the still. I think this was like in 2021, somehow we started to lean towards the the has been joined specifically by radicals has joined we started to like to find that practice even in the larger cases, with efficient implementation of this, we can mostly beat the sort picture. One of my papers I've recently published, but it's not here in this class. It's going to be among the reading list. We found some like corner cases that we combine the learned models for machine learning with a sorting that can actually beat the but not in all cases in some cases. So that's not a game. It's a tradeoff. In some cases, if you have a good optimizer that can spot these cases, then you can be more efficient. If you use the machine learning with this sort of the okay, so in all of these cases, we want to focus on two goals minimizing the synchronization overheads, because typically we have this joint implemented in parallel. So we have different threads trying to like access the data and read and sometimes they write their intermediate to the same like buffer or whatever they. So we need somehow to minimize the synchronization overhead. That's one thing. The second thing here, we want to minimize the trips that we can have outside our comfort zone, our locator, similar to what we discussed in the normal, if you remember this, normal areas might ban non-uniform memory access, since now we have some chips and their memory located right on each one of these chips. So now we want to reduce the number of trips that we can have from one to another. So that's also one thing that we want to take care of. So this this our design goals, we somehow named the algorithm that can so that everything about your hardware as hardware conscious. So now this algorithm is built with taking care of everything related to the hardware and know about, you know, my set up, about the threads, about everything. So it's optimizing everything from inside or it's oblivious. And just like I'm focusing on the high level algorithm, okay, we're going to focus on this part, which is trying to implement an algorithm with a like a joint or sort joint. But now in this class we're going to focus on the hash join that somehow minimizes this memory access. We want to keep everything local to to, to it's memory and like it's the resources and then we optimize that, the overhead that we need for moving the data across the different course depth. Since we are focusing on this minimization for the memory costs, we need to improve the cache behavior. So now we will utilize our cache because our catch is like is very limited and we need to optimize it somehow. We want to improve its performance and typically we, we do this by aligning the defenses. We have cache lines. So when you partition your data, you need to be aware of your line five and how can we partition the data to split in the deal to be anyone knows about the deal? B The theory is that translation leukocyte buffer. So basically it's a data structure that's very, very small and used by the operating system to cache whatever addresses they had but like to access from that virtual memory to the physical memory and so on. So whatever you are working on a certain partition of data. So it's called the hope like data that you are working on now. So it keeps getting the translation for from that that virtual page to the physical page and catch all of this stuff in the memory in the cache and use it like during the operation. So if you're juggling between different places or different pages allowed this still to be will be changed a lot and now the cache would be invalidated. So now you have another it. If you keep everything in one partition and you focus on this partition. So that addresses would be translated once as once and then read a lot. So that's the thing. So if you take care of all of this, like doing the attacks alignments, trying to position the data into a cache, like to fake cache on the till and besides is that we have this would be the optimal goal that we have to improve our campaign if we take care of this like, like things during our implementation that automatically we're going to get all that benefit that we have from the cache. Okay, so have something here. As I told you, this is one of the most important all out operations because it's like specifically designed for large cases and that the algorithm that we're going to go through, like taking care of the caching benefits that we have discussed directly, we have three steps in the house of the partition building and problem. So partitioning is possible if you have a small data, you don't need to participate, you just build a hash script and then you project, okay, so we take one of this relations and say, I build the hash table on the I.D. and then I think the other relation, which is B I brought this has stable build on that you call the and then get the machine. So in the partitioning here, if this hash table is large, well like a detail that I need to to partition to hash and to build a hash table for as long as I can look like it everything in one chunk. So I have to partition them and build the hash table like in imports and then either to have one global hash table by having threads running in parallel to build this table, or I build small hash tables and then merge them at that. So it's all about building the hash table. And probably this is the main bottleneck in the if you build the hash table efficiently and then project efficiently, then there is nothing more to do in the hash. Then the less paper acceptance I haven't seen and this is an experimental evaluation paper and it explored like many, many variations of implementing this. So somehow it provided components for 13. We at least seven of them is how join five other variations for source to join on the other. So it has a lot of variations of implementations. It's a good for not for results only but for design because you see there a thread of the described different approach. That's what it's a good thing to. So let's go through the partitioning fits. So on partitioning, we can say that if you think of the partitioning, maybe I can optimize the partition to the optimal degree that I can have or I can just do quick partition and don't care about, you know, the fine grained level. So if I optimize everything for the partitioning, I'm making it very easy for the threads to coordinate. After that, If I'm not doing this, I do quick partitioning, then I just chunk the data and ideally get that coordination effort to the threads. So threads now need to take care of who's reading now, who's writing where. But I just use the partitioning and that quick way which we call not looking want to just partition break them as best so we can do in the first option need on the network in partitioning which can be done once and then I say that okay, I need to partition this into ten partition. So whatever methodology may be, I just say that the first time I put them in one partition or I partition them based on a radix or whatever criteria I'm going to describe later, but whatever I found my first partition, I just keep it in the temporary space and other threads will be workman. Okay, so whatever I partition other threads would read and write from it by. Okay, but now if two threads are reading edges and one other thread is like writing or updating something, it needs to coordinate with the threads. Okay. So that's why this one is not that like used a lot in order basis. We prefer to go for the second approach, which is that look, partitioning and then the blocking partitioning. Here we can ambuja very conditions. Number one is that is doing this and kind of come up with a set and then say those. But again, that was nice. That's very good that I think it was named by Vicky. Yeah, we can write to the same partition and that's why you need to coordinate some part. So that's that, that's the problem. And the second approach here, this is a blocking partitioning. So I prepare exactly where it should thread will be writing and reading this partition so that I can avoid any bottlenecks or contention happen. So I know that there will be two partitions and they have two threats. I would prepare and the first partition aware that the first thread would be reading and writing. This will be the slots for it and the second thread will be having the slots and the first partition. And then also I'm preparing this for the second partition. The first chunk will be for the the first thread. I. I prepared that first and last index and second thread would be also preparing the first and the same. So now once I prepare the setup, when I say execute threads now know that we can read and write directly to this ranges. We know for sure that no one will be reading our writing to the same places, so I'm avoiding any conflicts from the beginning. Okay. But I have to be somehow optimizing my partition. I do multiple scans over the data as I'm going to like describe now to prepared this partition. Yeah, So that's that's the main thing. So let's see here one one approach and then looking partitioning which is that that should partition. So basically here you have all your data here and let's see that we have good segments as we have support and we have a nice function to partition the data on. But what we do here is that based on the hash value and I'm seeing that I have you like this hash stapled, but I have was maybe like initially, but maybe here we have in buckets. Okay. So in this and buckets, each one of this partitions will be having that couple that we have here. And then based on the hash value, I'll go through one of this one. So once I go here, so I know that I'm going to be in this box, okay? But now, since I'm running in parallel, so different threads can be reading and writing to this partition, right? So we need to take care of that. And to do that we need to have latches and, you know, and some lockers here to make sure that we are not reading. All right. Because at the same time, maybe we have two threads trying to write here and this in the same slot. So we need to have this latching on top of this. Whatever, whatever like thread comes first will take care of this and then after the finished it will release the log or the action here. And then the second thread would be writing with it. So this kind of an overhead for automatic looking partition. Yeah, people said we want it done, we want it to be active. Yeah. So yeah. So this would be like buckets and the hash table that they have, but it's one little structure, every single system access. Any buckets with this buckets. I'll show that global it's like a global this is a global custom. Another solution is, is that okay, let's make sure that every threat is writing to its own partition or like it's own space only. So we create many hash tables. So here I know that this will be like this on the hash until we be ending up into like maybe three bottles. So we have three files. So what I'm doing is creating three buckets in each one of these threats assignment like let's, let's make it just read like doing its own work locally by partitioning this into the three partitions that we have in that little table. And once we finish all of them, we think here and we try to do some consolidation like merging. So now I know that this but like this post partition here and thread one and the first partition here from the three to the third partition from three, the all should be grouped together to be the first partition in the global two. So I let them write this note parallel and then after that I do something. So after that I know this and then I know tools and then I measure three together. And so the benefit for that is that I avoided this contention here. But the drawback for that is that I had to manage this after. So there's also an over. Yeah, the multiple scan control measures. There were a lot of scans, multiple scans. Yeah. This was kind of coming so I'm going to just grab no multiple scan here, not multiple scan. Let's say this is not like the scan, this is like a biased memory copy. I'm just copying memories. And there are some techniques here to make this memory from by memory copy very efficient using non streaming buffers. There are some optimization here, but on the algorithmic level this is what happens. Okay, so we got everything running in battle and then we merged. Okay, so blocking partitioning, which is somehow the restricting of the partitioning that we have is happening like this. So we spend some time in planning the partitioning. We take multiple passes to plan the partition. So for example, we take one look, collect some statistics boundary. So I read all the data, get some statistics and then take another to repetitive structure like a histogram to position the different tuples that we have in the partition. And then using this data structure, the histogram, I make sure that each thread will be reading and writing to a specific place in one partition without any conflicts, usually building the histogram here is using something called the protect. Some does so many approaches in implementing this prefix, some efficiency. I'm going to show you in the running examples how we how we do that. But before going through that, we just give a quick overview about the rabbit's back. So we call this locking partitioning thing, running partitioning or adding space partitioning because it depends on how we divide our integrals into like different writing stacks. So if we have like this integral that is 19 well, 23, 811, or if we have the like here in the in the integer form, like what we have here, we can see that this has to write its values. But the two digit values that we have and we can solve them on patches. If we did the first radix here based on this id like this is like the most right one. So we can actually have this provided for bit and use it for sorting the data and partitioning the data based on the values and say that I'm going to group this into two partitions of one professionalism for and another partition more than two. Okay. So that's based on the values. And within this partition I can get an advantage of the second row. So within that, the partition that's less than four. If we have some number less than one, then we partition them in one partition. And if we have them more than one, we move them to another. So we have now two level of partition based on the courses that we have. So if you have any radical values, you can do any partitioning. Let us take one radix and then a partition based on that and then once I partition this, I go inside each one of this partition and then use the mixed products value to partition this one level and so on. But what's the advantage of that is that the number of partitions here are linear. It's not like that. The number of passes that we have are linear, which still efficient. The drawback for that is that the hierarchical partition difficult time because I have to do multiple tasks. So I have to partition first level, second and third level. And so, okay, so this is an example of how it works. Let's assume that we have this data that we have. We are just focusing on the partition. So we have to we have to call zero and we want to partition it. So what we do is that we get the house back and so we don't for this, right, that you get this running value. Let's say that this one we know it's having 01010 because I see that we are going to partition this into two so we can partition them into one partition that has zero numbers in the past. Okay. What I do here is that not doing the partition itself just for doing some statistics. So I know that here from this part I have two values that we go to partition view and I have two values that we go to partition and also here I have one bottom that was the partition zero and for the values that we both partition one. Okay, so I collect some statistics and after that I use this in this case to build a histogram that will bite me to the offset that each attribute will be right into. So a detail here, I know that this partition view and this partition view should be combined together right? So I know that the two elements would be written by the first one and one element would be written by the circumstance. And the same thing for partition one. Okay, so what do we do here is that we write here that we know that the partition feels like the first spread would be right here in the first two slots in partition view, and then it will be right back second. Third would be writing in the third element in partition view in the circle. If it had like more than partition, like it more than elements in this partition, let's say four, then it will be starting writing from the third place and then I tier three more. So I just look at the places and I do the same thing here for the three, one and two. But now for the second partition. And then after I do that, this is the second thing and then I do the read the reading for the data and then do the partition. So now each thread will be basically going to the places that I already allocated and write reading from here, writing. Then there is no latches, there is no lockers, nothing. Okay. So I optimize everything here in the beginning with multiple passes, but I avoid any contention here. Is this actually efficient or not? Now, as you can see, there is an overhead. I already did some passage in the beginning, right. And it's just having one pass to avoid this like contention and right to why is this efficient and why. So I already spent time overhead and preparing the partitions over here is probably one of those. So that the actual idea that if I spent more overhead in reading, it's better than I spend overhead and like so here I spend multiple passes, but just reading, reading the data, collecting some statistics, reading the data, collecting histogram, just reading. But now if I have an overhead in writing, writing here means that I have to put the latch. Someone would be waiting. There will be memories telling stuff like this. This would be hard for it. But in the reading I actually can do this, send in instructions. I can read multiple things together. That's actually this. Implementations are victories, so most of the time they are not that backed. So you're reading multiple items at the same time. Reading is very efficient. You use the streaming buffers and so on. So I spend as much as you need and the overhead in terms of reading, but I'll try to avoid this and. Right. Okay. And also that's one level of partitioning. Now, if I want to partition this more by for example, if this partnership is long and it appears that I need one more level of partitioning, then I will go to the next province and do the same thing here recursively. Now, consider this as one new and array and this one as a new input. Okay. And do the same thing. Okay. Couple of optimizations. If you just optimize that by writing using some buffers that look like resisting everything, there are some techniques to do that and most of this optimizations are mentioned in this interesting paper and velocity to solve thing. So also interesting, interesting to see how people optimize this in terms of implementation. So nothing more here on the algorithmic level other than just like using some efficient eyes and tricks in the previous slide slide, we have the results for you because you already have this is going to be actually very close. No, this is not the hash table. This is a partitioning. I'm just doing the partitioning piece by getting on this file with the same procedure. And yeah, we passed all the same procedure inside this. That's why when we then we know that this document is going to be processed by this sort of writing to this new structure. We're going to have it in picture, but we're going to have the yeah. You know, so the thing here that in the past, if we want to do that, the work on the hash table imperative. So we want to read or write to hash table entirely. So to do that we have to prepare partitions so that each thread will be working on one partition at the time. If I do this. So you know your question questions, why don't I just like that once I know the partition idea, I know the topic here. I just to go to put this in the hash table right first, the hash table is not physically partition, it's logically partition. And based on the has value, I don't know exactly whether it will go to the same partition has value as one. But what I'm doing here, just partitioning this based on the radical amount. So I'm just like this is just like preparing it for the actual work of like the building itself. But now to, to prepare this, we need to make sure that threads are not overlapping somehow when they are doing the work. But for example, I'm not something the same and like the same partition between two threads. So usually we have two threads right at the same time. No, we don't want to do this, so we partition them in part. Like what? The difference between the first that then this is the second. Yeah. Yeah. So here you see that we partition this. All the items here would be having 000 and the first set you wrote the first one and we, we have one, 111. Why is that useful for the action? But I think this one we do want to be double. Yeah. So it's not, it's not for now it's not useful for hashing as well just partitioning it to be useful for the scripts, not for the hash hashing would be random often. So I've got to describe this like integers. But let's keep in mind that when you partition the beta and you find based on the radix partition that you have now, it's not it's not enough for you. You can just do another level of partition based on the second rows. So I think all of this doubles in the same partition and the partition. Then you have this on the value files. Less than five are more than five, but it will be eight minus one thing and 01512. And so sorry, Professor. Yeah, because the redis partition might be repeated a few times. So in the end of like each radix partition does each starting, you can wait for each other before beginning another redis partition. You mean here? So here, like after. Like you partition. But when we read the data, When we read the data in the beginning, we don't need to wait for anything. But when we build the histogram, we don't need to wait for anything. When we write the data based on the histogram, we don't need to wait for anything. After we finish the partition. I mean, once we we start writing to the partitions, after all, the threads will be finishing their work. Then I start doing the build and I'm probing. But before building on probing I have to wait till all the partitions are written. Sorry. I mean like before you taking on like next read is partition right? Somehow. So yeah. Yeah. So I have to wait. I have to with I it depends on like I have to wait at least for the same partition to be written all partition because I don't know what are the digits that I'm going to build the histogram for inside this partition. So I need to have all the data inside one partition be written so that I can start collecting some statistics about them and doing this second level of radical partition. I see bigger. Yeah, we could a that. Yeah. Yeah. That's a good question. Actually we do that. We, we get some statistics from the data that we, we have in general and we say that we need two or three levels maximum it would be three. That's what we usually have. You would be those three. Three bits. Yeah. Three bits of partition. Okay. So once we partition the data, so now we have partitions we need to build the hash to. Okay. And then after that we're going to have the problem. So in building it, it's easy with this one table and then we put this in the hash table, but now we do this important. Okay, the most important thinking about people is that decision of choosing the hash function and also the decision of the hash is Kim. Actually, schema means that if you have a function is putting two tuples at the same place, how can you handle this collision? Okay, so there's a collision. How can you do that? So that's the schema here would be the strategy for handling this collision. Okay. So I'm not going to go through the details of the hash function because it's more advances. And so you like theoretically, all of them on average have all the one. And of course, there are some differences. Some hash functions tend to have more collisions than others. But in general, they are good, like all of them are like, if even if you take the simplest one, it will be okay. So we're not going to go through the details here. There is one paper I didn't mention. Maybe I mentioned in the reading list about experimenting the hash functions themselves, so it has some detailed experimental evaluation about how to use that, like which which function is used in the how the how they perform. If you're interested in going through the details, it would be done. But now let's look, stick it on a high level. So let's say now the hashing schemes that we have. So we have different hashing schemes. I'm going to go through the most important for that. Typically we have an order that uses a change version, the linear compression robin with harassment page. And I'm assuming that most you also like somehow get exposed to this because they are fundamental of the structures, but let's go over them very quickly. So for a change to hashing here, the idea is that whenever you're passing from happens to the same place and they have a collision, you keep them in a bucket list like or like have like a list linked. Unless something like making this and we consider this like the default implementation in many, many cases because it's very simple to implement, you just keep a list. And also typically there is no much overhead for that because if your hash function is good, you're ending up with the smallest anyway. So we don't need to optimize the list. Okay. Any dumb solution here should be working fine that if you hash function is good, which is of the case. Okay. So for simplicity, we we usually go for this. So what happens here? 10,000. You assume that you have these buckets in your in your house table and you have this space and you want to house them. So you have to have what you want to do. So let's say that this the first element I should do it goes to that second bucket. So in the second bucket I have nothing. So I put this in the first moment. So the second element you would be B So it goes to that is buckets or nothing is I put it here So C would be going to use the point was in the second bucket I still have slope and then D needs to go here so I don't have any more salt. So I will create another additional bucket. Here was the same size of the number of sphere and then I give it to you. So I have to ask about in the future. So I will go here. I know that it's in the bucket here. I keep checking if it ain't. No, it's no, I keep going. So I find the so in this list is a small and typically we have three buckets for example. Additionally as cumbersome, then there is no much overhead. Okay. Okay. So what we have e we can do either. Okay, so what's that? What's the problem in this bucket? That's one, that's the one that I mentioned. Another, another problem. Any problem that can happen here because it's ridiculous. You might not know how many if you want to carry alongside Operation, you might not know how to do that. Right. That's one thing. But this is not it's related to that. But this is not that. It's related to the length of this, but not that the prioritization slides this little bit, even this lengthy list is small enough so it's not that big. So I'm not having a problem in scanning it sequentially. The problem here is that this table, it's created in the beginning as a continuous one. That's one way to actually be good because now things would be after each other. So whenever I read this, I'm caching this one. So if I read this in the future, I have it ready. But this one when I created on the fly, I get it from somewhere else. This may be the contiguous places after this table is already placed by someone else and I just create this whenever I need. So it's not bland in the in the in the initial time. So this could be at the end of my memory. So whenever I need something here, I have to cache it all like I'm gonna have another friend. So It depends on how you implement it. Some people, you know, try to like have some of these buckets like land at an advance, like, I mean caching them even if they are empty. But the obvious drawback here is that you can keep some buckets that you are not going to maybe use, so you're wasting your resources. So if you have like enough memory and you don't care about listing slots that you have, you can even create some of these buckets like in advance and keep them linked to here and cached. But this is not typically happens in in that very large number of staples, like billions of tablets. I cannot create a hash table and three lightning buckets left for each entry in this house table. This would be very, very expensive. Okay. But this is one problem. So the other strategy is the linear problem. So linear cropping. Okay, So instead of doing this traversal and the change that we have here in linked lists, we try to avoid the problem of the power memory fetch and we try to put things even if we are like lighting after each other. So when I create the table I created somehow was larger space a bit. And then even if I have collision at certain place, I put it in the next one after that. So whenever I want to search for it, I go for the colliding one. If I don't find it, I know that I should keep searching sequentially. The main advantage of that, that it's sequential. So whenever I stop someone that the other ones after that will be cached anyway. So whenever I try this, it will be fast for me to retrieve that information. So this is an example. So I started with DE, but it was B It's fine now. I start with C, it's colliding here. So I pointed here. Now I put D so it's colliding here. It's like after that. So now E these. So I noticed like I keep progressing to find that the first spot. Okay, so what's the disadvantage of that. Okay. Yeah. Yeah. That's a good thing that you have to make sure that you're like reserving good number of slots in advance, that you can have this enough number of things and then you basically end up degenerate. We call it that degenerate case. This is a degenerate. Let's think of this situation that you have like sequential ideas and all of them somehow sharing some part of the hash. So they end up with being clustered in one place and has to be. So whenever I insert some one of these keys, I go and check like m keys and then put this. So the insertion will be and, and, and so I have to take ten times and the rest of the hash table will be sparse. Okay. Proving that we have only done this, we can be. No, we know. Yeah. So that's a good question. So basically what we do here in the linear problem, we put a threshold upfront. We say that, okay, if you don't find this t after this number of traversal, then this means that you don't find the key and the keys after that will be would be like belonging to someone else. Or you keep just like checking, checking till you find an empty slot. If you find an empty slot, this means nothing here. I, I'm assuming that it will be colliding with this one. And I keep checking and checking till I find that empty slot. This means that nothing would be here. So if it's not you, then it means that it's not inserted in the first place. So that's one strategy also for ending it. Okay, So that's so that's the thing here. So you have to write to reserve the highest people with a proper number of slots. And typically we go at least two X or the number of slots that we have for for this. Okay. So quickly here, go over the Robin Hood hashing and hashing because we need to start that representation. So in the Robin Hood view, we try you need to be smart in the linear problem. So instead of just going down to, like to scan everything, we we use the Robin Hood style, we steal from the rich and we give to the poor. So what's the rich and poor guys here? The rich here means that I'm very close to the first place I do the collision. The poor guy is the one who was at the list of the collision that I need to traverse and items to reach to it. So what happens is that I put a in the first place. I don't have any palladium, so that's fine. Then I have to be upwards here. It's fine. I have zero like zero displacement from my collision because I'm I don't have collision view and then see it's colliding here so I wouldn't be here. So I'm one place like one step from my colliding key here. And then I have the so D here is colliding with C, So it's it's been one, so one like, so with equal with the rich was together. Okay that's you as a C is displaced one item and it's displaced one item so will equal somehow. Now if I have E which is colliding with this one, this means that it will be displaced at least two times here. So this means that e is less rich of empty in this place. So now I'm going to switch this velocity. I'm going to give it give me here this place, and then I'm going to put D here. So this means that you will be displaced from here and will be displaced to compete. So now we're becoming do. And instead of having D here as one from C and E to B, that's three from A, So now I'm switching with with the So now you will be displaced too. And you will be displaced. It's like we are sharing the pain. All of us know that amortizing the cost of that and the same thing here. So we have your f, so f you would be like here. So I have I do the same thing if I have like more, more slots up there. Okay. So Robin Hood, this is supposed to be a smarter linear problem. It's linear problem, but smarter than Google Hashing is one of the fundamental hashing schemes now. So this is like a consider set of thought. And the idea here is simple is that also doing this amortized cost but now using helping, using help from the destruction. Now I'm not using one hash table I'm using to has to and I'm using two options. So I'm increasing my probability to have a slot to put the table there and decreasing the probability of that collision. Okay. So let's say that if you have two hash function, this means that at least you're reducing your collision probability was 50%. Now if I'm flying was one hash function, I have still probability not to be colliding with the second. So what I do here is that whenever I have to, I do a has value or for this key two times using the two hash function. Is it better known? It's fine because the hashing is order one. If I do order one two times, it's not that okay, it's still constant event. So I do this anyway for anything, whether it's gliding or not. I do this hash function evaluation two times, one on personal and then two if this one the primary table we pulled this one, the Prime Minister one we for this one. But second, if the table is sparse, we don't have any boolean, we just put it there. Okay, Now we have another key, which is why now I'm a chicken. Get in the Brahman and the primary to all the collision. Now it's in the second digit. If it's there, then go and put it to you. Now, if I'm looking for why, whenever I look for why, I also like I mean do discussion about it at the time and the attributes here are tickets here it's just good cheap because it has value is constant. Okay And then now I have Z. So let's say for our unfortunate one that we have this do have values and the two tables are flying. So what happens here is that we do an eviction similar to like the displacement that we had and so what we do is that, okay, I'm going to depend on the randomness. What I'm going to do is that I'm going to put Z here anyway. And then I think case. So X is already, if you remember, was high two times here. But in the first time, if it was here now, if I did this and I try, it was the second highest value. If it's not colliding, then I put it to you. Okay, So I'm telling you and then I'm telling you. So we're just checking. So we are Z yeah. So be it is gliding, Z is colliding. So I take it here and then I put the X and then I check here. I have this value for one. Okay. This x that they had intended for you was the highest value of x is empty here. Then I put x and if not, I put it to you and I take the one that's here and I do that again. So keep juggling between this, okay? Make sure it's nothing. Right. That's good. And in very real cases you have because there is some theoretical guarantees that if you have like a set of problems in each of these situations, when a certain size has a set number of hash function with certain property, that this will not happen this, but in some cases this happen even if you have this guarantees when is it if you have like very, very large number of tuples that can even go beyond that size of the house that we have. So we have collisions anyway. So a good practice here is that whenever we have, we have many suppose build the hash table was 1.3 for example of this number. So we put like 30% excellent each one of them. And because the overload factor, a load factor, there is a load factor here. If you go beyond this load factor, then the probability of being infinite is higher. Okay, so you have to stick to the load factor. But I know it was never quite right. It we copy why it did not exist. And I can say that right now, but I already know what here I have to do. I am day. I didn't need to do that. I'm saying you want to do this again. People just use our computer as one of the I already have it. I already computer in school Z. I know the high school of these colliding. So it is for those of you to edit module in the post so you compute So for your computers one has to it should span. It's like if you took has to it's like know I could have removed the one that was wasn't able to like I don't know you go there and depends on your subject. So for example I'm always evicted from the primary house so I vote for the one you so I take it here and then no switching Mike which anybody to do after evicted from or you are going to pick something that you will always want to do. So yes, one of the tables, it's not random. Yeah. So you always evicted from this one if you have the collision in both of them. So you're always affecting from the primary, then very minimal the way management has to do it. So if there is a evict. Okay, so if I want that thing that I evicted from here, if I asked you and I find a collision, then I have to evict someone from here. I put this one that's sliding down someone and I rehash it again. So it's like you know, juggling, you know? So you try, you try from different place. So I depend on the random in this case. I mean, we can we can like good for this of line. I mean, I just have to be finished. Okay. So after we do this hashing schemes, what we do is that we use this hashing schemes with the hash functions. After we do the partitioning, we build the hash tables that we have here and build the provision lists and schemas that we and then we problem with. The second relation. So probing is, is very efficient because you just read from the, from the tables that has table that you have you in parallel and doesn't have paper. Okay. So in summary here we got we went through different has joint algorithms. I know that we didn't cover all the details, but I just wanted to highlight the main idea of partition based on zoning, which is the writing space is one of the most important algorithms and it's the winner in most of the cases. Usually we combine this which has housing scheme like Google hashing. So usually we go with this and then the optimizer needs to pick the most relevant joint implementation to your query. And in the coming class we're going to go through the please join, describe how this is implemented and we compare this with the hijra. Okay, So now let's move to the first presentation. So let's go with you first. So let's remember that. Yeah, Yeah. So you really on Zoom, right? So can you go through and we join my zoom and then you can.Okay, let's start today's class. So we're going to continue our discussion of the joint algorithm that we started last week. Today, we're going to have the second branch of the joint algorithm, which is that sort of merge joint. This is also our classical branch of joint algorithms. It's been like use less than two has joined these days, but it's still a fundamental thing to know about it. So having, you know, it's you skip that you should be aware of. So as usual, we have most of the content from the cinema advancing to the discourse. We're going to start to read the ideas of this algorithm that will be from three influential papers. Most of the ideas coming from the first and the third paper. One technique only from the second one to be from the second one. But it's a really interesting just to recap. So we said that the joint algorithm is the operation that will be used for matching the published form to AMP relations. And it's very essential. So we have to do whatever we can to make it somehow efficient because it's fundamentally intuitive. So we call it the high joint. We're going to go through the source material, so it's easy. Basically, you have two relations. Let's assume that the typical case is matching the couples from the two relations based on an integer. If it's not identical like a string, you can still do some dictionary coding or whatever there to convert it to an integer because integer is easier for calculation and matching. So this is a general. Okay. So if you are building your joint algorithm based on it or like an integer, it's fine. You can transform anything else to it. Let's say that we have this case. One approach is to put this in a has a structure so that when you ask about an idea, go directly to it. That's one approach. Another approach is that I don't want to have a structure to be both. I can easily make use of that numeric properties that if I sort things, they will be property in the same part. Like if I lower number, like smaller numbers will be buffering and then larger number will be right at the end. So now if I'm searching for large numbers, I should go to the partitions or at the end only I should just get the one in the beginning. That's the main idea. So if you have two empty relationships, what you can do is that you sort this to relations, each one of them independently, and then you start doing something called the merge joint. So merge, join. I don't know if like you know this you do just scanning over that data that you have from the two relations and you like do this like very fast like you match if you don't find what you are looking for, you, you keep searching sequentially because now the data expects the main idea. So basically, if you want to do this in a a like decision, wait for this relations. And then you see here they had like I mean different colors somehow gradient colors here. This means that the very light one is the smaller one, the one is the larger one. So if we sort them somehow, we can have something like this from two relations. As you can see, we have you more like, you know, lighter elements than here. So this means that we have a smaller instead of here. So it's fine. We don't need to have like, exact matching as long as they are sorted. And then after that, we start comparing the heads here. So I'm going to go from here. For example, I use this as a point, this relation. So I start from you checking this with this. If it's the same, then it's in the matching table. If not, I keep forwarding this pointer here. I'm still checking this one. If it becomes larger, then this. This means that there is no matching here. So I'm still here. I'm going to advance my pointer here and I check and then I advance the check. And that's so I do I keep doing this in one pass over the two relations and then guaranteed at the end that I'm going to get all the matching thoughts. Okay, so this is easier because we sorted. So if we we want to have like an overview complexity for whatever we have here is that we have and look in blood stem. So it's and look. So we can easily of course if we do this for the total relation to is constant. So it doesn't matter. Okay. If you want to sit like a compare, this was the hash join. What we do there is that we want to we build the hash table and then we probe it. So building the hash table, order M and building it is ordering. So overall, it's ordering overall it's order. And look at that's why anything we have done should be much bigger. But as we know, theory is different from practical case. As you c imagine, we have to build X or data structure. So this adds storage overhead and it's that like if we have like a lab case, we have like very large number of stuff. This will cost us a lot. So this is an extra over here. What we talked about here in the complexity is that processing complex, but we didn't add any other thing. Plus, we have this kind of connectivity of the collisions that are generated in the linear probing and other things. So it's still a very complicated it's not like a determined competition or the hash rate. So you still have to go through this. But in the typical case that you have moderate size of relations and the data is we like, is it small enough, we can keep them in the memory so we can go for the hash. What's the winning case that is going will be the best regardless, whatever we have in the judgment that you wanted, right. That's like and this is a repeated case and corner case that we thought that we usually have. Think of it like you have the data of our student record sort of based on the idea. So it is actually done maybe by the administrator or whatever. So the data is already sorted. So what I can do here is just in loading the two relation sorting is giving this and log and operation. I'm just doing the merging. So now it's much better down there because this is exactly order and it's like building the hash table and the problem is on average order. And because we have this corner case for checking the collisions and other stuff, but this one would be exactly what I'm going to do. One task, Exactly one person with all the matching. Okay, So that's one winning. But let's say that this is not the case and the average case has joint is a bit it. As we noticed from the complexity discussion, sorting is the most expensive one. So we need to make sure that it's efficient. So most of the tricks that we're going to talk about today will be about how we improve the sorting this. That's, that's the thing. And of course any helping trick that you can have from taking care of the normal distribution by being looking through your data when you access this, all of this stuff will be substantially reduce the overhead that we have in this and using some in this sentence structure. And this is essential in and sorting as you going see here, let's assume that they are creating a memory here. Yeah. And also, what do you want to join them? You assume that most of the content of this class is assuming that the data is already sitting in the memory. There are other techniques that use this block based machine and look how join we would be on this because now this is are all in memory. So most of the data now is already that this not what I want that I would not know that mostly because it's a very pretty big thing. Yeah that that happens that we have like this cloud infrastructure aggregated across different many regions we didn't have to set up. So in any large scale company, we don't have one machine to put all the beta. The cloud service holds the data and we keep all the data in memory and it's cheap now. So now you can keep this and reduce cost. So that's the difficulty. But of course, if you want to switch to the disk based case, there are a lot of things that we have this we need to discuss. Okay. Okay. So in order to do an efficient slot machine, we have, of course, to parallel license, of course. But this is a analog. And so whatever we can do to reduce the overhead, the processing, we should we should do this specialization. And the first obvious step that we are going to do that before doing the sort and like The Imaginarium is to partition the data. So partitioning the data is that central thing. But here the trick, that's if you partition the data, you have to partition it somehow based on one relation because anyway you've got all the topics from the second relation. So but you partitioning should be focusing on one relation to make sure that you prove it on a certain like on a certain set of bridges and then you compare using that we're going to show this in details what are the different techniques how we are performing the partitioning. But the thing here that in in a typical case you partition or you form you partition based on one relational and then you saw the data in this partition locally. So now I'm reducing the overhead of sorting the data. It's an empire and then doing some kind of merging before doing the merging because we need to have the data sorted somehow before doing the merge joint and then doing that the merger. Okay, so we have three steps. So if we don't have the parallelism, we just stick to the sort of merge phases that we have to the beginning. We have two types of partitioning. The first part of partition is that if you have, as you mentioned, the data is already partition, so it's already implicitly partition coming from other, you know, you know, application that has a storage in the out like for example, we have a hybrid layout or whatever so that if is already partition, you don't spend so much time on partitioning them, you just to load the partition and then you start by actually doing the sort and merging fails is that's the thing, if you don't have this and if you have constraints about the data, for example, that data is partition in one column, but you want to join on another column. So now the partition that's already existing is useless. So you don't need to like to, to to reclaim it. So what you are going to do is that you load all the partition and the unit partition all of them. So this is that explicit bottleneck. So you spend some time here in the partition and when we do the partition here, we still have been some time in merging. That is also we have to access data across course. So we have this normal stuff that we've talked about before. So we have x overhead. So we need to be cautious in this. But the bottom line, if you have that partitioning already, they are matching your requirement. You don't have you have the partition for free. If not, you have to be careful when you re partition. So let's go through before building the partition. Let's describe the main ideas of the sort and I'm doing as we mentioned, we have to be efficient in sorting. So in sorting we can do this on a very fine granularity level and then keep merging the sorted. Right? So we say that, okay, if you have very large number of topics that you want to sort, you can sort them like 4x4 and then the sorted like runs of the force, you can start merging them. So merging will be somehow like in an order, an average. So now you spin and look on the smaller chunks of the data and then you keep running that the end operation for merging all of this sorted runs for a longer time and in so doing the end of M for longer. So you spend some time in end loaded on smaller chunks and then you keep running the merging in order and longer. And of course this is much better because if you keep doing this merging, which is somehow in a sequential manner, unhurried, this advantage will be perfection, having the data protection. So now you're automatically getting some advantage from the hardware that you have. But if you do this and low end and sorting, for example in QuickSort, if you keep shuffling the data between different places and in this case you are going to have like a lot of cash misses and this will be again is what we want to optimize in the heart. Well, okay, so people here, usually if they don't use the hardware efficiently, they go for the typical algorithm like QuickSort. This is like I mean, the most efficient traditional algorithm for sorting the data into the basis. So far. There are other algorithm, of course, you can apply any sorting algorithm, but this one is, is proved to be very efficient. But we want to make sure that we are going to do this in that in a more efficient way. We're going to apply somehow the same idea of the QuickSort, but on a hardware using that it registers and the victimization method that we had before. And so if we do that, we're going to take care of the text message that we talked about. And we inherently have attached cache conscious sorting algorithm. So what we are going to do is that we are going to exploit all the levels of caching that we have. So we're going to exploit the registers and then we go to the L1 and then we go to the L2 and then go to the L3 and then the memory. If we need that, like if we need that, then so you do your hard work on just a little like the sorting itself like you have for us, like you saw them. And so now you do this and look them purely without any overhead for exiting the data for memory or getting them. So that's the hot spot. And then while you are moving towards like higher level of caching, you start doing that sequential action for the data and that's still fine. Okay. So this is like, I mean, kind of at a high level sketch for what we are going to do. So we haven't sorted be done. We start getting into very small number of, of, of the sets of data and then we saw each one of them using the register and then we merge every couple of things. So we have like larger sorted runs and then this like I'm in the second level session and then we keep merging what we have in the larger level of cache and so on. Then we have the sorted. So this will be again and basically the operation that we have is almost the them. So you have multiple ends and without one end, looking back, what do we do here and on the register level is called sorting next. So remember we had the same instructions that we talked about, forget about the same. Now we're going to assume that we have this kind of hardware wires and we assign or number each one of these wires and we want to get the ultimate result. So we have nine, five, three, six. What we are going to do is that we are going to write some instructions on the register level in a fixed number of instructions. So I'm going to know for sure how many instructions I'm going to use for sorting this for items, which is like ten as we are going to see. So whatever the numbers that you have here, big or large, sortable, nerdy, sorted, you're going to use this fixed number of instructions. People have to this is very efficient for the compiler because now the compiler can predict what should be done in the next step. Whatever. I have four integrals to to sort. I know that this instructions would be repeated so I can in my complete program, I can do some tricks up to my stats. So I'm also helping the compiler to predict what should be done. Okay, so what happens here that logically we're going to do something like the QuickSort, somehow we're pushing the smaller items towards up and we pushing the larger items towards down. And we do this by making the first well, I think I'm taking the last two items, comparing them together. So now I know that this is my own fight. We know that this fight is smaller. So I put this up. I know that's me here this morning so I can sit up here. So now I know the fight is the smallest, but here we need the smallest from here. So I know that the minimum, of course, would be one of us. Right. So the next step to you is to get the minimum out of them. So I compare this with this. So I know that this would be the three and this would be fine. I would be switching is spicier. It is the right place. I'm not sure, but I'm pretty sure that three is the right. This is a minimum. You're doing this also for that six and nine because this is the largest and this is a logistic. So I do the same thing here. So this is the mini. So I can by this output, I'm six on mine. I know that this is the main thing. I'm switching here between six and nine. So I put 60 a 90. So 5.6, they are still here. Are they are beaten in the right place? I don't know. So I need to do another check for them. And for sure I know that the smallest will be in the upper part and the larger would be in the load. But now we have this items sorted in their place. So we have here picks of number of instructions, whatever the number that you have here, whether they are sorted or not, as simple as that. So if it's the same number, like it's still going to be three, five, six, nine, if you apply the same instructions, of course they are going to be wasted but still getting the same. Okay, so this is exactly the instructions that we can get as a sort of code. So we've been here the main max for the first couple of items and the main max for this. The second couple of items. And then once we do this, we get the main and max order that the whole group that we have here, the whole four tables, and then we do this main max to switch the things that we have. So we have four plus four plus two, we have two instructions. That's it. But yeah, let's do this. Same one in pairs. Just do the same, Max. Yeah, yeah, yeah. So, yeah, that's, that's the objective. So we use Minimax because when I compare two things, they actually become Minimax, right? There is no other case. It's Minimax because they are two. So what we did here is just on that, on a level of one resistor, can I use this with simple instructions, victories? It. Of course we can do this, but only if we can't sort different resistors at the same time because the same instruction cannot be part of life on the same resistor in the sending instruction. We need to do the same work multiple times, but here we cannot do that. So what do we do here to make sure that we are exploiting that victimization? So this is just a problem for that, is that we we group here that register like we get like something like foreshocks, for example, of this register, because we know that each one of them, for example, is for based like, you know, length for the key and for before the point. So let's assume that that we're talking about only one item that Becky, like you that we have on the value of the key. But typically we also have a place for pointer to the couple because when we saw the data also we keep track of the points that we have, but it's assumed that we focus on the key now and our and instructions will be our sitting in this section would be valid if we apply this on four items at the same time. So what we are going to do here is that we load this for couples and then for couples under 4000, 4000, and then we start. So we have now followed instructions and then we start sorting the corresponding cells from this couples with one instruction. So if I do a comparison here for Miramax, I do it here and I do similar here and they do similar here and do some of the same. Then instructions are applied in the same sequence, right? So regardless whether I'm applying this on on the first floor or the second floor or the third floor, the puts or I'm going to end up at the last instruction with the data sorted inside this floor. So if I know this, I'm sorted at the end of the instruction, we're going to have one five minus one and here we're going to have eight, 11, 14, 21 and so the same number of instructions, but now I'm increasing that the bandwidths of running them. So what happens is that we're going to use this ten main max instructions for each one of them in parallel. And at the end, if you look here before going through this, you'll see here that it's 811 1420 1159134, six and so on. But this is not the final sorted either because we loaded this up and we want to keep each register sorted. So what one solution here is to do transposing. So take this four and put them together in the first register. The second one here, this third one prettier and this fourth one which I already did the sorting logic. So what I need to do here is transporting them. So you're not transposing from algebra, right? Transposing is just like converting columns, rows and rows. So this is actually one of the symptom instructions that support it. So you can do this also in a victory. So what we do here is that we take this like sort of like vertically sorted runs and then we we transform them into row sorted. So we have now one five my 12th year if you so now I sorted how many items. Four times four. So 16 items almost Empire as like four sorted this. So that's on the level one here. We didn't have any overhead for in the memory. Everything is in the registers. We're using 17 instructions so before efficiency. Yeah. Why do we have to start with this first. So I need to have this one to be sorted. So just going to starting at the bottom with 100 because I can't use a sentence instruction here. I need this. So the 17 instruction will be running in. Parent needs to do the same work. Right. If I, if I solve this one like this and I use the same ten instruction, I can put a lighter because each step would be depending on that next light on the previous one. Right. So here I need I cannot use that. But if I run this in parallel, we are in the back which we motor into the one vector, like reach out of the load which look like which elements are loaded. So eight. So you have this one five. My well so this will be transposed like I doing the after doing the sorting like before, before the before the transposing. Yeah. So this four we build it in one sentence so we have four registers. Yeah. So the same the instruction would be running on this four and another one like it's running on this side. So for the entire season we have, we say that okay, you should sort this, but whenever I access this and this and this and this, I give it the index for the 012. So it will work on this four registers. Let me start because. No, no, it's not swapping. It's working on the register here, but in part. So when you give it the instruction, when you give it this register and you see that the instruction, okay, do the main max between the first elements here, first element and like let's say we're going to do the main max between this and this. Yeah. Okay. Give me that the minimum and maximum. So what's going to do that. Get been on maximum. You mean maximum here. Maximum maximum. You automatically up. Yeah. Right. So now we if you look at the logic we're doing a min max for one of them, but the same instruction would be repeating this for the the whole position. And then what's more in the back there we don't need here if you look at that this we don't need we do this in place it's in place sorting right? So we mean maximum, maximum max and whatever place that we have here it will be fixed and that would be the minimum. And so this is the same thing. Okay. So this is like for force two of the like thing, right? So if you look at the top, so you would be okay. Okay. So, so the details for the transposing is like shuffling. I'm starting operations, so we're going to bypass this, but this is like I'm in the high level idea. So if I like, picks a number of instructions for sorting this. So now we have this sorted runs in terms of for titles. So what we are going to do here is that we use this beginning to merge network. Yes. Day one inspection because it wasn't what it is doesn't become reason. Here's what we still do that to register. Yeah, unfortunately. Yeah. Because the instruction will be min max so it will be between to operate between two like. So what we are going to do here is that we try to measure the sorted ones and the high level idea without going through. Does that like the details of the implementation that's already in the paper is that we we use also another set of instructions to move more items with four items to have one sorted, run of eight items. Also fixed number of instructions combining from the main max and shuffling instruction. So the details of this is already in the paper so you can have a source code there. But for the second time we're going to we've done a little bit. But the main trick here that before sorting the before like working is to sort sorted once we read this one and then for like having the last one here to be close to the menu to make it more efficient for the hardware instructions, you can instead work on the sorted ones if you keep the same like order, but for like to reduce some of the number of instructions there, we have to reverse this. So the details for this in the before. But the main idea here is that I want to deliver is that you have fixed a number of instructions for the it's like 7%. This will be the same sort of trunk like one of the reasons like one of the registers that we have, one of the sorted ones there. This is a want two, three, four B1, B2 B 3 to 4. But now we were switching this we were getting when you want you close to both. So it's a symbol. Okay awesome. So it's in place modification. There are two variants to do in place to do out of place, but you can do complete. Okay, so we keep using this, you know, like returning person I talk to you which to a level that you you need to paralyzes it somehow it's a bottom it so one trick here is that you can actually do this in pairs by boolean. So we have two sorted ones and we can match them using platonic version, but here you have like imagine that you have been like, like m sorted ones. Okay, so we have sort of the box, so we need to, we have like I mean and over two tasks to be merged, right? Because now we have it like every pair would be merged together so we can put all of them in both and assign them to the threads whatever fit finished and it's input the input for specific sorted one is ready, goes to the pool and fix it and do the work and whenever the sorted runs are ready for larger input, you go and do that. So for example, on the first level, this one needs to be sorted from the the level one, just the one. So it's already there. So we cannot. But now for the second Liberal merging, we need the data to be sorted so this task will not be working. It will keep waiting till the empty is ready. Once it's ready it will be triggered and then we continue. What I know if you have this part of implementation, you can have this like tasks, you know, like put it over the different threads that you have and whenever it is ready with having a new work doing now, it's ready so you can start to work. Okay, so there's one obvious overhead was that that's best that context switching because now threads are free and they go to the pool by themselves to bring the work that we want to do. So this approach is good for this one, but it's not working in practice as we expect. So people don't like to do it a lot. But it's a good to understand that even the sorted runs merging can be running. Okay, so once you did the sorted data, so we have like all of these are now globally sorted after we keep like I mean moving from one level to another at the end we have this sorted data merged finally so we can do the merging like by matching the data that we have here and also we can do some merging in parallel. So you can just like have the different partitions of data and you can let the threads do that work in part because you know, I need to write, like to check each topline from the first relation with all the topics in the second relation in the same way. So they are independent. So we can actually spread them over that. That's one of the most important algorithm and this is why we're going to go over three variants. This is the most important one. This is a month away. So basically what we're going to do here is that we are going to override the same steps that we described before. And so here you have this algorithms implementing them. Okay. So what we are going to do, like in brief and just want to export. So we have two relations. So we think what we're going to do in the sorting phase and one relation, we are going to do this exact same thing on the second. So I'm going to describe to you on the first, as we mentioned, to do this parallel, we have to partition that and you need to have somehow lightweight partitioning overhead when you do this because you access across the different course. A lot of this will be having you more expensive cost than you expect. So what you do is that it's called literacy that we have for trace. And this data is where you hear we shot the data into four random partitions you to do that if it's still not sorted or not. Partition I want to do is that we are going to range partition. So what? Let's say that we're going to partition into four ranges. It say that we have a data from zero two and we're going to have from zero to N over four and two and over four, two and over two. And so we know the range, We know the range. So what we're going to do here is that we are going to sort each partition important. So we only need to communicate in the other threads at the end of this space. I know that all the elements here between zero and four over four, he brings you on and over four right? We use the same range partition here and here, the same thing. And here we are going on in over two. So if you want to have a globally sorted data, I should get all of this data together first and then I should get all of this data together after that. And then doing the same thing here. I'm right. If I group this data and merge them, then I'm going to have a globally sorted, right? Because now I know for a fact that each range is this is the smallest one, this is the largest one in, each one. So if I group the minimum for everyone, this is similar to what we did. And in the sorting network, right, we put the minimum, we compare the minimum with the minimum and maximum, but we do it on a on a larger layer. So what we do here is that first, since we have different course, we are going to experience an overhead of copying the data across the different course. So this is a bottom here. So we're moving the data from a far away CPU here to a one bin. Yeah, we are going to group the first partition one, the first and then the second partition from the second, spent external partition on the third and the fourth partition in the fourth. Okay, so now we copy this thing. So we have an x overhead and then once we copy them are being sorted after we copy them. No, are still not sorted. So we have to sort them inside each one of these partitions. But here there is an obvious thing that we can easily do this using whatever we have, like the beginning sorting things so we can do the sorted ones and then we do that measurement of different things. So the same approach that we have here, we kind of like looking okay, so now we have this is the smallest group, this is the largest group. And so right, so, so that's I mean the case would be the ranges. The sub ranges are more or less of the same size. Right? That's a good point. Let's assume that we're not. And if this doesn't happen, it's still a problem. What we are going to end up with is that having some partition larger than others, but still in the same order. Right. So so the question is that question. We knew that the range of somehow balance. Right. This is not the case. Not I'm not saying any assumption about this, that the ideal situation is to have a balanced thing, because at this case we're going to have balanced computation on all of these threads. The same number of items here are sorted on different threads in time. But if this doesn't happen, it's still working. Some threads will be finishing before others. It's still fine. Okay, let's assume that we did this. So now we ended up with having this data sorted. Each one of them. We do the same thing from the other side using the same range partitions. So what we did here, we do here and we do the same steps. So after we finish this, we're ready to do the measuring question here. How can we do that measure? So because it's exactly so exact, that's the thing, since we have the exact ranges so we know for sure the items here will be only check those items, right? There's no need to do to do checking you. Right. So I spend so much time in preparing the perfect partitioning and the perfect sorted runs to gain the benefit of that. So now what I'm going to do here doing some local merge joint. So I think this was basically this was, this was this this and this would be done in part. Okay. So we based so much overhead and the beginning in preparing the partitions, doing this across normal copy so that we do this merge join up it. Okay. So it's clear now. So what are the drawbacks here? So I like Euclidean dimension. One obvious drawback that is a bit as if the range is you are not parksons then there is no there is a big overhead here. But some threads will be finishing more than the others and maybe we have skewed competition opening lines literally between like the point nine. Okay. Right. That's another good question. I'm sorry, good comment that we optimized something that it's already somehow not a big problem. This like we optimize that scanning and the merge joint which is somehow ordering but we copied the data alone. So that's a bit of a problem. Surprisingly, in practice, this approach combined with the authorization that we do, is a bit better than optimized like look like doing this extra overhead. But that's a good observation. But theoretically as a system design thing. Yeah, we bid so much overhead for something that maybe it's not worth to do that. Okay, so the second variation of this algorithm is that we have this multiple assortment and this approach. We heard similar things, but now we're going to show you again that differently. What we are going to do is that we demonstrate this was doing this locally normal partitioning. So it's like it would be without any change but doesn't have a sign of chunky all the data. What we're going to do doing this for local range partitioning for that. So we do the sorting view and now I'm going to do this for both of them, but without making sure that the data is globally sorted. So I make sure that this sample sorted, this one is sorted, this one and this one is sorted and we have ranges inside them, but I am not sure if this would have all the minimal items right, because I probably will. I can see if you are. So I avoid that global issue writing things that I have for the across. No matter what I'm going to do here is isn't going to get rid of this by tricking each trunk here with all the sums this that's the second. So I'm now taking this with all of this taking this was all sort of checking this with all of this and sorting. And of course, I can do this in parallel, right? Because this is like bold vs I'm going to take this was all of this. So if I read this multiple times, then it's fine. I'm not watching anything, right? So I can do this. Two things, all of them in parallel. Basically check all of them in parallel. Okay. So that's another approach. Yeah. Very careful of these. Like I thought the same for you, but the same parts will always be part of the same pocket. Right. But here if you need to take this small items was all the small items. And from what we have the items are spread over. yeah we didn't merge them Remember from that previous one we did this global merging for everything. So now we make sure that we have local dimension, but now we don't have this. So we have to go through. Okay, so that's the second approach. So obviously here that's a bit better in terms of like not avoiding the writing. So another clever idea, this is the second paper that I mentioned in the beginning, like this is a and this is a clever one, by the way. I like it so much and I hope that it's better than others. But in practice, surprisingly, this is a little bit more like this performing than others. But let's see what it looks. So it is called the massively parent sort. It's similar to the multi.It more money goes. So before we start today's class, just want to check with you. Any problems to now in the project? Set up the group, set up anything. I assume that what I posted on the blackboard is somehow, you know, care for whatever you want to propose in the project. It's. It's nothing like, you know, very formal. It's something that I can just know from it. What you planning to do one day? Just is enough for me. As long as you can describe what you want to do in it, that's fine for me. If you need up to two pages, it's fine as well. You can use figures, diagrams, anything in that. Of course this is something an issue and you can update it whether this is kind of your initial thought on it. And then it's fine if you if you change your mind. Okay. Another thing related to that survey that I sent. So thanks for anyone who already submitted this evaluation, and I hope there were others who didn't submit this evaluation. Now, please go and submit this this kind of feedback for me to improve that experience for you. I already got some some of the feedback from you. Like some of them make sense, some others I I'm still, you know, finding a way to do it. For example, by posting the materials before the class was enough time, maybe like two or three days. I'm trying my best to do that, but it's very hard to do it because this is my first time to develop this material. So I take my time to improve this. And sometimes I do changes even before I post it today in the morning, I do some check ups in the morning and I just want to make sure that everything is intact for you. I don't like an overload I can do like somehow longer versions of the presentation, but this will not be beneficial. I'm going to anyway update you again and another one. So I prefer to post the final version before deadline, but I'll do my best to to finish this final version as early. Anything related to that grading of the review reports? If you have any problems, you can say to me out in here and also try to meet at the end. But doesn't giving you mind that this is not a01. So it's a subject as long as you you write your thoughts in in an efficient, critical thinking way you're going to be able to review on this. Okay, Of course it's subjective. So it's like an apples to oranges component. If you compare this with other one, it's not like this is no new one. And even for the exam, I'm planning to have it. That is Saint John. So it's somehow all subjective, you know, as long as it's making sense, it's okay. We'll try our best to post the grades as soon as we can. But if you have any problems related to something that doesn't make sense in the grading, you can still go with me on this. Okay. Our guest will be around 1030. I will give them seven five, pick him up in 5 minutes break, and then I'll pick him up and then bring him here. Okay. So let's start today's class. So this lesson will be about the optimization. And I'm trying to set this up with the biggest stock so that the talk today would be about optimization. Maybe it's a bit advanced because it's, you know, it's a high level equity optimization from an investor perspective. But we are going to try here to try to cover the main concepts that you might hear about today. Okay. I don't know about the content of the presentation yet, but let's try to go for ten now. We're aiming for that logical plan. So now if you remember, we started started like storage laid out, then we went through some details for that. We executed on some implementation optimization tracks and stuff like that. Very, very fine tuned, like SEM optimization factorization and stuff like this. Now you start to go back again for the logical plan. This is a very crucial component in any of the like committed to be system. This is the hardest problem that the optimization analytical level you as a user, you you submit a query in that syntax by this description. Maybe other language skills now and then the division. This will do a first checkup for the correctness of the syntax that you're not violating the syntax correctness. And then after that it will do some reordering, some replacement for some variables, maybe samples or whatever. We was still didn't do any planning for anything and then we'll pass on to that. Really optimize have to do that. Like the planning here would be how we divide this query into a set of logical operations that will be passed again to a query execution engine that will find find unit somehow or that submission of the machines that you have or whatever picking up physical implementation console. So in that we optimization, what we are going to do now is somehow very high level logical planning. There's another level of tuning, which is the physical planning still in the query optimizer, but today's plus would be about how we do this logical plan. We can think of the physical planning as more detailed logical planning thing. Now, if I decide that I'm going to run a joint operation now, the physical planning would be which? Operator But I'm going to use like will it sort manage joint has joined and it's going unsolved. Same ideas and concepts are the same from logical planning to the physical planning, but in physical banking a bit to you and to that current setup that you have, like the tables that you have, the storage characteristics, whatever. So logical planning somehow in the pool optimizer is kind of common between different systems and more or less we share the same heuristic rules, but physical pain, if like, like it differs completely from one system to another, from one setup to another. And that's so the goal here is to find a plan that minimize the cost. Of course, I don't know the cost because I didn't run the create. If I run that way, I know exactly what's the running time, right? So that the problem solved. But I try to minimize the cost before running it. So I will go for estimates. Okay. So I'll try to find, of course, like, like to develop a schema for finding a course initially based on some statistics, based on some parameters that you have and so on for your plan. Okay. And then I'll choose between different plans. So I'll try to come up with different plans and then provide these cost estimates for each one of them. Of course, this can have some errors, right? Okay, I'm fine with this. I'll do my best and then I'll select one of these plans. Okay. Obviously, you can see here that the query plan, since it's a back plan or a hierarchical plan, we have different orders of the operations. Okay, that we can do and we can easily go for exponential. And it's financial number of cabinets. Okay, So selecting this optimal plan based on the cost is still a very hot problem and be heart problem. And we can do this in a real time. If I know exactly the optimal plan that I can do it. But if I don't do it, if I don't know the optimal plan, I should go for some estimates for doing the okay. So that's the way it's the hardest. The problem we still to now develop a lot of techniques to somehow go to the near optimal solution. That's what we can do. That's the best that we can. Okay. by the way, materials for this class is not from the same. So who's going to watch like that? New class is not from the senior class, but still relevant. So. So there are some ideas there about optimization somehow related. I strongly recommend everyone to go and watch it as well, but this one is kind of from a multi system course, so it's somehow really I chose this material because it focuses on one type of optimizer that we adopted in many modern systems. That's why it's the future. Okay? The one in and me that I discuss is more of a classical one. Somehow, you know, it's still fine, but this is more it. Yes. Okay. So what we do here is that in the course, the plan will try to estimate this based on the summation of the course of the different operators and the plan. Okay. And then each operator will have a course that somehow proportional with the size of that input and output that we have from this operation. Okay. So if, for example, we have a CAM operator, so this is an operator is huge because it's going to be a blip on the whole table right in the basic in the beginning. So now we're dealing with billions of we have we will close this. So that's one thing. So this this, this operation will be affecting our plan significantly. But let's see that we're advancing our computation and then we reach to that root level. So the root level is somehow that the final output for us. So the final output will be maybe ten records, maybe the best ten students in the class or something like this. So the size that we have there is very, very small compared to what we are going to plan in the beginning. So it doesn't matter to put so much effort in that latest two stages. We try to optimize as much as we can in the earlier. That's the thing that will affect. Okay. So here put for the base tables that we start with. Tables can be with ABC. So we have to put some cost estimates for having index at table and also having money on the table. That's one thing. And also the operators end up in the middle of the plan, look like a telephone. Also the combined two types of costs, cost of having input for them and cost for doing the processing inside that if it's a joint operation, then we do some logic, right? So we match to you, we build that table, we put probe stuff like this. So this has a cost and also we have a cost for preparing the input for that. You know that we need to build the hash table or we need to sort the data also. Okay, So we have this two types of costs. What can I fix so much in the cost estimation? But we have in the plan is how you ably filter out all the numbers to 36. So that's the whole main criteria that we try to optimize in the optimization. So if you start from scanning the whole tables, try to push as much as you can, the filters that can remove all the things that will not be involved in the competition anyway. So let's say that I'm selecting the top to the students based on the last report. That's the thing. So I should filter out all the reports before the last one. I should start with this and then once I get this data, I start do whatever I do. Right. So the thing that students have been doing that maybe on their last report on that should be the main credit. So that's what we usually call this as push down predicates. Usually the predicates that will be using select for filtering and stuff like this would be the easier ones to start with in the beginning, and that could optimize. I will go for this as a first step after checking the indices of everything. Okay, so how many filters that I can push down as early as I can to filter the data that I got? So this is not straightforward because not every predicate I should push down there. There are some rules for that. But let's say that this is that general way of saying, okay, when we do this filtering for that was done for the filters, we, we, we try to optimize the selectivity. So selectivity here is the term that we use when we use for the output that we have for my filtering operation. So we say that this is a high selectivity operation. If it ends up with very low number of effects, it say that this is low selectivity. It ends up with so many results. But that doesn't mean any selection like, you know, it's just like image filtering based on one method to record it. So this means that it's low selective. So we usually prefer to do the high selectivity. And okay, let's look at doing selectivity is the most important thing. Yeah, if you like low serve selectivity, it hurts you to lose low selectivity means that we don't we don't throw that so much. Yeah, they like their approach. You like it or not, it doesn't. It doesn't. It doesn't make any sense. It doesn't make any preference. Doesn't help the. Why can we just just blindly pitch. No, no, no, we can't. We can't push anything because some predicates that we can use for pushing good affect like would affect the other relation outputs. And weirdly two incorrect outcomes. So we have to take the semantics of your plan. It's not just filtering out. You don't need to feel like you don't want to filter out the correct outputs, but you need just to filter out the things that you don't need. So you have to do something. Okay, But let's, let's say that now that the doing selectivity is the the thing that we should focus on after the pace of relations, because as you as you know, going is very expensive because you're increasing the number of topics that you have in this. But sometimes when you do the drawing, you have the input from different relations at the same time. And when you do, for example, the joining, sometimes you can do this like a message loop joined by Cartesian product. So you check in nested loops then that it's a quadratic thing. So if you do this, push down for the filters and get rid of the things that you don't need before that joint operation as much as you can, then you're succeeding in your plan. Yes. So this is an example of what we do somehow cost estimation. So this is a query here. Say select from the employees table and department. And it was all it's basically saying that, okay, get me all the employees in this department and their salary is more than ten. Okay. And then if they had it. So maybe you want to use the kids. They're just my kids. And then you have some statistics about this data that you have. So now this is the size of the relations that you have, the department employee, the kids. So this is like the number of statements or records. So we have here them in, I think, one 1 million, 3 million, some stuff like this. And here we have some statistics about that that's like distorted like the way that we are like having the data set. So here we are storing this every 100 tons in one page. So this page could be it's assumed that this is in memory, but this could be on disk as well. So whatever it is and also we have two images. We use them in RAM, we have things go by the page like control by backwards page. So we have this kind of system. So what we do here is that we come up with a plan like this. This is like the initial plan. Let's say that we optimizer is having this as one of the candidates. And what we do here is that we start going from bottom up and then do some calculation on the estimates of the sizes of the members that we have here at each one of this points. And then we see at the end what does the total that we can guess. So for example, we start with the employee on the department. So the department here we have one executive and then in this blend we post a filter based on the center. So if I filter some of this records to be more than ten, okay, then I just get a total of 90% of that or things like that. Yeah. So now this means that we have 4.1 selectivity. So we just selected 1.1 and then we ended up with 1000 and then this one. So that will be merged with this 100 would be joined. So it would be joined with this 100. And then let's see that we do this join to do this join. I wouldn't want to check every here with one record here. So we have as a maximum buy case here, check this one. So this is like I'm in the largest relations. So we have this so we have 1000. The kids here from that input is like 30 K and then we have to check here what are the, you know, the sizes of the final. I wouldn't say that this blend will end up with having this 3000 as a as a as a if we if we're going for having the course for the whole plan, we're not going for the final only we're going for the summation of all the operations. That's what we had in the beginning. The course of the plan is a summary of all that most of the operating procedure, these are the like the larger operators. So if we optimize this as much as we can. So I think this is a good step. So with about 90% of that, the puppeteer, then we did something smarter than this other. One way is to keep this here. Then I have to deal with the 10-K records here till I find this filter. Then I keep from carrying that 10-K, you know, like level with me till I find different. Okay. Yeah. Like the estimate, the cost of doing operators without them. So here I checked here, the department, somebody from the two relations. So I took that for the 100 that they had was the 1000. So I have to check, so I have to tell her the one seldom happens that they have to do as an input to this in relation to direct sorting. Or so here, I still didn't fix anything. I'm just like I'm saying that based on the size of the data that they have, I'm just having an estimate for or my plan more accurately. And I'm going to like, you know, describe later is to answer this question somehow by here which diagrams I'm going to use if it's a sort measuring. And then there is and look and here there's a like I mean, 1000 the 1000 I have you to saw the data. And also the same thing here. This is another thing if I use the has joined so it's an order and so I'm a little table here but I have to keep an eye on the storage overhead because I have a hash table. So it's it has other stuff to consider about Operation Desert Open Account, insert a whole select operation if we would just have a ground operation. yeah, Yeah. We just returned the count. What? This looks like it would be. Numbers of jaws have a lot of effect on all different including and we don't know if that's just an estimate. That's a that's an estimate. Yeah, that's an estimate. This is not an actual thing. That's an estimate. So I'm saying here that if I, I move the filtering for example, to be here, this is a better plan regardless of the operation that I'm going to selected for the joint, because this will reduce the number of numbers by 90% and that's regardless of I'm going to do here if I do this physical level tuning that I mentioned earlier that okay, now inside this joint, which operation will I'm going to use? So here this would be, for example, assortment joint or has joint on adding a more fine tuned cost isn't. But at least now using this cost estimate, I can get rid of the very bad blanks. I know you're number of screw you. Yeah. So this is kind of the number of threads and like loading the data from the pages, whether they are in memory or in the best way to do it. It's not like 100, just just like you. You also need to read the 100 data part 1100 wire, one solid and 100. Yeah. So you added you don't need to add it. Yes. Because this is the like the big notation. So we're kidding about that 1000. So 100 would not be anything. So we're just like, I mean, using this kind of estimates, it's not like the real number. I'm just to getting some numbers to guide me through the planning of this. Yeah, this is legit. So these are the estimates left me. No, no, no, no, no. Why don't I get the actual numbers? Because I didn't want it. I don't know. Right? I don't know. I'm just getting some estimates from previous ones or from the statistics that I have. Okay. That's the. So the very important optimizers that we have now is that's lingered the mindset and this is the first step in this linger to collect some statistics is to estimate sizes or the relation or the selectivity or like the intermediate sizes like what we did and so on. So this kind of the main inputs like sizes that usually goes up on to the end card means that the number of records and the whole released and the record is the number of pages this relation occupies. Obviously this number of pages that the number of figures. Okay, because now we're grouping records in the same page. I thought here is the number of distinct values. If I have an index on this one and this is important because if your ethics is somehow cluttered and we have so many like for example, we have an index with many collisions, then we have a lot of, you know, keys that we are going to check every time when we ask the same thing. Yeah, same values like the number of values when we ask the same. Okay, so this is not helpful. So the number of distinct is the increase. This means that the index is helping nil. Okay? It's that and this is the number of images occupied by the index because we know that self is something that index itself is something so that the index needs some pages to be stored on the data. I need some data simply stored. Okay. Then of course, this kind of basic estimates when we like go through the actual implementation. For example, if you go to PostgreSQL and see how it's implemented, it's not using this exactly more sophisticated estimates. No, but this kind of an example that can give them intuition about what happens. Okay. So when we do this selectivity, this is an example of how we do estimates or this. Let's say that we have a predicted that checks here with the column values of that is like equal to a specific. But how can we estimate that the cardinality. So what we do here is that we did the index cardinality, okay, so we know that this icon is that number of distinct values that we have in this index. So if I have one values that I'm looking for, then this means that the size of that input that we're having would be one over this index size. Okay, So that's in the successful data. If I gonna find this, then I have to come up with something roughly estimates. So for example, I'd say that it's like one over ten, like it's like one cost. So this constant is some kind of weight engineered constant. So you can replace this with any kind of sense, or maybe you can come up with a nice formula that can estimate this based on the table size. But the main idea here is that you come up with a cost and that would be a default for you if you don't have this great, you know, successfully. Okay. Now if I if I'm checking for a range, like let's say that I'm checking for all values larger than a certain bar. So now I do also another, you know, check here trying to estimate here based on the maximum value in that in the index and the values that they have, you know, what's the trend? And then I provide this by the whole range, the maximum minus. So this kind of probability estimates, it's just like an estimate, which gives you some indication. But now if I'm a checking here, two columns, so I have to check here. If I do this, I have to do like, you know, to go through that to win this, Right. So I have to get that cardinality for each one of these and this and then I prepare myself for them. That was the case. So I have to traverse the maximum and then whatever I have at the end, if I did this y the achieved then I have only one record, so it would be one over the maximum of the two. Okay, you can come up with other estimates, but this kind of an example of how we do this. So here, as you think as you see here, this is very easy. It doesn't depend on the query itself. It's just based on our intuition and how it looks. Okay, For more complex predictions here, my if I have a drawing like I'm checking here. Thank you. So I take that the value of the predicted the first one and then I find this by the second one because now I'm checking two things at the same point. So I check this and if I check whatever I have from the first predicted, that should be achieved also with the second. So it's like Mr.. The other side here, if we have an all like in the in the probability we can get this by one minus that pretty good. Neither all of like neither one of them is achieved. So we can do this by placing the needle here by one minus the same 20 that we have. Okay. So Justin is of course here we are assuming that there is a uniform assumption. So a more sophisticated approach is are for the correlated data, for the skew data, our formulas for that. So this is just something. Okay. So for the intermediate sizes here, but example, we have an operation like this, like the drawing thing with a drawing specifically, it's a it's a really hard problem, as we mentioned. So whatever we have here is an output from the selectivity of, for example, for example, we have the and we have this department, so we have this 100 and we have this from the end of the week, we have this 10,000. And then based on the predictions that we had before, this is an example of what we had before. So we do this selectivity based on that. So we multiply this by this, then the output that we have here. And then to achieve this volume, which is and we have to multiply the two things by this is like the end card multiplied by F one, and then the end card, the second relation multiplied by two. So this is that the final two. Okay. So because for that, like that peace table that we have usually in our class, usually when we have tables, we have either index it on my index it and the ABC is usually are categorized into cluster and then cluster. So cluster mean that the data that I'm looking for is stored like what we have in the index cluster mean that the data are stored in a way and I'm building a cluster, I'm building an index in a different way. Maybe I'm storing the database on the primary key and then I'm building an index on another key. Okay, So that's the recipe. What makes a difference here? How the data is grouped in the bit. Okay, So for example, here the difficulty is if we have an index, we just use an and then we have one course looking for a certain value. There's the index. So we just have one lookup in the index. So because we know it's okay to assume that the whole traversal for the three that we have is abstracted with one lookup and then we have lookup for the storage, the next one. So this is the pointer that we have from the key to the value and then some Scipio processing. This is like the logical processing part that we had in the planning is highlighted in red that will be added for this. So if we have any records then we have ten times that. But here we have only one. This would be one timestamp. Yeah. Now if we have an index and we have a range of query, so we have a range like would say that give me all the values we can be and we have a cluster index. So what I'm going to do is that I have this like biggest occupied in the index. This index is clustered, so I go there. If the number of pages and also get the number of that like are pages that are occupied because somehow this are mapped to the same thing. So the number of pages here would be equivalent to them or like maybe mapped to the same number of pages. There was the our cluster index. Okay, So now we have this stored continuously after each other and in the same page in the index and also in the storage layout. So once we get this, we multiply this by that. This is like 50 that we have and then that the number of bubbles that we got at the end will be multiplied by the number of dot operations that we have. So this is like, I mean, a way of calculating the whole course, loading the data and doing the process. Okay, Now it's different here. If we have one cluster index, because now the only difference will be in which which card that we are going to use it. Now, here we are going to go for the worst case that it will be in a different page, right? Because now in the index, of course, they are cluttered, but now when they are stored in the storage, they have a different key that they are stored based on. So the worst case is that I'm going to access one big breakfast, so I'm going to go get one page, get that record that I'm looking for, and then discard the remaining elements in this page. So of course, this not going to be happen in the processing, but at that estimation phase, I'm going to prepare myself for the worst case. So I'm going to go for this stream. You know, assumption. If I have more information that there is another average case, then I should use it for that extreme because this will give me a more educated. It's a good for the extreme one. We'll find that the whole terms here are the same, except that in court here, which is very different from now. If I have a sequential stay for the elements, what I'm going to do is that whatever elements that I'm going to get from the TI card, from all the pages that I'm going to sequentially scan, I'm going to read all of that, including the number of records inside them. And then each one of them will be applying a specific set of operation. So it will be w multiplied by this. Okay. Now, if we go to that point, I'm going to let them quickly give you an idea of the previous life. We're discussing the selectivity value or the criterion treated the length of the selectivity every time, because the previous example we reviewed here, we don't have any joint. It was just like for the scan operation, like this is the base table operation, even for the database that like the value is predetermined or like there have been no you mean the same quality called these values? No, no. Activity value. All the F here. yeah. So the F here is somehow estimated based on the query vector. So you have a query that will optimizer will do some statistics for the selectivity. So for example, if you're using that, that filtering that they're using the last record. So based on previous queries, you know, he knows that this would be like almost 10%, then I'm going to use this. Of course, maybe the data is updated after this last one, but I'm going to go with this data. It's kind of yeah, it's kind of saying, yeah, there are some work that do like do some one time checks for that and try to do the estimates. But this will be adding more overhead. The planning, it's already hard enough. We have like, you know, exponential space counters, you know, that we need to check so we don't need to add more approaches. Okay. So here for the join, as you see, we have two relations and we have a prediction that we take here. So what we do is that we start from one relation. We call it that base table, okay? And then we check the other relation. So basically like what we did like in the hash table. So you have one hash table and then you usually use hash people here for probing. Okay, So let's say that the base table will be issuing one query for I'm, I can build for searching this in the hash light. So like in the base table, we are going to search for an item in this case table inside the hash. Okay. So for each access to this hash table, we have a system cost to us. So let's assume that this course is just based on index, whatever, like it's a hash index or P3 or whatever. So we're going to apply the same formula that we had before. We have one one cost for the index lookup, one course for the record and induction for chicken for example. This is like that, that predicate check. And then once we do this, we augment this with the cost of preparing the hash table or scanning it like the one that we used for it, checking the values in the beat. Okay, so if you don't have an index on that, then you have to do some of the scanning somehow. So this will be based on the current assumption that we have. Then we will not going to be using this value of the index. We have one want. No, we have to go on scan somehow and this will depend on you are scanning this items in the same page or like you know, distributed across different pages also. So that's the main idea. After we we do the checks, if we have some types of merging like what we have in the source museum. So basically this kind of a special case from the previous one. You have a post here for scanning the two relations that you have, maybe preparing like reading them, loading them from the the heart to the memory or from the memory to the cache. And then you do the sort of if you don't have any sort of course, like the details already sorted from a previous operation, then you skip this and then you use the sequential output here on the page here that if you know as an optimizer that okay, that previous operation is kind of a sort machine, then the output most probably would be sorted. So while I'm doing my estimate, I just describe it. So I just focus on scanning the case. If I know that the previous one is has joined, then most probably the data is randomly distributed. Then I have to do another round of sorting, then I do that. So while building the plan, you're trying to estimate the operations as well and then you get the cost of that. So this is your job. Okay. So there it's obviously for the like for the simple enumerating plan. So what we do here is that we go for some heretics like was in this election, try to not to consider this cross products not like to do this like this it looked like and on focus on this left the plans. So what we describe in the joint operation is kind of a deep joint So usually you plan like this, you have a left operating a base table, you're drawing or something. And then what we have here would be doing something and so on. So you don't have a balance, like you don't do any tool and then going to and then you comply. Usually this is a why do we do this like this left the planning because this is easier for estimating the cost. The second one could be better in terms of the processing, but would be hard for us to. Estimate the costs while we are doing this on balance with doing this balance because we don't know exactly what's the shape of the tree at the end. But here I can easily synthesize this symbol that we. Okay, but there is one problem here. We still, since we still have this live the drawing, we still have to determine which lines of this relation should we start and then we go there. So that's the joint order. So if we have like three tables, like, see, okay, which ones will be joined first A or B or C, B or C, and then which one would be after that. So this is an exponential thing. And if you just enumerate this for three or three tables, we have this set to two, the power three. So we have like eight for that. So this is almost very hard. Think of it like if you have like like ten or 11, it's easily expanding to an exponential case. So it would be hard. So what we do here is that we use dynamic programing to solve this. Why do we do this dynamic programing? Because we depend on the case that if I solved this for two relations, this should be part of my solution for three relations. Okay, so I suffer from the one relation, isolate for one relation, which would be M and then I start doing this two relations. The combination of two relations. If I solved them, I don't have to revisit solving the two relations again when I'm solving this for the three relations and after that maybe for relation. If I'm not doing this, then I'm programing. So, so I have to revisit solving the two relations problem every time. Okay, That's the main idea. We've got to get this. This is just an example for two plans inside Boss. The brace for the simple way that we have here and we are providing this cost estimates for them based on the hash. I'm going out with the order that you can see this. And if you have a question, we can go over. Okay. So that's kind of the algorithm, the dynamic programing algorithm that we are using in Slender. So basically what you have here is that, you know, the possible relation that you want to join. So I want to say that the size of our and then for each company, for each ESOPs in this one, I'll try to solve it. And whenever I go for a higher order of, of life, of a relations that they have here, I'm going to reuse what I solved before. Okay, So this is bottom up thing. Once you take this, for example, I start and it's just get the first one like that one, one like that, that's that first relations. So this would be easier to check. Let's see, we have that second the second level, which is by two relations. Okay, So what I'm going to do here is that I will go from every couple of relations and then I'll check here. If I let's say A, B, I remove B, and then I get the cost of. Then if I found that the cost of joining B was A is is like, you know, less than what we had before then, okay, I replace this with the minimum that we have as a minimum C if let's say that EMC become the minimum and see if like I've been having and B then I replaced whatever I had, Nancy was empty. Okay? And so now if I go to the third level, like I have A, B, C, now I know that AC is that best option, but they have for that to buy relation, then whatever. I have A, B, C, and they want to start with relation. In the beginning I'm to start with AC. Okay. Will this be the optimal case at the end? No, this could be suboptimal, but I have to work with what I have. So I solved the problem in the beginning. Okay, for one relation and then after two relation, then three relation. Maybe when I saw that relation together in the beginning, I find another best combination. But if I do this, this will be factorial problem. Okay, it would be harder. Okay. So that's what we have to go on. So this is an example. If you start with all the base relation like we have D so we know that the cost for A, B, c, D will just as getting the relation. And then if we have that joint for AB ac, BCG or a B, I'm going to see whether AB or b A and then whatever I finished here, I update the table that they have. You see that when I have like larger order of relation that we have here, we're going to use what we had in the previous entries and in the field. Yeah. Like because the number of joints is not that much. Well where do we want to avoid doing. We're not going to do it like a standard relation even if we have time. What are the combination spoken? If we have two in relation, I'll show you. How can we choose the order here in relation to force? Like, remember, it's not clear to me so that the number is still bigger. So ten factorial still? Yeah. So ten factorial is big due to the power ten, it's still all of them is stupid, but to do the power ten is less aggressive than ten factorial. In my case that's that's how we try to solve this as a subset of the problems. Okay. So if you want to estimate the complexity, then yeah. So we have to minimize their execution by neglecting them. Nobody over there here. The course is based on the two sets, the logical and the memory like what we describe like now here we include this w the CPU operation that we told you. So I may be wrong. Like if we're using dynamic programing, I mean between the joints of like more than a thousand different. So this would be like the scenario, there would be a memory or memory or what a good this would be like a lot of space to do in the table. Yeah, it will be, it will be that like we have a case of memory overheads. Of course that's, that's obvious. But we're trying as much as we can. But like I mean that impossible thing, like there are some impossible like options that we come up with but we cannot even like we can actually detail that planning for each one of these options and can give them more accurate estimates and even like, I mean, have some interest for them. But this would increase the complexity, the storage and the processing at the same time. That's that's another problem. So this is kind of the heuristic solution that we found nice for solving this. Okay. So that so if you can give that for the complexity we go for that, that the combinations of one combinations of two and three and we keep adding the complex for all of them. So you can imagine this as as a binary problem here when we have this one and the intelligence was that it was finding the places of having only one digit being having one and and the bits and then we have two digits, two bits under three digits and bits. And so that's a similar. Okay. So this is just an example of Yeah. If we now if, if so now we did this like planning of the ordering. Right. So we say that okay, if you have the three, so we have two one and so on inside them. So when we do that join for each one of these subsets. Okay. So each one of this subclans that we have now, we can do some sequential processing, like for example, scanning the two relations for the joint that we have now at this level. Okay. So this would be something like that multiplied by each one of this operation that we have. So it still doesn't make a big deal. So that's why that joint order. So I'm finding the optimal plan on the high level is the one that matters because it's this already exponential from there. If you multiply this by a linear term, it's not going to be a big problem. Okay, So in some cases we have this. Yeah, because if I told you like I mean we have some sorted data so we have to play around. If I have an information about sorted data, then I don't go through that dynamic table and go for checking all the possible that I can stick to one possible option that I should be any order that have this relation as one of the between. Well, like actually listen to them join because I know that I will make use of the sorting nature of that will happen. So I can just bypass checking all the different, you know, birds that we have. You can say I know some information. So we call this information as interesting order problem. So if you have some more information you can cut and you go for some shortcuts. Okay. So in summary here, this is like any kind of a realistic build optimizer we saw we showed here like, I mean, how we use that planning in a dynamic programing way and use some cost estimates for that and that coming. Plus we're going to go deeper in the estimation and some of the techniques and the cost model is more, you know, sophisticated techniques than that we had few before. But what we describe is just enough for you to understand, to get the idea of how we estimate on that. So I'm going to I'm going to go now on the grep like our speaker here. Okay. We just have 5 minutes. Yep. Then please all attend this. You.You know. Good morning, guys. So before we start today, there's a couple of things that I want to mention that you get. So based on the feedback that they get from you so far, I mean, if anyone want to do the feedback or the survey that I posted you like, please, I really appreciate if you submit this. I'm trying to use this for myself to improve the experience for you. That's. So based on the survey so far. So many people complain about the way that we are doing that. Presentations. People say that, okay, so having three presentations is too much class, and some of the students here prefer to hear more of the classical. Okay. You know, compared to like hearing about different ideas in the presentation. Okay. So they'd be just like I the algorithm there so that the main objective of having the presentations is for you to improve your skills or, you know, presenting them in front of audience because this is very important in both academic and industrial setup. Now what do the so in the last lecture? So we have like to speak. So he's preparing lectures. He's like no giving talks because you have to show yourself also like in front of people in the company to do an industrial setup. And also I think obviously in academia, if you're teaching or if you're presenting a paper or something. So this is something that you have to improve it. People in the program have many chances to do that. But in the master's program or like master courses, they don't have so much opportunities to do that. So we're trying to do that. But maybe for you, because you don't have so much experience in the business 15 minutes, the presentation is kind of maybe level for you like to understand the main idea of that of the topic, and also it depends on the quality of the presentation. So if someone is doing it in the right way, so you're not giving out so much benefit out of that. So so what we one of the suggestions that we had in the survey, which is actually really good to compile some of this presentations and maybe presenting one paper and give it like more than I do now. If we do that, we focus only on one paper. We do give it more time and then it should be more detailed that that's the case. But part of the grading that all the people in this presentation have to present. So it's not like you're like having one presenter with the group and then he's going to do everything. No, I'm also getting some grades on your presentation, discussion and effort that you do in the presentation. Okay, So what we are going to do here is that we're going to have only one paper, the class for students to present. The three students that are assigned in this class will be grouped together. They're going to work together like 10 minutes, 10 minutes, 10 minutes. It doesn't matter for me. And we're going to present that in minutes. Like whether it's the technical contribution, experimental evaluation and so on. Bottom line that the three will be observing the whole because of the paper, because at the end, if there's a question, maybe one of the three should be able to answer the question. It's not like I'm I'm preparing the experimental evaluation or like that introduction, then I don't know anything about the other parts of the. Okay. I might ask specific one more specific question. Just double check. Okay. So that's for the regular presenters. I don't think that this will be a problem for for who will still be present and will be present in the future of course, this thing will not be applied on the people who are presenting today because they already like did the planning for that. This will be something after that we would like two weeks from now. Okay, so people who is like we're going to present on 27 February will be doing that. Please contribute to other and you know, like from the sheet who is going to be presenting in this day prior to plan this earlier to figure out, okay for people who are already presented and the like, follow that the old format in order to be fair with them. So there will be one or 2% as an extra for them filled in like you know, determine the maximum for that will be an extra month for them because they prepared the longer presentation and they made much more effort that this would get extra for them for their effort to be that for the presentation. Now, this would be somehow fair because now you're planning to present like less time putting less effort than that eventually it shouldn't be a problem for you because you're you know, it's like more like, you know, ratio of the grades anyway by just like in case I mean most of the people anyway like did very good and the previous presentation so they might not need this thing but think it's just to give them some credit for them. Okay so now that last content will be almost more than one hour, less will be one hour and 20 minutes. And then we'll have 30 minutes for the presentation. Okay. For the review reports, I get also some comments about giving some examples for a good review. And that review. Okay. So I talked with me that these kind of like an input more details as the feedback and you're like you would like in your graded assignments like you'll get a report and then he might like get some hands or I don't know, good practices. I will discuss with him about this. I'm going to post this on the on was actually came forward I kind of elaborated on this in the guidelines but maybe if we post more concrete examples, this would be easier for you. Okay. Related to the exam. So we have an exam next week, but the exam. So no classes on Tuesday. On Thursday we will have an exam on Thursday. So the exam would be at the same time of the class. It will be one and a half hour would be from 10 a.m. to 1130. Okay. But this would be a take home exam so that the exam would be available for you online. We're going to check whether we're going to do this through grade school board. I post this on Blackboard at the time of the exam. They're going to have one half hour. This is an open book exam and whatever you want must be responsible. No shipping, no your private chats, stuff like this. This will be detected in way. No. So it would be more of an essay question for the exam. Things like what we do here in the class like discussion. So I give you a problem situation and ask you about how you design an algorithm maybe for joining this group or maybe for optimizing the plan, or maybe something like this. So now you you'll have different titles. And of course this like different algorithms could be working fine, but I will go for specific things like what's the best thing, what I'm afraid of. So you have to be concrete and defining this. And I think following what we are doing in the class, paying attention, I know exactly what we are discussing. This would be easy for you. Yeah, as long as you're like reading the papers and stuff like that. You don't need to memorize or read every single thing in the papers. We just to get the main ideas from the papers as we described here. And then you can reflect this. Okay. For any special cases about the timing of the exam, this would be handled individually by me if you have any concerns, but this is not going to be useful. I mean, it's not like any special excuse would be granted, right? So the exam would be from 10 to 1131. Okay. There would be no class on Tuesday. No class on Tuesday, no class on Thursday, just the exam on Thursday. But if you report for five, I think will be the deadline for in that week after that. So the whole week, the whole coming week is for you need to have exam here. And also another question. Okay. And also related to that group projects. So if you if you started doing this and you want to get an early feedback, even before that submission of the proposal, you kind stop by the office hours and get something, a discussion about this. If you still have doubts about whether it's big or small for you, this process, we can discuss how it works. Okay, So will take it from the feedback that some of you like don't have any experience in the implementation, which is fine. You can deal with this as a programing post, you know, but it's a bigger problem and that's why we will likely reallocate some time more than usual. Okay, So any questions so far about what I went through now? Yeah, look at these will be some regressions that what be because it's maybe remember what I mean I can give you one example so I give you two papers on this and say that this is the characteristics for our institution of the data. These are the characteristics of some of the data and they want to define them. What's the best way of doing it? You can propose an algorithm if you I mean, if you propose an institute, don't get to work. But obviously based on this is not the best, then you have to justify what you're proposing. So I might mention something like, okay, so what are the tradeoffs? What is the best thing for doing that and why is the best? Okay. And as like, as long as you're close to that, the correct answer, which should be, you know, correct semantically, then you should get more points. That's the type of request. Maybe I get some query plans and say, okay, so if we if we have these two plans, which one is best? And from the perspective of the might and why you have to justify giving something for, it's like, you know, this is similar to what we discussed here, right. We get some examples, we go through them, we say why it's a good why it's bad. So it would be nothing more than this. The whole purpose of that is just making sure that your following what we are doing here and you know, another way of, you know, testing your knowledge of our own questions, I'm planning to be between 6 to 8 questions at how specific should we answer those questions? It's a safe question. So you're right. So basically, you're like, it's fine for me to do anyway. Like if you want to write in words or if you just write your hand and then you speak to this like candidate and then upload it, that's fine, as long as it's clear that's I mean, I don't want to force you to be here and in the room while you are doing the exam. That's why I'm doing this on my experience. Okay, so let's let's start this class. So we are going to focus mainly on cost estimation. So we briefly went over this and in the last class we touched on backward optimization. We give some initial information about how we do the cost estimation and then went through that planning itself. I would put in the plans being dynamic programing and other topics. So today would be about how we do this cost estimation in more detail, what are the tradeoffs, how we do this actually in the system. No, I gave you like in the last class, I gave you some numbers. So this will be how we generate this numbers. Okay, So it's more of a mathematical to this labor for this class. So it's fun. So people like having come out, you'll have to put so much effort, like in understanding the system designs more concepts that we had before in your math classes, most of today's work would be from MIT Systems course. There is one paper which is an archive. It's a very good paper. It's a demo published, I think, but it's a very good like if you want to go through it, it would be beneficial. I'm going to just like describe one idea. Okay, So let's take one example of how community estimation look like. It looks like in one system, like most of Greece. So if you go to Posterous, you'll install it. You have the data and then you post a query. There's one feature and post office that they explain. So and explain here, you explain the query. So you say that this is what I want to run explaining except living here doesn't run backwards. It just gives you a plan and gives you estimates for what's going to happen. So for example, here in the plan, they say that, okay, we're going to have high volume and this is the scanning operation. This is the operation building the stable. And then after that, we're going to have another hydronium isolated. And then for each step, we will find an estimate for the number of roles that's expected. That's expected in each one of these steps. This is the community information that we talked about before. Okay. So that's one maybe if you are a database administrator and you want to get more details, you when you like, you do this explaining quickly for different plans and then you get some questions. If you don't like about this, you just run this, run the query and optimizer, figure out what's the best based on basic. Okay, this is more for debugging, but if you want to, one of the thing then of course is that there's a focus is improving the values or what is it except cost here. Yeah, yeah. This is like random. The end of the back end of the path. Yeah. I'm just like giving you an example for this box of that because this is like it's inside of the book. Okay, so what's the overhead for doing this? Usually this would be base package. It should be like within maybe seconds. We don't afford like having seconds for doing this. Of course, this could be more complicated if we have bigger queries or like, I mean, more complicated like, I mean having more joins. So in doing more joints, we have to estimate many recursive things and this would take some time. But General, we we have to keep it within milliseconds. Okay. If that if it's like done always to be equated, it's very fast then you don't need to have even like millisecond. It could be like nanoseconds. That's so how do we do this like cost this. So one thing that I what our like guest speaker last mention that we have some statistics that we have to keep and then we use the statistics to do that. So this is one example of a table about real station, all with our data. So we have like different columns and this is like that part of this column. And one of them, let's say like Total owns like this is the total number of passengers that we get on a train or like this line that we are going to use. This is like calling for some examples later. So basically, if you have some column like this, you can keep different statistics about it. One way is to keep a histogram about it's a values or you keep some statistics about the most common values. So I know that the like the number of passengers, for example, they are almost 100 everyday as we know the population per the city. And this is a number, the typical number of passengers that we go to the world and then they come back. So typically it's around 100, maybe like at some point. So I know this range about so I can't keep accurate statistics or the numbers or the values between 80 to 100 or I can keep some histograms for the whole back end. So this is a way of doing a histogram. So let's say that the values that we have for the total on here is between zero and 485. So we know this one way to do this is to divide this range of values into buckets and to base the are equal with. And then within each bucket we calculate that like we count the number of items that fill in this bucket and then we see what's the fraction of this number of bubbles or like ten items in this bucket compared to the total that we. So as you can see here, it will just from the example that we have, you like most of the values are in the investor experiment. So it's a more skewed what's that, what's the pros and cons of this approach? If, if a user inserts a very large value, it needs to be the buckets might need to be rehashed. If we have if we have a user input the value 5 million and I've got in this so that was devoted about developer copy. Yeah, that's one thing. Yeah. So that's not kind of a drawback for us, right. So what's, what's up, What's an advantage do It's very easy, right? It's really, it's just a one to stand on the whole data that they have. I know that the number of spends just like that increase the counters and that's one and I can do this was victimization send whatever dealt for this phone so that if we just added like a column say on a greater than 1 to 85 and put a value there. Yeah so so let's assume that our data is fixed. So I know the range of the values upfront, like this is like the table and then I'm going to like divide, fix it, bends like that. Let's, let's say that what, what are our problems that could be even if we have fixed the range of values, you so obviously here I said that all the values that we owe on both state will be having the same like selectivity, but this couldn't be the case. Maybe it's the more toward like from 40 to 50. But now if I did some predicates was asking about values in the 0 to 10 the same number right so it's still very confusing. I thought like that idea was 0 to 10, like in 45. Q Which is not the case. So I'm here having some assumptions about the information within the center. I'm assuming here that whenever you were here, you were getting some like selectivity which is equal across the different values. Okay, What this is what happens you like, for example, if I predicted here, say that if you take the total answer, like are like less than or equal to five, hey, then this will be an exact estimation for me, because I know that it's the all the items, the number of items that they represent 0.61 from the from the whole data. So that's fine. This is accurate, but the case here. So I pick this like selectivity value multiplied by the number of copies or records that I have here. And then I I'm estimates and this is the actual which is exactly the same because this is like the total number of items between the two. It's exact. So but the thing here, if I have an example like this, say that I want to get all the number, like all the items, less than 7500. So this means that I'm going to have all this by the whole thing here and the help of this thing. So what I'm going to do here is that saying, okay, I'm going to all the beginning of the uniform, Then if if I get like in the middle of the span, this means that the number of properties here in this band, this then this will be top of this. Like, that's my assumption because this is just uniform. So I'm going to get here off of this value. And the sum this was that value that I have you. And then multiply this by the total base and then this is like that is actually, for example, this was good and perfect if this is uniform, but if it's skewed, maybe the whole items would be in this part looking at this part and it's actually beautiful. So it should be accurate. It should be like, I mean, just 0.61 multiplying this this that's something that we have a problem. So how can we solve this in a way that we can solve this little bit? It's really right so different. But so now it's just having this depth. I'm equipped budgets. We try to accommodate, you know, flexible buckets. That's what we do here and equip this one, maybe this one here. We fix the selectivity value and then we have the same number of bins. But now we shrink or expand the size of the bucket based on the values that we have here that would be in this one. So this is like if you want to like get some at like taste about that, the values of this histogram, you can actually in both the query event. So it's a stored another animal's attributes like so it kind of makes a decent table. So you can actually use a query like this to get the values of the histogram that we are building. So in this example, if I use this equity depth histogram and I got this filter or the view which is like larger than 195.5, So it's exactly this two page. So I know that this blend with each one of them is representing the same selectivity, but then I just buy this with a number which is actually pretty close to the actual. Okay, so this is much better, but still within the same buckets. We have the same issue, right? So I try to reduce the variance across buckets, but even at this granularity level within the bucket, I'm assuming uniformity, right? So this is still having the same problem. Okay. We still have uniformity within the bucket, maybe like half of the values. On top of that, the bucket size would still inheriting the problem. But less than that, the first because this is a bit complicated compared to the first diagram, but this is still manageable. Most of that systems we do this like describing it, right. Most of us got this out of the and I said this the first one. I mean, we have two options, but this one is most commonly used. Okay, So now we have extreme outliers in this range. There would be a problem, right? Because this would destroy the whole distribution of the histogram. And maybe this outlier is appealing once, maybe it's a being a loss. What we do usually here is that we keep another directory for this outlier values and the most common values, and we record that selectivity values that we have from previous experiences. So if I have a query on the values, a tool, for example, and I run the query and I know exactly what are the number of it, I keep them maybe in the future, if someone is asking about this value, we can just go directly on the values. Okay, so obviously we have a drawback here. I can't keep this directly for all the values. This would be infinite for me. So much overhead. And there is another value that I do not have a problem that these values need to be are frequent. Right? So if I have changes, I have to go over the whole table and change it so that that's very hard to maintain. So what happens here if we have, for example, up like this, you're checking for this value. So that's about that selectivity or that exact value of view. So I don't have the values here. So I'm not going to go here just to go there and get the action. But this should be accurate. But now what happens if we have something where a pretty good there is some kind of overlap between the the histogram bands that we have and also some of the values. So for example, here I, I want to get the like the answer for the query that has this predicate between 812 and if I'm 60. So I know that the value is about 850, but this is not sufficient for me. I need to get the range. So now what I've got to do here that also I'm going to visit the bins and see what's the range here. What did we do here? So do we just like get this and use it? No, it will be under estimation for me. Why is this if I just like depending on like I say, okay, 850 is actually indispensable, so I don't need to look here. I just to get that. But then I'll use the estimate, which is like the divide is like an eight over seven. What's a problem here? It's still a valid estimation, right? I'm saying that. But the real problem is, is the thing itself isn't going to answer it. So it's. Yeah, yeah, it's this this is not my focus on the corner case. I'm just talking about that quantity of this dimension estimation. Focus on the question. As you can see here, one of the values here, which is that 850 is actually seven over seven, which is almost very big compared to the eight over 70 that we have. Right. So it's good for me or like it's better for me to overestimate than underestimate, right. If I just put this or like eight over thousand, you this, this means that the numbers there are uniform somehow and I'm not employing any skewness information. Right. But if I add this 7000 over eight it's been this the number would be bigger and I'm saying that that the estimation should be larger than this. You should put more weight on the operator in this case because it's somehow there is a skewness and that most of the items here actually between this like two values are 850 was this of 50 by itself is like seven over seven. Okay. So I if I have choice to go for underestimation or overestimation, I usually go for overseas. Okay. So here in this case, I like to simplify things. I just add them to increase the number of the selectivity and then continue the process. Yeah, sorry, I'm confused. We only care about 13 divided by eight 3860. Right. So why we are adding extra? So we have here 850 which is in the range here, but that's included right or right then. Okay. So if I didn't include this, I just use a bend. This means that eight 850 is just one of the items that will be having selective estimation of target. But 850 by itself is seven over thousand. So it's like most of the items are in there. Most of the items here are actually a 5858, 59, but that's like we only care about the range between 838 six. They're at baseline a theater, right? But still, this is not reflecting the skewness that we have in the distribution having skew. So the cost estimation here is just an indication in your equity plan that this plan is important somehow. Like I these would be important if we have a skewness inside, this would be definitely a much bigger problem than having a uniform thing. So if I just include the uniform estimation that I have here, then I'm dealing with this operator as like as any other operator. If I'm increasing its estimation, then I'm putting more emphasis on how it's going to be a bottleneck. And then the preplan would be changing according to that, like because the theater like you actually you only care about the value range. Like, exactly. That's fitting. Yeah. But yeah, that's exactly what we do. But the cost estimation itself needs to present the importance of all of the skewness, right? Or that the bottleneck that we have in this operation. So if I have a chance to represent it better than just using that important thing, then it would be actually better. Of course, like this value is not accurate because I'm adding like I'm maybe like enlarging things that shouldn't be, but this is much better compared to just using that data. I'm just adding more emphasize. Okay. Yeah. This makes sense. If we were looking at a bunch of like the problem that we have is that we don't have the values and I'm getting out of my way and it makes sense to have some extra stuff like this. It was never something like this where you're isn't exactly been dividing lines up with the boundaries of women. In that case, you should just take the benign because the babies act. We know that the babies. Yeah we know that the baby is accurate. But the little penis. Yeah, I. I get what you're saying, but the thinking is that even if the baby is accurate, this will not be so. It depends on the query itself. So this is kind of an intermediate step that other operators will be like complete processing after it. Then this is not the final answer that I'm looking for. This is just an intermediate output that I'm using to to to other other operators. In this case, this answer itself will be changing. So so if it's just a select for all the values between this range, then what you're saying is right. But if this is just like a step for an intermediate output to be used for joining with another table, then actually that drawing would be changing the range of the value because this is not like the actual thing. But join will be reducing this number because it will be filtering based on my mother predicted, okay, if we do that, then if the estimation itself is less than what we expect in the processing, then the number like the importance of this operator itself, will be reduced. Then we go for overestimation. Okay. Anyway, like there are more sophisticated approaches compared to this to improve it best double your base value. Selectivity means there's a greater frequency of the value in the data kind of. Yeah, kind of. So seven or 10,000 means that there is a more number of things in that, right? That's correct. There are multiple values within that range. That's the most common that we keep. We keep adding them. Yeah. That this is not the best way. This is just the way they're doing them. And then you have to keep it like concise as much as you can. But if, for example, like, like if you have a simple, perfect joint, Nevada's between like 813, between eight, 1386. So nobody the contrary like estimate are just adding both the selectivity in the brighter beings and also the like most common values. So let's say we have Bambi, we are going to join them. Yeah. So they would be filtered first based on balance. Yeah. And then the results will be joined with. Yeah. So now if a the filter values are somehow skewed, the values themselves are skewed, which is like having more that is from 850. Yeah. If I just deal with this as a any form thing, the value like that across the estimate itself from this operation will be tough. But if I increase it somehow, this means that the operation, like the output that we have, you would be fit. The joint operator will be skewed. So it's kind of a differentiation between the uniform and skewness in some situation. So that's that's the thing that we have to mobilize. Yeah, Yeah. This is this like based on the historical data or like the entire mode because this is based on historical data and the bins, even the bins need to be calculated if there is an update and yeah, so yeah, so sometimes we update this. Yeah. So periodically the metadata table will be updated by some background proceeding. But yes, it looks kind of probably, yeah, of course we have so many the least we have to do this. Okay. So any more questions related to that would be why we have to go. Okay, so that's fine for most of the questions, but most of the quiz that we have here, based on one column now we have multiple columns. So the straightforward way is that we can just like, you know, give the estimates for each one of them. And then if we have like an end condition, we just multiply them, as we mentioned in the last class. But this will be kind of underestimating the value because this bit like assumes that independence between the two columns, which in most of the cases in real application properties, we have some correlations. Okay, So this is just an example of you. If you have like one column that query one concretely, without them, we had the same predictive security, but now combined you'll see here that the estimate after doing this selectivity by time, like by multiplying them together, would be very like very less than the two back because now we are underestimating them. So this is obviously not the best way of doing that. One way of doing this is just following some mistakes and it's good so far is doing this recently. We've just put some constants based on the experience because they don't know exactly what under the dynamics of the updates and they don't want to keep it like, I mean, somehow complicated. So after a while they do some calculation and then they put a constants. If I have like these two columns just for the cost. Of course this is not accurate. And for example, here the constant could be just a a like somehow the square root for some like for some of the values of this estimation and then you use it which is somehow Ristic could be wrong, could be right based on the application. But this is not obviously the most of multiple. Another approach is actually to to maintain this column like this common values statistics. But now for barest of columns. Okay so now I see. Okay. So I know that most of the queries that they have, they have built thing based on the these two predicates. They like prebuilt of the values. So obviously this will be much larger, much less overhead for me. But, you know, this can solve a issue that we have here having more accurate results in instead of just relying on the independent. Yes, that's one thing. Another thing is not just like doing this for the common values is just doing this for the histograms. Now we build the histograms for two columns together, so it's like 2D histogram, but just one dimensional histogram. What happens if we have three columns will have three, D has two, so this is more complicated. Okay. The problem here that if we had many columns which consider I build the combination of columns, should I build that? If we have eight columns, this means that we can have at least one of these like 100 values. Then we will have like 100 bins. I'm sorry for these values. Then we would have 100 to the power of 80 combinations. Which one? I should. Obviously I'm not going to store all of them, so I have to keep some statistics about what are the most, you know, axes, the combination of columns. And then I keep like, you know, updating them over the time. Okay. So that's another issue. But even if we if we keep like I mean, keep it simple for three complex, three columns, for example, other combination for building principle, if I'm just having an independent histogram, one dimension has to go through each one of them, then the cost for that would be the building. Each one of them for the other end was I'm just like, I'm getting them. I'm then multiplying them by three because this is the number of points. So this is very efficient cost. But if I'm having this like three D histogram for all of them, this would be to the Berkeley. It's still even for small combination of all of the variables that because I know that there are some dependency between apps. One way to do this solve the problem here and still just going for the two extremes you might need a completely independent or completely dependent is to break some of this dependencies. Okay. So I figured out somehow from my workload that okay, this dependency between B and C is not frequent or maybe not like not changing statistics of the data so much. Then I break it. I keep the dependency between NC and maybe that's it and I build the histogram for the method. I mentioned histogram for AC. So I mentioned look out for AB. So now that cost would be two times into the part. So. So this is a tradeoff. Is this the most accurate one? No, but this is better than this in terms of accuracy and better than this in terms of the overheads. Okay. So this paper, the bicycle that I mentioned in the beginning is kind of exploring this idea so much so it's worth to to look at it. It's using the Bayesian networks for doing that. And I think it's it's a pretty good procedure. Like if you have like more complicated, like dependencies or correlations between columns like this, if we have eight columns. So this column, if we have built like three dimensional histogram for this, like the overhead would be very hard compared to the previous some of these dependencies and figuring out that there are certain correlations that they should maintain then that because they would be seven times this 100 was a part of this is two dimensions. Okay. That's the main so this is using graphical. Okay. So what we talked about here is just the equality or the gets the range of predicates, but there are other predicates that we can do some cost estimation like in this matching your matching based on a specific suffix or prefix or substring. Okay. Or better. This is very hard to do. And usually we just do some purist questions or do more complicated approaches that but proved in practice that they are not that efficient using a pretty extreme subgroup. So you put so much effort in figuring out what are the most common sub strings in this, and then you build heretics for that. But this is very time consuming. We cannot do this. And the old nineties and even maintaining the statistics from time to time is very hard because even changing one string will change the whole prefix three for the difference. That's one thing. Also, we can do the tricks based on user defined functions. You know, this is in a way that you can actually define some logic. You write this in a set, like I said, but programing syntax and this logic would be used as just a quality graph. Of course, this would be very hard to do an estimation because I don't know what's the logic inside it. The only way for for doing this is to run it and see what's the output and get the number of rows as an output and see how much this as a ratio from the whole data. So this is very hard. Okay. So usually the library tickets and those are defined function predicates are not like, you know, investigate as well and we keep them with a risk. They are not that common anyway. But if we end up with having so many code like, you know what is about them, we will have very bad performance or that we will. Okay, one way here like to solve this problems is just to run equity on a sample between so because yeah, I don't know this is a black box for me I think just a random sample let's say 1% of the like the data that they have and then I run the use of defined function or the like predicted and then get the number of outputs. Let's say that I take them like act like I have 100, like 100 tables. Then the 1% will be, let's say like let's say I have 1000, that 100 like 1% would be like ten, but I want the predicates or like they would have defined function and the output is two. Then I'm saying that it's a two out of ten, so the selectivity is 20%. Okay, then I'm saying that okay, for the whole data selectivity is 20%. So what's the tradeoff? You know, exactly. That's one big problem. And on the on the positive side, like running this sample, maybe efficient, just 1%. But the problem in the sample is it's not if it's not represented, we're going to miss a lot. Another important positive thing about this is actually that we don't need to have any dependance, assumption or anything, whether it be correlated uncorrelated or just run it on whatever the metric there, it will give me. I'm right, but as long as the sample is presented, this would be fine. Okay. So that's exactly what we have to do. So we can use this with any operator, any predicate, no, some assumptions. But here and we can easily mess some statistics, you know, about some values. Okay. Because it's not representative. Of course, this will take like more time because we're running on the 1% we've run that we run the actual weight on the 1%. So this is more time compared to that, that a risk that we had before just running statistics or like doing this probability assumptions like having the multiplication of that, like the selective distribution from different columns, that's what that would be like, very expensive compared to that. But most importantly as a drawback here is the delay dying operation. We usually care about the cost estimation in the most expensive operation because we are somehow directing that plan. I was I was going to construct it. So the most expensive operation is a joint and we can easily miss some of the values based on the sample. So let's take this example. So we have this table that we we had in the last class, the kids that they look at. So we want to do a document over the edge will be one way to do that. Community estimation here is we just to get the sample and then we run the joint over the sample and then we say that, okay, this is the number as an output. This would be the issue. Okay? But here we then get the sample is our random sample. So I get the samples here, which somehow when I join them, we don't have any matching results. So it was a misleading thing. It's actually looked like just a representative. Give me a wrong answer. So I actually knew some random things that are actually not represented for the joint operations. So this is a problem. So there are a lot of people doing research on how we select the presumptive samples for the joint owners to make this for A lot of research papers do still not solve problem. Okay, so here this is an example of where another query, another way that could be fine if we do that. This operation in the example here, we can if we have like just matching the tuples without predicates, like we get all the matching problems from, from the tool table, we can use the physics of like the community estimation along the whole thing. So if I just do this once in the offline phase or it's like it's like our end of joint operation maybe once and both like over each build or like it's a bit of tables and then keep it because I know that there are some drawing squares would be about matching them. Then I can just use the, the output cardinality that we have here for this. So if I run the Dwayne here, I know that this would be 3 seconds me, this is or if I run the domain for the three, it would be four. So I can just use them for doing the estimation estimation when I see another query in the running time. This would be perfect if I don't have any filtering because I know exactly this is the the number of matches. So this is easy. If we put this in a relational format, this one command relationship, it would be fine. And there is no filters because I know the answer would be the same every time I run the query. So I'm just like, you know, computing it offline, not like it, but if we have tickets or we have, you know, some filtering criteria, then there's another issue that could be happening a correlation device, because this filter will be if it's like filtering some values here, it would be actually filtering related values from near and related values from here. So the answer that we are going to have at the end would not be representative. Okay, So that's a big problem. The correlation. Okay. So what we do here is that we use some kind of more sophisticated formulas like what we do here in Postgres, trying to estimate the number of unique values based on that filter and also the number of unique values after which the maximum. Okay, you can take this. Maybe apply. It's fine to be easy and weighted, but the final point is like this you get into an estimate for the number of samples in one relation and the second relation here based on whether we are using a specific filter amounts. And then you get the maximum of the unique values of using naming values in each one of these dates. But you want to do this just like a simple formula. It gives a better estimation if we use it compared to that. Just, you know, about getting this like basic for life. We have no filters. So that's the similar. Okay. So based on that seems I could use up I can run the values you build on this formula, you have all the quantities and then you have you decide about that. The tables are the size of a medium and each one of these tables and then you use them for estimating the joint. Okay, so it's easy, but needs somehow more than two. Okay, So this is the same values. One thing that I just want to mention at the end of it, even in the join.me we want to go for the overlap on page 100 because here this is one situation that could be happening. I see that the number of all the estimated templates for the joint operation is a small I underestimate. Then this will lead the optimizer to somehow select nested loop joint, which is a very simple thing, but not only practice, it will go for the joint. Should we go for that should be going for the joint. Right? So now I review the performance plots, but if I did that overestimation somehow in the beginning, then yeah, I might go to the hash join unnecessarily compared to that loop done. But now I'm putting much much effort in and doing better plans and having a bit of overhead. But at least the user performance will be guaranteed somehow because I'm usually shooting for the best look like, I mean putting myself on the line so that, you know, maybe I'm getting it in the right way or in the wrong way. So a good practice here. If you're doing cost estimation and you have a trend of to go for the over, then Okay. So that so let's just recap what we we went over. So quality estimation is one way of estimating the importance of that operators that we have and the group the position plan if you do it accurately then it would be good for you. But it's a very error prone process. So you have to be careful when you do that. That's why in most of the recent research now we try to figure out the plan without even going to the edge using some machine learning techniques. We can do that. So basically I can run some plans, run them, maybe based on some cost estimation in the beginning. And then I take the machine learning model and make it somehow learn what are that good blend in total without going through the details and and just based on the running time. And then when I see something similar in the future, I just pick the similar plan without going through the cost estimation. The main drawback for for that is that even with doing this, if you're, you're choosing the plan or like, I mean learning the model that picks this plan in the beginning based on wrong cost estimation, then you keep getting wrong choice after the best advantage of having this is if you doesn't use the model, you don't have to go through some of the updates that we have. In the cost estimation. You don't need to go and make the data periodically because you need to calculate the cost of submission for every experimental. So that's another aspect. Okay. So it's it's still a tradeoff, but most of the systems use a cut, not the estimation. And the the query optimizer is depending on, okay, so any questions about okay, so this like is relatively easy to understand, but it's a very crucial because now in practice they are somehow having more complicated operation. But these are the basics that you have to understand when you're dealing with the customers. Okay, So let's start now. Our presentations.